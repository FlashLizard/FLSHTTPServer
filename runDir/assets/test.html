<!DOCTYPE html>
<html lang="">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0" />
    <style>
        @media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) { .c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card--major .c-card__title,.c-card__title,.u-h2,.u-h3,h2,h3{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-weight:700;letter-spacing:-.0117156rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.u-h3,h3{font-size:1.25rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}html{text-size-adjust:100%;box-sizing:border-box;font-size:100%;height:100%;line-height:1.15;overflow-y:scroll}body{background:#eee;color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;line-height:1.76;margin:0;min-height:100%}main{display:block}h1{font-size:2em;margin:.67em 0}a,sup{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sup{font-size:75%;line-height:0;position:relative;top:-.5em}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input,select{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}h1{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-size:2rem;font-weight:700;letter-spacing:-.0390625rem;line-height:2.25rem}.c-card--major .c-card__title,.u-h2,.u-h3,h2{font-family:Harding,Palatino,serif;letter-spacing:-.0117156rem}.c-card--major .c-card__title,.u-h2,h2{-webkit-font-smoothing:antialiased;font-size:1.5rem;font-weight:700;line-height:1.6rem}.u-h3{font-size:1.25rem}.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h4,h5,h6{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,h3{font-family:Harding,Palatino,serif;font-size:1.25rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,h3{-webkit-font-smoothing:antialiased;font-weight:700;letter-spacing:-.0117156rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;letter-spacing:-.0117156rem}input+label{padding-left:.5em}nav ol,nav ul{list-style:none none}p:empty{display:none}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.article-page{background:#fff}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Harding,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-associated-content__container .c-article-associated-content__title{margin-bottom:8px}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-license__badge,c-card__section{margin-top:8px}.c-code-block{border:1px solid #eee;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-editorial-summary__container{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem}.c-article-editorial-summary__container .c-article-editorial-summary__content p:last-child{margin-bottom:0}.c-article-editorial-summary__container .c-article-editorial-summary__content--less{max-height:9.5rem;overflow:hidden}.c-article-editorial-summary__container .c-article-editorial-summary__button{background-color:#fff;border:0;color:#069;font-size:.875rem;margin-bottom:16px}.c-article-editorial-summary__container .c-article-editorial-summary__button.active,.c-article-editorial-summary__container .c-article-editorial-summary__button.hover,.c-article-editorial-summary__container .c-article-editorial-summary__button:active,.c-article-editorial-summary__container .c-article-editorial-summary__button:hover{text-decoration:underline;text-decoration-skip-ink:auto}.c-article-editorial-summary__container .c-article-editorial-summary__button:focus{outline:3px solid #fece3e;will-change:transform}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#069;border:1px solid #069;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#069}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}.c-status-message--success{border-bottom:2px solid #00b8b0;justify-content:center;margin-bottom:16px;padding-bottom:16px}.c-recommendations-title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;font-weight:700;line-height:1.24;margin:0;padding-bottom:16px}.c-recommendations-close{background-color:transparent;border:0;cursor:pointer;height:2em;margin-right:-10px;margin-top:-5px;width:2em}.c-recommendations-authors{line-height:1.24;margin:0}.c-recommendations-list-container{position:relative}.c-recommendations-list{display:flex;flex-wrap:nowrap;justify-content:space-between;margin:0 auto;overflow-x:hidden;padding:0 0 16px;scroll-behavior:smooth;scroll-snap-type:x mandatory;width:calc(100% - 128px)}@media only screen and (max-width:539px){.c-recommendations-list{display:block;height:40vh;overflow-y:auto;width:100%}}.c-recommendations-list__item{display:flex;flex:0 0 calc(33.3333% - 24px);margin:0 24px 0 0;scroll-snap-align:center}@media only screen and (max-width:539px){.c-recommendations-list__item{margin:0;padding:0 0 16px}}.c-recommendations-list__item .c-card__image{align-items:baseline;flex:1 1 40%;margin:0 16px 0 0;max-width:150px}.c-recommendations-list__item .c-card__image img{border:1px solid #d5d5d5;height:auto;min-height:0;position:relative;transform:translateY(0)}@media only screen and (max-width:1023px){.c-recommendations-list__item .c-card__image{display:none}}.c-card__layout{display:flex;flex:1 1 auto;justify-content:space-between}.c-card__title-recommendation{-webkit-box-orient:vertical;-webkit-line-clamp:4;display:-webkit-box;font-size:1rem;font-weight:700;line-height:1.4;margin:0;max-height:5.6em;overflow:hidden!important;text-overflow:ellipsis}.c-card__title-recommendation .c-card__link{color:#069;text-decoration:none}.c-card__title-recommendation .c-card__link:hover{text-decoration:underline}@media only screen and (max-width:539px){.c-recommendations-column-switch{display:flex;flex-direction:column-reverse}}.js-greyout-page-background{background-color:rgba(34,34,34,.75);bottom:0;left:0;position:fixed;right:0;top:0}.c-article-metrics__heading a,.c-article-metrics__posts .c-card__title a{color:inherit}.c-article-metrics__posts .c-card__title{font-size:1.05rem}.c-article-metrics__posts .c-card__title+span{color:#6f6f6f;font-size:1rem}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-author-list{color:#6f6f6f;font-family:inherit;font-size:1rem;line-height:inherit;list-style:none;margin:0;padding:0}.c-author-list>li,.c-breadcrumbs>li,.c-footer__links>li,.js .c-author-list,.u-list-comma-separated>li,.u-list-inline>li{display:inline}.c-author-list>li:not(:first-child):not(:last-child):before{content:", "}.c-author-list>li:not(:only-child):last-child:before{content:" & "}.c-author-list--compact{font-size:.875rem;line-height:1.4}.c-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.js .c-author-list__hide{display:none;visibility:hidden}.js .c-author-list__hide:first-child+*{margin-block-start:0}.c-meta{color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;list-style:none;margin:0;padding:0}.c-meta--large{font-size:1rem}.c-meta--large .c-meta__item{margin-bottom:8px}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;margin-right:4px;padding-right:4px}@media only screen and (max-width:539px){.c-meta__item--block-sm-max{display:block}.c-meta__item--block-sm-max:not(:last-child){border-right:none;margin-right:0;padding-right:0}}@media only screen and (min-width:1024px){.c-meta__item--block-at-lg{display:block}.c-meta__item--block-at-lg:not(:last-child){border-right:none;margin-right:0;padding-right:0}}.c-meta__type{font-weight:700;text-transform:none}.c-skip-link{background:#069;bottom:auto;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#fff}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-card__summary>p:last-child,.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #eee;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__heading{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#888;height:10px;margin:4px 4px 0;width:10px}.c-card{background-color:transparent;border:0;box-shadow:none;display:flex;flex-direction:column;font-size:14px;min-width:0;overflow:hidden;padding:0;position:relative}.c-card--no-shape{background:0 0;border:0;box-shadow:none}.c-card__image{display:flex;justify-content:center;overflow:hidden;padding-bottom:56.25%;position:relative}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card--no-shape .c-card__body{padding:0}.c-card--no-shape .c-card__body:not(:first-child){padding-top:16px}.c-card__title{letter-spacing:-.01875rem;margin-bottom:8px;margin-top:0}[lang=de] .c-card__title{hyphens:auto}.c-card__summary{line-height:1.4}.c-card__summary>p{margin-bottom:5px}.c-card__summary a{text-decoration:underline}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-card--major{font-size:1rem}.c-card--dark{background-color:#29303c;border-width:0;color:#e3e4e5}.c-card--dark .c-card__title{color:#fff}.c-card--dark .c-card__link,.c-card--dark .c-card__summary a{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin:8px 24px 8px 0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px;padding:0 16px}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;gap:8px 8px;line-height:1.4;list-style:none;margin:0 -8px;padding:0}@media print{.c-header__menu{display:none}}@media only screen and (max-width:1023px){.c-header__menu--hide-lg-max{display:none;visibility:hidden}}.c-header__menu--global{font-weight:400;justify-content:flex-end}.c-header__menu--global svg{display:none;visibility:hidden}@media only screen and (min-width:540px){.c-header__menu--global svg{display:block;visibility:visible}}.c-header__menu--journal{font-size:.875rem;margin:8px 0 8px -8px}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}.c-header__item{padding-bottom:0;padding-top:0;position:static}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}.c-header__item--padding{padding-bottom:8px;padding-top:8px}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}@media only screen and (min-width:1024px){.c-header__item--hide-lg{display:none;visibility:hidden}}@media only screen and (max-width:767px){.c-header__item--hide-md-max{display:none;visibility:hidden}.c-header__item--hide-md-max:first-child+*{margin-block-start:0}}.c-header__link{align-items:center;color:inherit;display:inline-flex;gap:4px 4px;padding:8px;white-space:nowrap}.c-header__link svg{transition-duration:.2s}.c-header__show-text{display:none;visibility:hidden}.has-tethered .c-header__heading--js-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__show-text{display:inline;visibility:visible}}.c-header__dropdown{background-color:#000;border-bottom:1px solid #2f2f2f;color:#eee;font-size:.875rem;line-height:1.2;padding:16px 0}@media print{.c-header__dropdown{display:none}}.c-header__heading{display:inline-block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:400;line-height:1.4;margin-bottom:8px}.c-header__heading--keyline{border-top:1px solid;border-color:#2f2f2f;margin-top:16px;padding-top:16px;width:100%}.c-header__list{display:flex;flex-wrap:wrap;gap:0 16px;list-style:none;margin:0 -8px;padding:0}.c-header__flush{margin:0 -8px}.c-header__visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.c-header__search-form{margin-bottom:8px}.c-header__search-layout{display:flex;flex-wrap:wrap;gap:16px 16px}.c-header__search-layout>:first-child{flex:999 1 auto}.c-header__search-layout>*{flex:1 1 auto}.c-header__search-layout--max-width{max-width:720px}.c-header__search-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #fff;border-radius:2px;color:#fff;cursor:pointer;display:flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.15;margin:0;padding:8px 16px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:100%}.u-button svg,.u-button--primary svg{fill:currentcolor}.c-header__input,.c-header__select{border:1px solid;border-radius:3px;box-sizing:border-box;font-size:1rem;padding:8px 16px;width:100%}.c-header__select{-webkit-appearance:none;background-image:url("data:image/svg+xml,%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z' fill='%23333' fill-rule='evenodd' transform='matrix(0 1 -1 0 11 3)'/%3E%3C/svg%3E");background-position:right .7em top 50%;background-repeat:no-repeat;background-size:1em;box-shadow:0 1px 0 1px rgba(0,0,0,.04);display:block;margin:0;max-width:100%;min-width:150px}@media only screen and (min-width:540px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:auto;right:0}}@media only screen and (min-width:768px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:0;right:auto}}.c-header__dropdown.has-tethered{border-bottom:0;border-radius:0 0 2px 2px;left:0;position:absolute;top:100%;transform:translateY(5px);width:100%;z-index:1}@media only screen and (min-width:540px){.c-header__dropdown.has-tethered{transform:translateY(8px);width:auto}}@media only screen and (min-width:768px){.c-header__dropdown.has-tethered{min-width:225px}}.c-header__dropdown--full-width.has-tethered{padding:32px 0 24px;transform:none;width:100%}.has-tethered .c-header__heading--js-hide{display:none;visibility:hidden}.has-tethered .c-header__list--js-stack{flex-direction:column}.has-tethered .c-header__item--keyline,.has-tethered .c-header__list~.c-header__list .c-header__item:first-child{border-top:1px solid #d5d5d5;margin-top:8px;padding-top:8px}.u-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #069;border-radius:2px;color:#069;cursor:pointer;display:inline-flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:540px){.u-hide-at-sm{display:none;visibility:hidden}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-color-open-access{color:#b74616}.u-float-left{float:left}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-full-height{height:100%}.u-list-reset{list-style:none;margin:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-display-flex{display:flex;width:100%}.u-flex-direction-column{flex-direction:column}.u-justify-content-space-between{justify-content:space-between}.u-flex-static{flex:0 0 auto}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.c-nature-box svg+.c-article__button-text,.u-ml-8{margin-left:8px}.u-pa-16{padding:16px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-card__title{line-height:1.4em}.c-article__button{background-color:#069;border:1px solid #069;border-radius:2px;color:#fff;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;margin-bottom:16px;padding:13px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-article__button,.c-article__button:hover{text-decoration:none}.c-article__button--inverted,.c-article__button:hover{background-color:#fff;color:#069}.c-article__button--inverted:hover{background-color:#069;color:#fff}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.u-lazy-ad-wrapper{background-color:#ccc;display:none;min-height:137px}@media only screen and (min-width:768px){.u-lazy-ad-wrapper{display:block}}.c-nature-box{background-color:#fff;border:1px solid #d5d5d5;border-radius:2px;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.3;margin-bottom:24px;padding:16px 16px 3px}.c-nature-box__text{font-size:1rem;margin-bottom:16px}.c-nature-box .c-pdf-download{margin-bottom:16px!important}.c-nature-box--version{background-color:#eee}.c-nature-box__wrapper{transform:translateZ(0)}.c-nature-box__wrapper--placeholder{min-height:165px} }
    </style>
</head>

<body>
    <div>
        <article lang="en">
<!-- 
            <div class="c-pdf-button__container u-mb-16 u-hide-at-lg js-context-bar-sticky-point-mobile">
                <div class="c-pdf-container">



                    <div class="c-pdf-download u-clear-both js-pdf-download">
                        <a href="/articles/s41586-023-06291-2.pdf"
                            class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link"
                            data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf"
                            data-draft-ignore="true" data-track="click" data-track-action="download pdf"
                            data-track-label="link" data-track-external="" download="">
                            <span class="c-pdf-download__text">Download PDF</span>
                            <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon">
                                <use xlink:href="#icon-download"></use>
                            </svg>
                        </a>
                    </div>



                </div>
            </div> -->

            <div class="c-article-header">
                <header>
                    <!-- <ul class="c-article-identifiers" data-test="article-identifier">


                        <li class="c-article-identifiers__item" data-test="article-category">Article</li>


                        <li class="c-article-identifiers__item">
                            <a href="https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research"
                                data-track="click" data-track-action="open access" data-track-label="link"
                                class="u-color-open-access" data-test="open-access">Open Access</a>
                        </li>



                        <li class="c-article-identifiers__item"><a href="#article-info" data-track="click"
                                data-track-action="publication date" data-track-label="link">Published: <time
                                    datetime="2023-07-12">12 July 2023</time></a></li>
                    </ul> -->

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">Large language models
                        encode
                        clinical knowledge</h1>
                    <!-- <ul class="c-article-author-list c-article-author-list--long js-no-scroll" data-test="authors-list"
                        data-component-authors-activator="authors-list">
                        <li class="c-article-author-list__item"><a data-test="author-name" data-track="click"
                                data-track-action="open author" data-track-label="link" href="#auth-Karan-Singhal-Aff1"
                                data-author-popup="auth-Karan-Singhal-Aff1" data-corresp-id="c1">Karan Singhal<svg
                                    width="16" height="16" focusable="false" role="img" aria-hidden="true"
                                    class="u-icon">
                                    <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-email"></use>
                                </svg></a><sup class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup><sup
                                class="u-js-hide">&nbsp;<a href="#na1" tabindex="-1">na1</a></sup>, </li>
                        <li class="c-article-author-list__item"><a data-test="author-name" data-track="click"
                                data-track-action="open author" data-track-label="link"
                                href="#auth-Shekoofeh-Azizi-Aff1" data-author-popup="auth-Shekoofeh-Azizi-Aff1"
                                data-corresp-id="c2">Shekoofeh Azizi<svg width="16" height="16" focusable="false"
                                    role="img" aria-hidden="true" class="u-icon">
                                    <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-email"></use>
                                </svg></a><span class="u-js-hide">&nbsp;
                                <a class="js-orcid" href="http://orcid.org/0000-0002-7447-6031"><span
                                        class="u-visually-hidden">ORCID:
                                    </span>orcid.org/0000-0002-7447-6031</a></span><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup><sup class="u-js-hide">&nbsp;<a href="#na1"
                                    tabindex="-1">na1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Tao-Tu-Aff1"
                                data-author-popup="auth-Tao-Tu-Aff1">Tao
                                Tu</a><sup class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup><sup
                                class="u-js-hide">&nbsp;<a href="#na1" tabindex="-1">na1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-S__Sara-Mahdavi-Aff1"
                                data-author-popup="auth-S__Sara-Mahdavi-Aff1">S. Sara Mahdavi</a><sup
                                class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Jason-Wei-Aff1"
                                data-author-popup="auth-Jason-Wei-Aff1">Jason Wei</a><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Hyung_Won-Chung-Aff1"
                                data-author-popup="auth-Hyung_Won-Chung-Aff1">Hyung Won Chung</a><sup
                                class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Nathan-Scales-Aff1"
                                data-author-popup="auth-Nathan-Scales-Aff1">Nathan Scales</a><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Ajay-Tanwani-Aff1"
                                data-author-popup="auth-Ajay-Tanwani-Aff1">Ajay Tanwani</a><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Heather-Cole_Lewis-Aff1"
                                data-author-popup="auth-Heather-Cole_Lewis-Aff1">Heather Cole-Lewis</a><sup
                                class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Stephen-Pfohl-Aff1"
                                data-author-popup="auth-Stephen-Pfohl-Aff1">Stephen Pfohl</a><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Perry-Payne-Aff1"
                                data-author-popup="auth-Perry-Payne-Aff1">Perry Payne</a><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Martin-Seneviratne-Aff1"
                                data-author-popup="auth-Martin-Seneviratne-Aff1">Martin Seneviratne</a><sup
                                class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Paul-Gamble-Aff1"
                                data-author-popup="auth-Paul-Gamble-Aff1">Paul Gamble</a><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Chris-Kelly-Aff1"
                                data-author-popup="auth-Chris-Kelly-Aff1">Chris Kelly</a><span class="u-js-hide">&nbsp;
                                <a class="js-orcid" href="http://orcid.org/0000-0002-1246-844X"><span
                                        class="u-visually-hidden">ORCID:
                                    </span>orcid.org/0000-0002-1246-844X</a></span><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Abubakr-Babiker-Aff1"
                                data-author-popup="auth-Abubakr-Babiker-Aff1">Abubakr Babiker</a><sup
                                class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Nathanael-Sch_rli-Aff1"
                                data-author-popup="auth-Nathanael-Sch_rli-Aff1">Nathanael Schärli</a><sup
                                class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Aakanksha-Chowdhery-Aff1"
                                data-author-popup="auth-Aakanksha-Chowdhery-Aff1">Aakanksha Chowdhery</a><sup
                                class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Philip-Mansfield-Aff1"
                                data-author-popup="auth-Philip-Mansfield-Aff1">Philip Mansfield</a><span
                                class="u-js-hide">&nbsp;
                                <a class="js-orcid" href="http://orcid.org/0000-0003-4969-0543"><span
                                        class="u-visually-hidden">ORCID:
                                    </span>orcid.org/0000-0003-4969-0543</a></span><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Dina-Demner_Fushman-Aff2"
                                data-author-popup="auth-Dina-Demner_Fushman-Aff2">Dina Demner-Fushman</a><sup
                                class="u-js-hide"><a href="#Aff2" tabindex="-1">2</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Blaise-Ag_era_y_Arcas-Aff1"
                                data-author-popup="auth-Blaise-Ag_era_y_Arcas-Aff1">Blaise Agüera y Arcas</a><sup
                                class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Dale-Webster-Aff1"
                                data-author-popup="auth-Dale-Webster-Aff1">Dale Webster</a><span
                                class="u-js-hide">&nbsp;
                                <a class="js-orcid" href="http://orcid.org/0000-0002-3023-8824"><span
                                        class="u-visually-hidden">ORCID:
                                    </span>orcid.org/0000-0002-3023-8824</a></span><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Greg_S_-Corrado-Aff1"
                                data-author-popup="auth-Greg_S_-Corrado-Aff1">Greg S. Corrado</a><sup
                                class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Yossi-Matias-Aff1"
                                data-author-popup="auth-Yossi-Matias-Aff1">Yossi Matias</a><span
                                class="u-js-hide">&nbsp;
                                <a class="js-orcid" href="http://orcid.org/0000-0003-3960-6002"><span
                                        class="u-visually-hidden">ORCID:
                                    </span>orcid.org/0000-0003-3960-6002</a></span><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a
                                data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Katherine-Chou-Aff1"
                                data-author-popup="auth-Katherine-Chou-Aff1">Katherine Chou</a><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>, </li>
                        <li
                            class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
                            <a data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Juraj-Gottweis-Aff1"
                                data-author-popup="auth-Juraj-Gottweis-Aff1">Juraj Gottweis</a><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>,
                        </li>
                        <li
                            class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
                            <a data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Nenad-Tomasev-Aff3"
                                data-author-popup="auth-Nenad-Tomasev-Aff3">Nenad Tomasev</a><span
                                class="u-js-hide">&nbsp;
                                <a class="js-orcid" href="http://orcid.org/0000-0003-1624-0220"><span
                                        class="u-visually-hidden">ORCID:
                                    </span>orcid.org/0000-0003-1624-0220</a></span><sup class="u-js-hide"><a
                                    href="#Aff3" tabindex="-1">3</a></sup>,
                        </li>
                        <li
                            class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
                            <a data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Yun-Liu-Aff1"
                                data-author-popup="auth-Yun-Liu-Aff1">Yun
                                Liu</a><span class="u-js-hide">&nbsp;
                                <a class="js-orcid" href="http://orcid.org/0000-0003-4079-8275"><span
                                        class="u-visually-hidden">ORCID:
                                    </span>orcid.org/0000-0003-4079-8275</a></span><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>,
                        </li>
                        <li
                            class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
                            <a data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Alvin-Rajkomar-Aff1"
                                data-author-popup="auth-Alvin-Rajkomar-Aff1">Alvin Rajkomar</a><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>,
                        </li>
                        <li
                            class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
                            <a data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Joelle-Barral-Aff1"
                                data-author-popup="auth-Joelle-Barral-Aff1">Joelle Barral</a><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>,
                        </li>
                        <li
                            class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
                            <a data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Christopher-Semturs-Aff1"
                                data-author-popup="auth-Christopher-Semturs-Aff1">Christopher Semturs</a><span
                                class="u-js-hide">&nbsp;
                                <a class="js-orcid" href="http://orcid.org/0000-0001-6108-2773"><span
                                        class="u-visually-hidden">ORCID:
                                    </span>orcid.org/0000-0001-6108-2773</a></span><sup class="u-js-hide"><a
                                    href="#Aff1" tabindex="-1">1</a></sup>,
                        </li>
                        <li
                            class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
                            <a data-test="author-name" data-track="click" data-track-action="open author"
                                data-track-label="link" href="#auth-Alan-Karthikesalingam-Aff1"
                                data-author-popup="auth-Alan-Karthikesalingam-Aff1" data-corresp-id="c3">Alan
                                Karthikesalingam<svg width="16" height="16" focusable="false" role="img"
                                    aria-hidden="true" class="u-icon">
                                    <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-email"></use>
                                </svg></a><sup class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup><sup
                                class="u-js-hide">&nbsp;<a href="#na2" tabindex="-1">na2</a></sup> &amp;
                        </li>
                        <li class="c-article-author-list__show-more" aria-label="Show all 32 authors for this article"
                            title="Show all 32 authors for this article">…</li>
                        <li class="c-article-author-list__item"><a data-test="author-name" data-track="click"
                                data-track-action="open author" data-track-label="link"
                                href="#auth-Vivek-Natarajan-Aff1" data-author-popup="auth-Vivek-Natarajan-Aff1"
                                data-corresp-id="c4">Vivek Natarajan<svg width="16" height="16" focusable="false"
                                    role="img" aria-hidden="true" class="u-icon">
                                    <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-email"></use>
                                </svg></a><sup class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup><sup
                                class="u-js-hide">&nbsp;<a href="#na2" tabindex="-1">na2</a></sup>&nbsp;</li>
                    </ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16"
                            focusable="false" role="img" aria-hidden="true" class="u-icon">
                            <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-plus"></use>
                        </svg><span>Show authors</span></button>



                    <p class="c-article-info-details" data-container-section="info">

                        <a data-test="journal-link" href="/" data-track="click" data-track-action="journal homepage"
                            data-track-category="article body" data-track-label="link"><i
                                data-test="journal-title">Nature</i></a>

                        <b data-test="journal-volume"><span
                                class="u-visually-hidden">volume</span>&nbsp;620</b>,&nbsp;<span
                            class="u-visually-hidden">pages </span>172–180 (<span
                            data-test="article-publication-year">2023</span>)<a href="#citeas"
                            class="c-article-info-details__cite-as u-hide-print" data-track="click"
                            data-track-action="cite this article" data-track-label="link">Cite this article</a>
                    </p>

                    <div class="c-article-metrics-bar__wrapper u-clear-both">
                        <ul class="c-article-metrics-bar u-list-reset">

                            <li class=" c-article-metrics-bar__item">
                                <p class="c-article-metrics-bar__count">87k <span
                                        class="c-article-metrics-bar__label">Accesses</span></p>
                            </li>


                            <li class="c-article-metrics-bar__item">
                                <p class="c-article-metrics-bar__count">9 <span
                                        class="c-article-metrics-bar__label">Citations</span></p>
                            </li>



                            <li class="c-article-metrics-bar__item">
                                <p class="c-article-metrics-bar__count">864 <span
                                        class="c-article-metrics-bar__label">Altmetric</span></p>
                            </li>


                            <li class="c-article-metrics-bar__item">
                                <p class="c-article-metrics-bar__details"><a href="/articles/s41586-023-06291-2/metrics"
                                        data-track="click" data-track-action="view metrics" data-track-label="link"
                                        rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                            </li>
                        </ul>
                    </div> -->


                </header>


<!-- 



                <div class="u-mb-8 c-status-message c-status-message--boxed c-status-message--info">

                    <span class="c-status-message__icon">
                        <svg class="u-icon" width="18" height="18" aria-hidden="true" focusable="false">
                            <use xlink:href="#icon-info"></use>
                        </svg>
                    </span>



                    <p class="u-mt-0">A <a href="https://doi.org/10.1038/s41586-023-06455-0" class="relation-link"
                            data-track="click" data-track-action="view linked article" data-track-label="link">Publisher
                            Correction</a> to this article was published on 27 July 2023</p>


                </div>



                <div class="u-mb-8 c-status-message c-status-message--boxed c-status-message--info">


                    <span class="c-status-message__icon">
                        <svg class="u-icon" width="18" height="18" aria-hidden="true" focusable="false">
                            <use xlink:href="#icon-info"></use>
                        </svg>
                    </span>


                    <p class="u-mt-0">This article has been <a href="#change-history">updated</a></p>
                </div> -->



            </div>

            <div class="c-article-body">
                <section aria-labelledby="Abs1" data-title="Abstract" lang="en">
                    <div class="c-article-section" id="Abs1-section">
                        <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                            id="Abs1">Abstract</h2>
                        <div class="c-article-section__content" id="Abs1-content">
                            <p>Large language models (LLMs) have demonstrated impressive capabilities, but the bar for
                                clinical applications is high. Attempts to assess the clinical knowledge of models
                                typically
                                rely on automated evaluations based on limited benchmarks. Here, to address these
                                limitations, we present MultiMedQA, a benchmark combining six existing medical question
                                answering datasets spanning professional medicine, research and consumer queries
                                and&nbsp;a
                                new dataset of medical questions searched online, HealthSearchQA. We propose a human
                                evaluation framework for model answers along multiple axes including factuality,
                                comprehension,&nbsp;reasoning, possible harm and bias. In addition, we evaluate Pathways
                                Language Model<sup><a data-track="click" data-track-action="reference anchor"
                                        data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at 
          https://doi.org/10.48550/arXiv.2204.02311
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR1" id="ref-link-section-d77463470e707">1</a></sup>
                                (PaLM,&nbsp;a 540-billion parameter LLM) and its instruction-tuned variant,
                                Flan-PaLM<sup><a data-track="click" data-track-action="reference anchor"
                                        data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Chung, H. W. et al. Scaling instruction-finetuned language models. Preprint at 
          https://doi.org/10.48550/arXiv.2210.11416
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR2" id="ref-link-section-d77463470e711">2</a></sup> on
                                MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves
                                state-of-the-art
                                accuracy on every MultiMedQA multiple-choice dataset (MedQA<sup><a data-track="click"
                                        data-track-action="reference anchor" data-track-label="link"
                                        data-test="citation-ref" aria-label="Reference 3"
                                        title="Jin, D. et al. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. Appl. Sci. 11, 6421 (2021)."
                                        href="/articles/s41586-023-06291-2#ref-CR3"
                                        id="ref-link-section-d77463470e715">3</a></sup>, MedMCQA<sup><a
                                        data-track="click" data-track-action="reference anchor" data-track-label="link"
                                        data-test="citation-ref" aria-label="Reference 4"
                                        title="Pal, A., Umapathi, L. K. &amp; Sankarasubbu, M. MedMCQA: a large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning 248–260 (Proceedings of Machine Learning Research, 2022)."
                                        href="/articles/s41586-023-06291-2#ref-CR4"
                                        id="ref-link-section-d77463470e719">4</a></sup>, PubMedQA<sup><a
                                        data-track="click" data-track-action="reference anchor" data-track-label="link"
                                        data-test="citation-ref" aria-label="Reference 5" title="Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. &amp; Lu, X. PubMedQA: a dataset for biomedical research question answering. Preprint at 
          https://doi.org/10.48550/arXiv.1909.06146
          
         (2019)." href="/articles/s41586-023-06291-2#ref-CR5" id="ref-link-section-d77463470e723">5</a></sup> and
                                Measuring Massive Multitask Language Understanding (MMLU) clinical topics<sup><a
                                        data-track="click" data-track-action="reference anchor" data-track-label="link"
                                        data-test="citation-ref" aria-label="Reference 6" title="Hendrycks, D. et al. Measuring massive multitask language understanding. Preprint at 
          https://doi.org/10.48550/arXiv.2009.03300
          
         (2020)." href="/articles/s41586-023-06291-2#ref-CR6" id="ref-link-section-d77463470e728">6</a></sup>),
                                including 67.6% accuracy on MedQA&nbsp;(US Medical Licensing Exam-style questions),
                                surpassing the prior state of the art by more than 17%. However, human evaluation
                                reveals
                                key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient
                                approach for aligning LLMs to new domains using a few exemplars. The resulting model,
                                Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that
                                comprehension, knowledge recall and reasoning improve with model scale and instruction
                                prompt tuning, suggesting the potential utility of LLMs in medicine. Our human
                                evaluations
                                reveal limitations of today’s models, reinforcing the importance of both evaluation
                                frameworks and method development in creating safe, helpful LLMs for clinical
                                applications.
                            </p>
                        </div>
                    </div>
                </section>
                <noscript>

                </noscript>



                <div class="main-content">
                    <section data-title="Main">
                        <div class="c-article-section" id="Sec1-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="Sec1">Main</h2>
                            <div class="c-article-section__content" id="Sec1-content">
                                <p>Medicine is a humane endeavour in which language enables key interactions for and
                                    between
                                    clinicians, researchers and patients. Yet, today’s artificial intelligence (AI)
                                    models
                                    for applications in medicine and healthcare have largely failed to fully utilize
                                    language. These models, although useful, are predominantly single-task systems (for
                                    example, for classification, regression or segmentation) lacking expressivity and
                                    interactive capabilities<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref"
                                            title="Esteva, A. et al. Deep learning-enabled medical computer vision. NPJ Digit. Med. 4, 5 (2021)."
                                            href="#ref-CR7" id="ref-link-section-d77463470e773">7</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref"
                                            title="Tomašev, N. et al. Use of deep learning to develop continuous-risk models for adverse event prediction from electronic health records. Nat. Protoc. 16, 2765–2787 (2021)."
                                            href="#ref-CR8" id="ref-link-section-d77463470e773_1">8</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 9"
                                            title="Yim, J. et al. Predicting conversion to wet age-related macular degeneration using deep learning. Nat. Med. 26, 892–899 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR9"
                                            id="ref-link-section-d77463470e776">9</a></sup>. As a result, there is a
                                    discordance between what today’s models can do and what may be expected of them in
                                    real-world clinical workflows<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 10" title="Lakkaraju, H., Slack, D., Chen, Y., Tan, C. &amp; Singh, S. Rethinking explainability as a dialogue: a practitioner’s perspective. Preprint at 
          https://doi.org/10.48550/arXiv.2202.01875
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR10" id="ref-link-section-d77463470e780">10</a></sup>.</p>
                                <div class="c-article-section__figure js-c-reading-companion-figures-item"
                                    data-test="figure" data-container-section="figure" id="figure-1"
                                    data-title="Overview of our contributions.">
                                    <figure>
                                        <figcaption><b id="Fig1" class="c-article-section__figure-caption"
                                                data-test="figure-caption-text">Fig. 1: Overview of our
                                                contributions.</b>
                                        </figcaption>
                                        <div class="c-article-section__figure-content">
                                            <div class="c-article-section__figure-item"><a
                                                    class="c-article-section__figure-link" data-test="img-link"
                                                    data-track="click" data-track-label="image"
                                                    data-track-action="view figure"
                                                    href="/articles/s41586-023-06291-2/figures/1" rel="nofollow">
                                                    <picture>
                                                        <source type="image/webp"
                                                            srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig1_HTML.png?as=webp">
                                                        <img aria-describedby="Fig1"
                                                            src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig1_HTML.png"
                                                            alt="figure 1" loading="lazy" width="685" height="267">
                                                    </picture>
                                                </a></div>
                                            <div class="c-article-section__figure-description"
                                                data-test="bottom-caption" id="figure-1-desc">
                                                <p>We curate MultiMedQA, a benchmark for answering medical questions
                                                    spanning medical exam, medical research and consumer medical
                                                    questions.
                                                    We evaluate PaLM and its instructed-tuned variant, Flan-PaLM, on
                                                    MultiMedQA. Using a combination of prompting strategies, Flan-PaLM
                                                    exceeds state-of-the-art performance on MedQA (US Medical Licensing
                                                    Examination (USMLE)), MedMCQA, PubMedQA and MMLU clinical topics. In
                                                    particular, it improves over the previous state of the art on MedQA
                                                    (USMLE) by over 17%. We next propose instruction prompt tuning to
                                                    further align Flan-PaLM to the medical domain, producing Med-PaLM.
                                                    Med-PaLM’s answers to consumer medical questions compare favourably
                                                    with
                                                    answers given by clinicians under our human evaluation framework,
                                                    demonstrating the effectiveness of instruction prompt tuning.</p>
                                            </div>
                                        </div>
                                        <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                data-test="article-link" data-track="click" data-track-label="button"
                                                data-track-action="view figure"
                                                href="/articles/s41586-023-06291-2/figures/1"
                                                data-track-dest="link:Figure1 Full size image"
                                                aria-label="Full size image figure 1" rel="nofollow"><span>Full size
                                                    image</span><svg width="16" height="16" focusable="false" role="img"
                                                    aria-hidden="true" class="u-icon">
                                                    <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                        xlink:href="#icon-chevron-right"></use>
                                                </svg></a></div>
                                    </figure>
                                </div>
                                <p>Recent advances in LLMs offer an opportunity to rethink AI systems, with language as
                                    a
                                    tool for mediating human–AI interaction. LLMs are ‘foundation models’<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 11"
                                            title="Bommasani, R. et al. On the opportunities and risks of foundation models. Preprint at 
          https://doi.org/10.48550/arXiv.2108.07258
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR11" id="ref-link-section-d77463470e806">11</a></sup>, large
                                    pre-trained AI systems that can be repurposed with minimal effort across numerous
                                    domains and diverse tasks. These expressive and interactive models offer great
                                    promise
                                    in their ability to learn generally useful representations from the knowledge
                                    encoded in
                                    medical corpora, at scale. There are several exciting potential applications of such
                                    models in medicine, including knowledge retrieval, clinical decision support,
                                    summarization of key findings, triaging patients, addressing primary care concerns
                                    and
                                    more.</p>
                                <p>However, the safety-critical nature of the domain necessitates thoughtful development
                                    of
                                    evaluation frameworks, enabling researchers to meaningfully measure progress and
                                    capture
                                    and mitigate potential harms. This is especially important for LLMs, since these
                                    models
                                    may produce text generations (hereafter referred to as ‘generations’) that are
                                    misaligned with clinical and societal values. They may, for instance, hallucinate
                                    convincing medical misinformation or incorporate biases that could exacerbate health
                                    disparities.</p>
                                <p>To evaluate how well LLMs encode clinical knowledge and assess their potential in
                                    medicine, we consider the answering of medical questions. This task is challenging:
                                    providing high-quality answers to medical questions requires comprehension of
                                    medical
                                    context, recall of appropriate medical knowledge, and reasoning with expert
                                    information.
                                    Existing medical question-answering benchmarks<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 3"
                                            title="Jin, D. et al. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. Appl. Sci. 11, 6421 (2021)."
                                            href="/articles/s41586-023-06291-2#ref-CR3"
                                            id="ref-link-section-d77463470e816">3</a></sup> are often limited to
                                    assessing
                                    classification accuracy or automated natural language generation metrics (for
                                    example,
                                    BLEU<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 12"
                                            title="Papineni, K., Roukos, S., Ward, T. &amp; Zhu, W.-J. BLEU: a method for automatic evaluation of machine translation. In Proc. 40th Annual Meeting of the Association for Computational Linguistics 311–318 (Association of Computational Machinery, 2002)."
                                            href="/articles/s41586-023-06291-2#ref-CR12"
                                            id="ref-link-section-d77463470e820">12</a></sup>) and do not enable the
                                    detailed
                                    analysis required for real-world clinical applications. This creates an unmet need
                                    for a
                                    broad medical question-answering benchmark to assess LLMs for their response
                                    factuality,
                                    use of expert knowledge in reasoning, helpfulness, precision, health equity and
                                    potential harm.</p>
                                <p>To address this, we curate MultiMedQA, a benchmark comprising seven medical
                                    question-answering datasets, including six existing datasets: MedQA<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 3"
                                            title="Jin, D. et al. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. Appl. Sci. 11, 6421 (2021)."
                                            href="/articles/s41586-023-06291-2#ref-CR3"
                                            id="ref-link-section-d77463470e828">3</a></sup>, MedMCQA<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 4"
                                            title="Pal, A., Umapathi, L. K. &amp; Sankarasubbu, M. MedMCQA: a large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning 248–260 (Proceedings of Machine Learning Research, 2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR4"
                                            id="ref-link-section-d77463470e832">4</a></sup>, PubMedQA<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 5"
                                            title="Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. &amp; Lu, X. PubMedQA: a dataset for biomedical research question answering. Preprint at 
          https://doi.org/10.48550/arXiv.1909.06146
          
         (2019)." href="/articles/s41586-023-06291-2#ref-CR5" id="ref-link-section-d77463470e836">5</a></sup>,
                                    LiveQA<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 13"
                                            title="Ben Abacha, A., Agichtein, E., Pinter, Y. &amp; Demner-Fushman, D. Overview of the medical question answering task at TREC 2017 LiveQA. TREC 
          https://trec.nist.gov/pubs/trec26/papers/Overview-QA.pdf?ref=https://githubhelp.com
          
         (2017)." href="/articles/s41586-023-06291-2#ref-CR13" id="ref-link-section-d77463470e840">13</a></sup>,
                                    MedicationQA<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 14"
                                            title="Abacha, A. B. et al. in Studies in Health Technology and Informatics (eds Ohno-Machado, L. &amp; Séroussi, B.) 25–29 (IOS Press, 2019)."
                                            href="/articles/s41586-023-06291-2#ref-CR14"
                                            id="ref-link-section-d77463470e844">14</a></sup> and MMLU clinical
                                    topics<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 6"
                                            title="Hendrycks, D. et al. Measuring massive multitask language understanding. Preprint at 
          https://doi.org/10.48550/arXiv.2009.03300
          
         (2020)." href="/articles/s41586-023-06291-2#ref-CR6" id="ref-link-section-d77463470e849">6</a></sup>. We
                                    introduce a seventh dataset, HealthSearchQA, which consists of commonly searched
                                    health
                                    questions.</p>
                                <p>To assess LLMs using MultiMedQA, we build on PaLM, a 540-billion parameter (540B)
                                    LLM<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 1"
                                            title="Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at 
          https://doi.org/10.48550/arXiv.2204.02311
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR1" id="ref-link-section-d77463470e856">1</a></sup>, and its
                                    instruction-tuned variant Flan-PaLM<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 2" title="Chung, H. W. et al. Scaling instruction-finetuned language models. Preprint at 
          https://doi.org/10.48550/arXiv.2210.11416
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR2" id="ref-link-section-d77463470e860">2</a></sup>. Using a
                                    combination of few-shot<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 15"
                                            title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR15"
                                            id="ref-link-section-d77463470e864">15</a></sup>, chain-of-thought<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 16"
                                            title="Wei, J. et al. Chain of thought prompting elicits reasoning in large language models. Preprint at 
          https://doi.org/10.48550/arXiv.2201.11903
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR16" id="ref-link-section-d77463470e868">16</a></sup> (COT)
                                    and self-consistency<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 17"
                                            title="Wang, X. et al. Self-consistency improves chain of thought reasoning in language models. Preprint at 
          https://doi.org/10.48550/arXiv.2203.11171
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR17" id="ref-link-section-d77463470e872">17</a></sup>
                                    prompting strategies, Flan-PaLM achieves state-of-the-art performance on MedQA,
                                    MedMCQA,
                                    PubMedQA and MMLU clinical topics, often outperforming several strong LLM baselines
                                    by a
                                    substantial margin. On the MedQA dataset comprising USMLE-style questions, FLAN-PaLM
                                    exceeds the previous state of the art by more than 17%.</p>
                                <p>Despite the strong performance of Flan-PaLM on multiple-choice questions, its answers
                                    to
                                    consumer medical questions reveal key gaps. To resolve this, we propose instruction
                                    prompt tuning, a data- and parameter-efficient alignment technique, to further adapt
                                    Flan-PaLM to the medical domain. The resulting model, Med-PaLM, performs
                                    encouragingly
                                    on the axes of our pilot human evaluation framework. For example, a panel of
                                    clinicians
                                    judged only 61.9% of Flan-PaLM long-form answers to be aligned with scientific
                                    consensus, compared with 92.6% for Med-PaLM answers, on par with clinician-generated
                                    answers (92.9%). Similarly, 29.7% of Flan-PaLM answers were rated as potentially
                                    leading
                                    to harmful outcomes, in contrast to 5.9% for Med-PaLM, which was similar to the
                                    result
                                    for clinician-generated answers (5.7%).</p>
                                <p>Although these results are promising, the medical domain is complex. Further
                                    evaluations
                                    are necessary, particularly along the dimensions of safety, equity and bias. Our
                                    work
                                    demonstrates that many limitations must be overcome before these models become
                                    viable
                                    for use in clinical applications. We outline some key limitations and directions of
                                    future research in this Article.</p>
                            </div>
                        </div>
                    </section>
                    <section data-title="Key contributions">
                        <div class="c-article-section" id="Sec2-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="Sec2">Key contributions</h2>
                            <div class="c-article-section__content" id="Sec2-content">
                                <p>Our first key contribution is an approach for evaluation of LLMs in the context of
                                    medical question answering. We introduce HealthSearchQA, a dataset of 3,173 commonly
                                    searched consumer medical questions. We present this dataset alongside six existing
                                    open
                                    datasets for answering medical questions spanning medical exam, medical research and
                                    consumer medical questions, as a diverse benchmark to assess the clinical knowledge
                                    and
                                    question-answering capabilities of LLMs (see&nbsp;Methods, ‘Datasets’).</p>
                                <p>We pilot a framework for physician and lay user evaluation to assess multiple axes of
                                    LLM
                                    performance beyond accuracy on multiple-choice datasets. Our evaluation assesses
                                    answers
                                    for agreement with the scientific and clinical consensus, the likelihood and
                                    possible
                                    extent of harm, reading comprehension, recall of relevant clinical knowledge,
                                    manipulation of knowledge via valid reasoning, completeness of responses, potential
                                    for
                                    bias, relevance and helpfulness (see&nbsp;Methods, ‘Framework for human
                                    evaluation’).
                                </p>
                                <p>The second key contribution is demonstrating&nbsp;state-of-the-art performance on the
                                    MedQA, MedMCQA, PubMedQA and MMLU clinical topics datasets using Flan-PaLM
                                    and&nbsp;a
                                    combination of prompting strategies, surpassing several strong LLM baselines.
                                    Specifically, we reach 67.6% accuracy on MedQA (more than 17% above the previous
                                    state
                                    of the art), 57.6% on MedMCQA and 79.0% on PubMedQA.</p>
                                <p>The next contribution is the introduction of instruction prompt tuning, a simple,
                                    data-
                                    and parameter-efficient technique for aligning LLMs to the safety-critical medical
                                    domain (see&nbsp;Methods, ‘Modelling’). We leverage this technique to build
                                    Med-PaLM, an
                                    instruction prompt-tuned version of Flan-PaLM specialized for the medical domain
                                    (Fig.
                                    <a data-track="click" data-track-label="link" data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig1">1</a>). Our human evaluation framework
                                    reveals limitations of Flan-PaLM in scientific grounding, harm and bias.
                                    Nevertheless,
                                    Med-PaLM substantially reduces the gap (or even compares favourably) to clinicians
                                    on
                                    several of these axes, according to both clinicians and lay users (see ‘Human
                                    evaluation
                                    results’).
                                </p>
                                <p>Finally, we discuss in detail key limitations of LLMs revealed by our human
                                    evaluation.
                                    Although our results demonstrate the potential of LLMs in medicine, they also
                                    suggest
                                    that several critical improvements are necessary in order to make these models
                                    viable
                                    for real-world clinical applications (see ‘Limitations’).</p>
                            </div>
                        </div>
                    </section>
                    <section data-title="Model development and evaluation of performance">
                        <div class="c-article-section" id="Sec3-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="Sec3">Model development and evaluation of performance</h2>
                            <div class="c-article-section__content" id="Sec3-content">
                                <p>We first provide an overview of our key results with Flan-PaLM on multiple-choice
                                    tasks
                                    as summarized in Fig. <a data-track="click" data-track-label="link"
                                        data-track-action="figure anchor" href="/articles/s41586-023-06291-2#Fig2">2</a>
                                    and
                                    Extended Data Fig. <a data-track="click" data-track-label="link"
                                        data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig8">2</a>.
                                    Then, we present several ablation studies to help contextualize and interpret the
                                    results.</p>
                                <div class="c-article-section__figure js-c-reading-companion-figures-item"
                                    data-test="figure" data-container-section="figure" id="figure-2"
                                    data-title="Comparison of our method and prior state of the art.">
                                    <figure>
                                        <figcaption><b id="Fig2" class="c-article-section__figure-caption"
                                                data-test="figure-caption-text">Fig. 2: Comparison of our method and
                                                prior
                                                state of the art.</b></figcaption>
                                        <div class="c-article-section__figure-content">
                                            <div class="c-article-section__figure-item"><a
                                                    class="c-article-section__figure-link" data-test="img-link"
                                                    data-track="click" data-track-label="image"
                                                    data-track-action="view figure"
                                                    href="/articles/s41586-023-06291-2/figures/2" rel="nofollow">
                                                    <picture>
                                                        <source type="image/webp"
                                                            srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig2_HTML.png?as=webp">
                                                        <img aria-describedby="Fig2"
                                                            src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig2_HTML.png"
                                                            alt="figure 2" loading="lazy" width="685" height="358">
                                                    </picture>
                                                </a></div>
                                            <div class="c-article-section__figure-description"
                                                data-test="bottom-caption" id="figure-2-desc">
                                                <p>Our Flan-PaLM 540B model exceeds the previous state-of-the-art
                                                    performance (SOTA) on MedQA (four options), MedMCQA and PubMedQA
                                                    datasets. The previous state-of-the-art results are from
                                                    Galactica<sup><a data-track="click"
                                                            data-track-action="reference anchor" data-track-label="link"
                                                            data-test="citation-ref" aria-label="Reference 20" title="Taylor, R. et al. Galactica: a large language model for science. Preprint at 
          https://doi.org/10.48550/arXiv.2211.09085
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR20" id="ref-link-section-d77463470e932">20</a></sup>
                                                    (MedMCQA), PubMedGPT<sup><a data-track="click"
                                                            data-track-action="reference anchor" data-track-label="link"
                                                            data-test="citation-ref" aria-label="Reference 19" title="Bolton, E. et al. Stanford CRFM introduces PubMedGPT 2.7B. Stanford University 
          https://hai.stanford.edu/news/stanford-crfm-introduces-pubmedgpt-27b
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR19" id="ref-link-section-d77463470e936">19</a></sup> (MedQA)
                                                    and BioGPT<sup><a data-track="click"
                                                            data-track-action="reference anchor" data-track-label="link"
                                                            data-test="citation-ref" aria-label="Reference 21"
                                                            title="Luo, R. et al. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Brief. Bioinformatics 23, bbac49 (2022)."
                                                            href="/articles/s41586-023-06291-2#ref-CR21"
                                                            id="ref-link-section-d77463470e940">21</a></sup> (PubMedQA).
                                                    The
                                                    percentage accuracy is shown above each column.</p>
                                            </div>
                                        </div>
                                        <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                data-test="article-link" data-track="click" data-track-label="button"
                                                data-track-action="view figure"
                                                href="/articles/s41586-023-06291-2/figures/2"
                                                data-track-dest="link:Figure2 Full size image"
                                                aria-label="Full size image figure 2" rel="nofollow"><span>Full size
                                                    image</span><svg width="16" height="16" focusable="false" role="img"
                                                    aria-hidden="true" class="u-icon">
                                                    <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                        xlink:href="#icon-chevron-right"></use>
                                                </svg></a></div>
                                    </figure>
                                </div>
                                <h3 class="c-article__sub-heading" id="Sec4">State of the art on MedQA</h3>
                                <p>On the MedQA dataset consisting of USMLE-style questions with 4 options, our
                                    Flan-PaLM
                                    540B model achieved a multiple-choice question accuracy of 67.6%, surpassing the
                                    DRAGON
                                    model<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 18"
                                            title="Yasunaga, M. et al. Deep bidirectional language-knowledge graph pretraining. Preprint at 
          https://doi.org/10.48550/arXiv.2210.09338
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR18" id="ref-link-section-d77463470e959">18</a></sup> by
                                    20.1%.</p>
                                <p>Concurrent with our study, PubMedGPT, a 2.7B model trained exclusively on biomedical
                                    abstracts and papers, was released<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 19" title="Bolton, E. et al. Stanford CRFM introduces PubMedGPT 2.7B. Stanford University 
          https://hai.stanford.edu/news/stanford-crfm-introduces-pubmedgpt-27b
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR19" id="ref-link-section-d77463470e966">19</a></sup>.
                                    PubMedGPT achieved a performance of 50.3% on MedQA questions with 4 options. To the
                                    best
                                    of our knowledge, this is the state-of-the-art on MedQA, and Flan-PaLM 540B exceeded
                                    this by 17.3%. Extended Data Table <a data-track="click" data-track-label="link"
                                        data-track-action="table anchor" href="/articles/s41586-023-06291-2#Tab4">4</a>
                                    compares the best performing models on this dataset. On the more difficult set of
                                    questions with 5 options, our model obtained an accuracy score of 62.0%.</p>
                                <h3 class="c-article__sub-heading" id="Sec5">Performance on MedMCQA and PubMedQA</h3>
                                <p>On the MedMCQA dataset, consisting of medical entrance exam questions from India,
                                    Flan-PaLM 540B reached a performance of 57.6% on the development-test set. This
                                    exceeds
                                    the previous state-of-the-art result of 52.9% by the Galactica model<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 20"
                                            title="Taylor, R. et al. Galactica: a large language model for science. Preprint at 
          https://doi.org/10.48550/arXiv.2211.09085
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR20" id="ref-link-section-d77463470e981">20</a></sup>.</p>
                                <p>Similarly, on the PubMedQA dataset, our model achieved an accuracy of 79.0%,
                                    outperforming the previous state-of-the-art BioGPT model<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 21"
                                            title="Luo, R. et al. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Brief. Bioinformatics 23, bbac49 (2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR21"
                                            id="ref-link-section-d77463470e988">21</a></sup> by 0.8% (Fig. <a
                                        data-track="click" data-track-label="link" data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig2">2</a>). Although this improvement may
                                    seem
                                    small compared to those for the MedQA and MedMCQA datasets, the single-rater human
                                    performance on PubMedQA<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 3"
                                            title="Jin, D. et al. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. Appl. Sci. 11, 6421 (2021)."
                                            href="/articles/s41586-023-06291-2#ref-CR3"
                                            id="ref-link-section-d77463470e995">3</a></sup> is 78.0%, indicating that
                                    there
                                    may be an inherent ceiling to the maximum possible performance on this task.</p>
                                <h3 class="c-article__sub-heading" id="Sec6">Performance on MMLU clinical topics</h3>
                                <p>The MMLU dataset contains multiple-choice questions from several clinical knowledge,
                                    medicine and biology-related topics. These include anatomy, clinical knowledge,
                                    professional medicine, human genetics, college medicine and college biology.
                                    Flan-PaLM
                                    540B achieved state-of-the-art performance on all these subsets, outperforming
                                    strong
                                    LLMs such as PaLM, Gopher, Chinchilla, BLOOM, OPT and Galactica. In particular, on
                                    the
                                    professional medicine and clinical knowledge subsets, Flan-PaLM 540B achieved a
                                    state-of-the-art accuracy of 83.8% and 80.4%, respectively. Extended Data Fig. <a
                                        data-track="click" data-track-label="link" data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig8">2</a> summarizes the results, providing
                                    comparisons with other LLMs where available<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 20" title="Taylor, R. et al. Galactica: a large language model for science. Preprint at 
          https://doi.org/10.48550/arXiv.2211.09085
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR20" id="ref-link-section-d77463470e1010">20</a></sup>.</p>
                            </div>
                        </div>
                    </section>
                    <section data-title="Ablations">
                        <div class="c-article-section" id="Sec7-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="Sec7">Ablations</h2>
                            <div class="c-article-section__content" id="Sec7-content">
                                <p>We performed several ablations on three of the multiple-choice datasets—MedQA,
                                    MedMCQA,
                                    and PubMedQA—to better understand our results and identify the key components
                                    contributing to Flan-PaLM’s performance.</p>
                                <h3 class="c-article__sub-heading" id="Sec8">Instruction tuning improves performance
                                </h3>
                                <p>Across all model sizes, we observed that the instruction-tuned Flan-PaLM model
                                    outperformed the baseline PaLM model on MedQA, MedMCQA and PubMedQA datasets. The
                                    models
                                    were few-shot-prompted in these experiments using the prompt text detailed in
                                    Supplementary Information, section&nbsp;<a data-track="click"
                                        data-track-label="link" data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">11</a>. The detailed results are
                                    summarized in Supplementary Table <a data-track="click" data-track-label="link"
                                        data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">6</a>. The improvements were most
                                    prominent in the PubMedQA dataset where the 8B Flan-PaLM model outperformed the
                                    baseline
                                    PaLM model by over 30%. Similar strong improvements were also observed in the case
                                    of
                                    62B and 540B variants. These results demonstrate the strong benefits of instruction
                                    fine-tuning. Similar results on MMLU clinical topics are reported in Supplementary
                                    Information, section&nbsp;<a data-track="click" data-track-label="link"
                                        data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">4</a>.</p>
                                <p>We have not yet completed a thorough analysis of the effect of instruction prompt
                                    tuning
                                    on multiple-choice accuracy; in this section, our analysis is of Flan-PaLM, not
                                    Med-PaLM. Med-PaLM (instruction prompt-tuned Flan-PaLM) was developed to improve the
                                    long-form generation results of Flan-PaLM presented in ‘Human evaluation results’ by
                                    better aligning the model to the medical domain. However, given the success of
                                    domain-agnostic instruction tuning for answering multiple-choice questions,
                                    in-domain
                                    instruction prompt tuning appears promising, and we present a preliminary result in
                                    Extended Data Table <a data-track="click" data-track-label="link"
                                        data-track-action="table anchor" href="/articles/s41586-023-06291-2#Tab5">5</a>
                                    and
                                    further describe this experiment in Supplementary Information, section&nbsp;<a
                                        data-track="click" data-track-label="link"
                                        data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">5</a>.</p>
                                <h3 class="c-article__sub-heading" id="Sec9">Scaling improves performance on medical
                                    question answering</h3>
                                <p>A related observation from Supplementary Table <a data-track="click"
                                        data-track-label="link" data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">6</a> was the strong performance
                                    improvements obtained from scaling the model from 8B to 62B and 540B. We observed an
                                    improvement of approximately 2× in performance when scaling the model from 8B to
                                    540B in
                                    both PaLM and Flan-PaLM. These improvements were more pronounced in the MedQA and
                                    MedMCQA datasets. In particular, for the Flan-PaLM model, the 540B variant
                                    outperformed
                                    the 62B variant by more than 14% and the 8B variant by more than 24%. Given these
                                    results and the strong performance of the Flan-PaLM 540B model, we built on this
                                    model
                                    for downstream experiments and ablations. The scaling plots are provided in
                                    Supplementary Information, section&nbsp;<a data-track="click"
                                        data-track-label="link" data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">7</a>.</p>
                                <h3 class="c-article__sub-heading" id="Sec10">COT prompting</h3>
                                <p>Supplementary Table <a data-track="click" data-track-label="link"
                                        data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">2</a> summarizes the results from
                                    using
                                    COT prompting and provides a comparison with the few-shot prompting strategy using
                                    the
                                    Flan-PaLM 540B model. We did not observe improvements using COT over the standard
                                    few-shot prompting strategy across the MedQA, MedMCQA and PubMedQA multiple-choice
                                    datasets. This may be owing to the existence of many possible chain-of-thought
                                    reasoning
                                    paths towards a particular answer, and sampling one path may not produce the most
                                    accurate result. This motivated the experiments with self-consistency, as discussed
                                    below. The COT prompts used are summarized in Supplementary Information,
                                    section&nbsp;<a data-track="click" data-track-label="link"
                                        data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">12</a>. In addition, we also explored
                                    the
                                    use of non-medical COT prompts. The results presented in Supplementary Information,
                                    section&nbsp;<a data-track="click" data-track-label="link"
                                        data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">6</a> suggest that COT prompting is
                                    effective in priming the model to solve these types of problems rather than adding
                                    new
                                    knowledge to the model.</p>
                                <h3 class="c-article__sub-heading" id="Sec11">Self-consistency improves multiple-choice
                                    performance</h3>
                                <p>It has been shown that self-consistency can be of use when COT prompting hurts
                                    performance<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 17"
                                            title="Wang, X. et al. Self-consistency improves chain of thought reasoning in language models. Preprint at 
          https://doi.org/10.48550/arXiv.2203.11171
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR17" id="ref-link-section-d77463470e1088">17</a></sup>;
                                    previous work showed considerable improvements on arithmetic and common-sense
                                    reasoning
                                    tasks. We applied self-consistency to MultiMedQA, fixing the number of
                                    chain-of-thought
                                    answer explanation paths (decodes) to 11 for each of three multiple-choice datasets.
                                    We
                                    then marginalized over the different decodes to select the most consistent answer.
                                    Using
                                    this strategy, we observed considerable improvements over the standard few-shot
                                    prompting strategy for the Flan-PaLM 540B model on the MedQA and MedMCQA datasets.
                                    In
                                    particular, for the MedQA dataset we observed an improvement of more than 7% with
                                    self-consistency. However, self-consistency led to a drop in performance for the
                                    PubMedQA dataset. The results are summarized in Supplementary Table <a
                                        data-track="click" data-track-label="link"
                                        data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">3</a>. We further provide example
                                    responses from the Flan-PaLM 540B model for MedQA in Extended Data Table <a
                                        data-track="click" data-track-label="link" data-track-action="table anchor"
                                        href="/articles/s41586-023-06291-2#Tab6">6</a>.</p>
                                <h3 class="c-article__sub-heading" id="Sec12">Uncertainty and selective prediction</h3>
                                <p>LLMs are capable of long, coherent, and complex generations. However, they can also
                                    generate factually inaccurate statements. In medical settings in particular, such
                                    failure modes need to be carefully vetted, and in real-world applications,
                                    generations
                                    that are unlikely to be true should be withheld. Instead, we may want to defer to
                                    other
                                    information sources or experts when needed. One solution is therefore for LLMs to
                                    communicate uncertainty estimates along with their responses.</p>
                                <p>Although uncertainty measures over LLM output sequences remains an open area of
                                    research<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 22"
                                            title="Lin, S., Hilton, J. &amp; Evans, O. Teaching models to express their uncertainty in words. Preprint at 
          https://doi.org/10.48550/arXiv.2205.14334
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR22" id="ref-link-section-d77463470e1109">22</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 23"
                                            title="Kadavath, S. et al. Language models (mostly) know what they know. Preprint at 
          https://doi.org/10.48550/arXiv.2207.05221
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR23" id="ref-link-section-d77463470e1112">23</a></sup>, we
                                    explored a simple proxy as an initial approach to measuring the relationship between
                                    LLM
                                    uncertainty and statement accuracy. We created a selective prediction task<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 24"
                                            title="Tran, D. et al. Plex: towards reliability using pretrained large model extensions. Preprint at 
          https://doi.org/10.48550/arXiv.2207.07411
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR24" id="ref-link-section-d77463470e1116">24</a></sup>, using
                                    the number of decodes matching a given answer from self-consistency as a measure of
                                    uncertainty, and used it to withhold the answer if the model was not appropriately
                                    confident. We performed the experiment using 41 decodes from the Flan-PaLM 540B
                                    model
                                    with chain-of-thought prompting and self-consistency. We observe that as the
                                    deferring
                                    fraction increases (that is, as a higher confidence is required to provide a
                                    prediction), the performance of the model on MedQA improves, reaching an accuracy of
                                    up
                                    to 82.5% at a deferring fraction of 0.45 (Fig. <a data-track="click"
                                        data-track-label="link" data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig3">3</a>). This suggests that our measure
                                    of
                                    response uncertainty may be reasonable and that LLMs seem to encode uncertainty
                                    about
                                    their knowledge in the medical domain. However, more research is needed beyond this
                                    preliminary analysis.</p>
                                <div class="c-article-section__figure js-c-reading-companion-figures-item"
                                    data-test="figure" data-container-section="figure" id="figure-3"
                                    data-title="Selective prediction analysis.">
                                    <figure>
                                        <figcaption><b id="Fig3" class="c-article-section__figure-caption"
                                                data-test="figure-caption-text">Fig. 3: Selective prediction
                                                analysis.</b>
                                        </figcaption>
                                        <div class="c-article-section__figure-content">
                                            <div class="c-article-section__figure-item"><a
                                                    class="c-article-section__figure-link" data-test="img-link"
                                                    data-track="click" data-track-label="image"
                                                    data-track-action="view figure"
                                                    href="/articles/s41586-023-06291-2/figures/3" rel="nofollow">
                                                    <picture>
                                                        <source type="image/webp"
                                                            srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig3_HTML.png?as=webp">
                                                        <img aria-describedby="Fig3"
                                                            src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig3_HTML.png"
                                                            alt="figure 3" loading="lazy" width="685" height="430">
                                                    </picture>
                                                </a></div>
                                            <div class="c-article-section__figure-description"
                                                data-test="bottom-caption" id="figure-3-desc">
                                                <p>Analysis of deferral behaviour of the Flan-PaLM 540B model with
                                                    self-consistency. We observe that if we defer more frequently using
                                                    an
                                                    uncertainty threshold based on self-consistency, the model becomes
                                                    increasingly accurate on questions it does not defer.</p>
                                            </div>
                                        </div>
                                        <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                data-test="article-link" data-track="click" data-track-label="button"
                                                data-track-action="view figure"
                                                href="/articles/s41586-023-06291-2/figures/3"
                                                data-track-dest="link:Figure3 Full size image"
                                                aria-label="Full size image figure 3" rel="nofollow"><span>Full size
                                                    image</span><svg width="16" height="16" focusable="false" role="img"
                                                    aria-hidden="true" class="u-icon">
                                                    <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                        xlink:href="#icon-chevron-right"></use>
                                                </svg></a></div>
                                    </figure>
                                </div>
                                <h3 class="c-article__sub-heading" id="Sec13">Human evaluation results</h3>
                                <p>We randomly selected 100 questions from HealthSearchQA, 20 questions from LiveQA, and
                                    20
                                    questions from MedicationQA as a smaller long-form answer benchmark for detailed
                                    human
                                    evaluation. These questions reflect real-world consumer queries for medical
                                    information.
                                    These selected questions were disjoint from exemplars used for instruction prompt
                                    tuning
                                    to produce Med-PaLM.</p>
                                <p>We asked a panel of clinicians to generate expert reference answers to these
                                    questions.
                                    We then produced answers using Flan-PaLM and Med-PaLM (both 540B models). A few
                                    qualitative examples of these questions and the corresponding Med-PaLM responses are
                                    shown in Extended Data Table <a data-track="click" data-track-label="link"
                                        data-track-action="table anchor" href="/articles/s41586-023-06291-2#Tab7">7</a>.
                                    The
                                    three sets of answers were evaluated by a different panel of clinicians along the
                                    axes
                                    presented in Extended Data Table <a data-track="click" data-track-label="link"
                                        data-track-action="table anchor" href="/articles/s41586-023-06291-2#Tab2">2</a>,
                                    without revealing the source of answers. One clinician evaluated each answer. To
                                    reduce
                                    the effect of variation across clinicians on generalizability of our findings, our
                                    panel
                                    consisted of nine clinicians (based in the USA, UK and India). We used the
                                    non-parametric bootstrap to estimate any significant variation in the results, where
                                    1,000 bootstrap replicas were used to produce a distribution for each set, and we
                                    used
                                    the 95% bootstrap percentile interval to assess variations. These results are
                                    described
                                    in detail below and in Supplementary Information, section&nbsp;<a data-track="click"
                                        data-track-label="link" data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">10</a>, with visualizations in Figs.
                                    <a data-track="click" data-track-label="link" data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig4">4</a>–<a data-track="click"
                                        data-track-label="link" data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig6">6</a>.</p>
                                <div class="c-article-section__figure js-c-reading-companion-figures-item"
                                    data-test="figure" data-container-section="figure" id="figure-4"
                                    data-title="Clinician evaluation of answers.">
                                    <figure>
                                        <figcaption><b id="Fig4" class="c-article-section__figure-caption"
                                                data-test="figure-caption-text">Fig. 4: Clinician evaluation of
                                                answers.</b>
                                        </figcaption>
                                        <div class="c-article-section__figure-content">
                                            <div class="c-article-section__figure-item"><a
                                                    class="c-article-section__figure-link" data-test="img-link"
                                                    data-track="click" data-track-label="image"
                                                    data-track-action="view figure"
                                                    href="/articles/s41586-023-06291-2/figures/4" rel="nofollow">
                                                    <picture>
                                                        <source type="image/webp"
                                                            srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig4_HTML.png?as=webp">
                                                        <img aria-describedby="Fig4"
                                                            src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig4_HTML.png"
                                                            alt="figure 4" loading="lazy" width="685" height="684">
                                                    </picture>
                                                </a></div>
                                            <div class="c-article-section__figure-description"
                                                data-test="bottom-caption" id="figure-4-desc">
                                                <p><b>a</b>–<b>f</b>, Clinicians were asked to rate answers to questions
                                                    in
                                                    the HealthSearchQA, LiveQA and MedicationQA datasets for agreement
                                                    with
                                                    scientific and clinical consensus (<b>a</b>), the presence of
                                                    incorrect
                                                    content (<b>b</b>), the omission of content (<b>c</b>), the extent
                                                    of
                                                    possible harm (<b>d</b>), the likelihood of harm (<b>e</b>) and
                                                    possible
                                                    bias in answers (<b>f</b>). We compare answers from Flan-PaLM,
                                                    Med-PaLM
                                                    and clinicians. Across all axes, answers from clinicians were judged
                                                    to
                                                    be better than those from Flan-PaLM. Med-PaLM answers were
                                                    substantially
                                                    better than Flan-PaLM answers across alignment with scientific
                                                    consensus, harm, missing content and bias, often comparing
                                                    favourably
                                                    with answers from clinicians, demonstrating the value of instruction
                                                    prompt tuning for alignment to the medical domain. The evaluation
                                                    involves 140 questions, each rated by a single clinician. We used
                                                    the
                                                    non-parametric bootstrap to estimate any significant variation in
                                                    the
                                                    results, with 1,000 bootstrap replicas used to produce a
                                                    distribution
                                                    for each set. We used the 95% bootstrap percentile interval to
                                                    assess
                                                    variations. Detailed results with intervals are presented in
                                                    Supplementary Information, section&nbsp;<a data-track="click"
                                                        data-track-label="link"
                                                        data-track-action="supplementary material anchor"
                                                        href="/articles/s41586-023-06291-2#MOESM1">10</a>.</p>
                                            </div>
                                        </div>
                                        <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                data-test="article-link" data-track="click" data-track-label="button"
                                                data-track-action="view figure"
                                                href="/articles/s41586-023-06291-2/figures/4"
                                                data-track-dest="link:Figure4 Full size image"
                                                aria-label="Full size image figure 4" rel="nofollow"><span>Full size
                                                    image</span><svg width="16" height="16" focusable="false" role="img"
                                                    aria-hidden="true" class="u-icon">
                                                    <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                        xlink:href="#icon-chevron-right"></use>
                                                </svg></a></div>
                                    </figure>
                                </div>
                                <div class="c-article-section__figure js-c-reading-companion-figures-item"
                                    data-test="figure" data-container-section="figure" id="figure-5"
                                    data-title="Evaluation of comprehension, retrieval and reasoning capabilities by clinicians.">
                                    <figure>
                                        <figcaption><b id="Fig5" class="c-article-section__figure-caption"
                                                data-test="figure-caption-text">Fig. 5: Evaluation of comprehension,
                                                retrieval and reasoning capabilities by clinicians.</b></figcaption>
                                        <div class="c-article-section__figure-content">
                                            <div class="c-article-section__figure-item"><a
                                                    class="c-article-section__figure-link" data-test="img-link"
                                                    data-track="click" data-track-label="image"
                                                    data-track-action="view figure"
                                                    href="/articles/s41586-023-06291-2/figures/5" rel="nofollow">
                                                    <picture>
                                                        <source type="image/webp"
                                                            srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig5_HTML.png?as=webp">
                                                        <img aria-describedby="Fig5"
                                                            src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig5_HTML.png"
                                                            alt="figure 5" loading="lazy" width="685" height="207">
                                                    </picture>
                                                </a></div>
                                            <div class="c-article-section__figure-description"
                                                data-test="bottom-caption" id="figure-5-desc">
                                                <p><b>a</b>,<b>b</b>, Evaluation of correctness (<b>a</b>) and
                                                    incorrectness
                                                    (<b>b</b>) of reading comprehension, recall of knowledge and
                                                    reasoning
                                                    steps. The results indicate a gap between Flan-PaLM and clinicians,
                                                    and
                                                    show that Med-PaLM is able to substantially reduce the gap. The
                                                    evaluation involves 140 questions, each rated by a single clinician.
                                                    We
                                                    used the non-parametric bootstrap to estimate any significant
                                                    variation
                                                    in the results, with 1,000 bootstrap replicas used to produce a
                                                    distribution for each set. We used the 95% bootstrap percentile
                                                    interval
                                                    to assess variations.</p>
                                            </div>
                                        </div>
                                        <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                data-test="article-link" data-track="click" data-track-label="button"
                                                data-track-action="view figure"
                                                href="/articles/s41586-023-06291-2/figures/5"
                                                data-track-dest="link:Figure5 Full size image"
                                                aria-label="Full size image figure 5" rel="nofollow"><span>Full size
                                                    image</span><svg width="16" height="16" focusable="false" role="img"
                                                    aria-hidden="true" class="u-icon">
                                                    <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                        xlink:href="#icon-chevron-right"></use>
                                                </svg></a></div>
                                    </figure>
                                </div>
                                <div class="c-article-section__figure js-c-reading-companion-figures-item"
                                    data-test="figure" data-container-section="figure" id="figure-6"
                                    data-title="Lay user assessment of answers.">
                                    <figure>
                                        <figcaption><b id="Fig6" class="c-article-section__figure-caption"
                                                data-test="figure-caption-text">Fig. 6: Lay user assessment of
                                                answers.</b>
                                        </figcaption>
                                        <div class="c-article-section__figure-content">
                                            <div class="c-article-section__figure-item"><a
                                                    class="c-article-section__figure-link" data-test="img-link"
                                                    data-track="click" data-track-label="image"
                                                    data-track-action="view figure"
                                                    href="/articles/s41586-023-06291-2/figures/6" rel="nofollow">
                                                    <picture>
                                                        <source type="image/webp"
                                                            srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig6_HTML.png?as=webp">
                                                        <img aria-describedby="Fig6"
                                                            src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig6_HTML.png"
                                                            alt="figure 6" loading="lazy" width="685" height="247">
                                                    </picture>
                                                </a></div>
                                            <div class="c-article-section__figure-description"
                                                data-test="bottom-caption" id="figure-6-desc">
                                                <p><b>a</b>,<b>b</b>, Lay user assessment of answers, addressing
                                                    relevance
                                                    to the intent of the query (<b>a</b>) and helpfulness (<b>b</b>).
                                                    Med-PaLM answers are more likely to address the intent of users and
                                                    be
                                                    more helpful than Flan-PaLM answers, but they remain inferior to
                                                    those
                                                    provided by clinicians. The evaluation involves 140 questions, each
                                                    rated by a single non-expert lay user. We used the non-parametric
                                                    bootstrap to estimate any significant variation in the results,
                                                    where
                                                    1,000 bootstrap replicas were used to produce a distribution for
                                                    each
                                                    set. We used the 95% bootstrap percentile interval to assess
                                                    variations.
                                                </p>
                                            </div>
                                        </div>
                                        <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                data-test="article-link" data-track="click" data-track-label="button"
                                                data-track-action="view figure"
                                                href="/articles/s41586-023-06291-2/figures/6"
                                                data-track-dest="link:Figure6 Full size image"
                                                aria-label="Full size image figure 6" rel="nofollow"><span>Full size
                                                    image</span><svg width="16" height="16" focusable="false" role="img"
                                                    aria-hidden="true" class="u-icon">
                                                    <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                        xlink:href="#icon-chevron-right"></use>
                                                </svg></a></div>
                                    </figure>
                                </div>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec14">Scientific
                                    consensus</h4>
                                <p>We aimed to understand how the answers related to current consensus in the clinical
                                    and
                                    scientific community. We judged clinicians’ answers to be aligned with the
                                    scientific
                                    consensus in 92.9% of questions, whereas Flan-PaLM was found to be in agreement with
                                    the
                                    scientific consensus in only 61.9% of answers (Fig. <a data-track="click"
                                        data-track-label="link" data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig4">4</a>). For other questions, answers
                                    were
                                    either opposed to consensus, or no consensus existed. This suggested that generic
                                    instruction tuning on its own was not sufficient to produce scientific and
                                    clinically
                                    grounded answers. However, 92.6% of Med-PaLM answers were judged to be in accordance
                                    with the scientific consensus, showcasing the strength of instruction prompt tuning
                                    as
                                    an alignment technique to produce scientifically grounded answers.</p>
                                <p>We note that since PaLM, Flan-PaLM, and Med-PaLM were trained using corpora of web
                                    documents, books, Wikipedia, code, natural language tasks, and medical tasks at a
                                    given
                                    point of time, one potential limitation of these models is that they can reflect the
                                    scientific consensus of the past instead of today. This is not a commonly observed
                                    failure mode for Med-PaLM today, but this motivates future work in continual
                                    learning of
                                    LLMs and retrieval from a continuously evolving corpus.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec15">
                                    Comprehension,
                                    retrieval and reasoning capabilities</h4>
                                <p>We sought to understand the medical comprehension, knowledge retrieval and reasoning
                                    capabilities of Med-PaLM. We asked a panel of clinicians to rate whether answers
                                    contained any (one or more example of) evidence of correct or incorrect medical
                                    reading
                                    comprehension, medical knowledge retrieval and medical reasoning capabilities, using
                                    the
                                    same approach as CHARD<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 25" title="Feng, S. Y., Khetan, V., Sacaleanu, B., Gershman, A. &amp; Hovy, E. CHARD: clinical health-aware reasoning across dimensions for text generation models. Preprint at 
          https://doi.org/10.48550/arXiv.2210.04191
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR25" id="ref-link-section-d77463470e1299">25</a></sup>.
                                    Correct and incorrect evidence were assessed in parallel because it is possible that
                                    a
                                    single long-form answer may contain evidence of both correct and incorrect
                                    comprehension, retrieval and reasoning.</p>
                                <p>Answers generated by experts were again superior to those of Flan-PaLM, although
                                    performance was improved by instruction prompt tuning for Med-PaLM (Fig. <a
                                        data-track="click" data-track-label="link" data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig5">5</a>). This trend was observed for all
                                    six
                                    sub-questions used to evaluate these capabilities. For example, for evidence of
                                    correct
                                    retrieval of medical knowledge, we found that clinician answers scored 97.8%,
                                    whereas
                                    Flan-PaLM scored 76.3%. However, the instruction prompt-tuned Med-PaLM model scored
                                    95.4%, reducing the performance gap with clinicians.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec16">Incorrect or
                                    missing content</h4>
                                <p>The goal of this evaluation was to understand the completeness and correctness of the
                                    generated answers by assessing whether an answer omits any information that it
                                    should
                                    not omit, or whether the answer contains any content that it should not. Where there
                                    was
                                    deemed to be missing or omitted content, the rater was asked whether it was of great
                                    or
                                    little potential clinical importance.</p>
                                <p>Again, the clinician-generated answers were judged to be superior (Fig. <a
                                        data-track="click" data-track-label="link" data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig4">4</a>). The answers from clinicians
                                    showed
                                    evidence of inappropriate or incorrect content in 1.4% of cases, compared with 16.1%
                                    for
                                    Flan-PaLM. Instruction prompt tuning seemed to degrade performance, with 18.7% of
                                    the
                                    Med-PaLM answers judged to contain inappropriate or incorrect content.</p>
                                <p>By contrast, instruction prompt tuning improved model performance with respect to
                                    omission of important information. Flan-PaLM answers were judged to omit important
                                    information in 47.6% of answers, whereas Med-PaLM omitted important information in
                                    15.3%
                                    of the answers, decreasing the gap with clinicians, whose answers were judged to
                                    have
                                    missing information in 11.1% of the cases. Several qualitative examples are shown in
                                    Extended Data Table <a data-track="click" data-track-label="link"
                                        data-track-action="table anchor" href="/articles/s41586-023-06291-2#Tab8">8</a>,
                                    suggesting that answers from LLMs may be able to complement and complete physician
                                    responses to patient queries in future use cases.</p>
                                <p>One potential explanation of these observations is that instruction prompt tuning
                                    teaches
                                    the Med-PaLM model to generate more detailed answers than the Flan-PaLM model,
                                    reducing
                                    the omission of important information. However, a longer answer also increases the
                                    risk
                                    of introducing incorrect content.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec17">Possible
                                    extent
                                    and likelihood of harm</h4>
                                <p>We sought to identify the severity and likelihood of potential harm based on people
                                    acting on the generated answers. We asked raters to assume that the output of models
                                    might lead to actions by clinicians, consumers or patients, and to estimate the
                                    possible
                                    severity and likelihood of physical or mental health-related harms that might
                                    result. We
                                    based the options for selection by raters on the Agency for Healthcare Research and
                                    Quality (AHRQ) common formats<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 26"
                                            title="Williams, T., Szekendi, M., Pavkovic, S., Clevenger, W. &amp; Cerese, J. The reliability of ahrq common format harm scales in rating patient safety events. J. Patient Saf. 11, 52–59 (2015)."
                                            href="/articles/s41586-023-06291-2#ref-CR26"
                                            id="ref-link-section-d77463470e1341">26</a></sup>, which presents options to
                                    assign severity of harm among death, severe or life-threatening injury, moderate
                                    harm,
                                    mild harm or no harm. We acknowledge that this definition of harm is more typically
                                    used
                                    in the context of analysing harms incurred during healthcare delivery and that even
                                    in
                                    such settings (where the context for harms occurring is known with considerably
                                    greater
                                    specificity) there is frequently substantial variation in physician estimation of
                                    harm
                                    severity<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 27"
                                            title="Walsh, K. E. et al. Measuring harm in healthcare: optimizing adverse event review. Med. Care 55, 436 (2017)."
                                            href="/articles/s41586-023-06291-2#ref-CR27"
                                            id="ref-link-section-d77463470e1345">27</a></sup>. The validity of the AHRQ
                                    scale cannot therefore be assumed to extend to our context, where our rater outputs
                                    should be regarded as subjective estimates because our work was not grounded in a
                                    specific intended use and sociocultural context.</p>
                                <p>Despite the broad definition and subjectivity of the ratings, we observed that
                                    instruction prompt tuning produced safer answers that reduced both estimated
                                    likelihood
                                    and severity. Whereas 29.7% of the Flan-PaLM responses were judged as potentially
                                    leading to harm, this number fell to 5.9% for Med-PaLM, similar to the result for
                                    clinician-generated answers (5.7%).</p>
                                <p>Similarly, on the likelihood of harm axes, instruction prompt tuning enabled Med-PaLM
                                    answers to match the expert generated answers (Fig. <a data-track="click"
                                        data-track-label="link" data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig4">4</a>).</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec18">Bias for
                                    medical
                                    demographics</h4>
                                <p>The final axis for clinician answer evaluation was bias. For this pilot evaluation,
                                    we
                                    sought to understand whether the answer contained any information that was
                                    inaccurate or
                                    inapplicable for a particular demographic. Specifically for each response, reviewers
                                    were asked “Does the answer provided contain any information that is inapplicable or
                                    inaccurate for one particular patient-group or demographic? For example, is the
                                    answer
                                    applicable only to patients of a particular sex where patients of another sex might
                                    require different information?”, to which they provided a yes or no response. For
                                    this
                                    definition of bias, Flan-PaLM answers were found to contain biased information in
                                    7.9%
                                    of the cases (Fig. <a data-track="click" data-track-label="link"
                                        data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig4">4)</a>.
                                    However, this number decreased to 0.8% for Med-PaLM, comparing favourably with the
                                    experts, whose answers were judged to contain evidence of bias in 1.4% of cases.</p>
                                <p>It should be noted that most of the questions were framed neutrally and did not
                                    contain
                                    specific demographic inferences. This initial approach to evaluating bias is limited
                                    and
                                    does not serve as a comprehensive assessment of potential harms, fairness or equity.
                                    Further fairness and equity considerations are discussed in ‘Fairness and equity
                                    considerations’.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec19">Lay user
                                    assessment</h4>
                                <p>Beyond expert evaluation, we also asked a panel of five non-experts in the domain
                                    (laypeople without a medical background, based in India) to assess the answers. The
                                    results are summarized in Fig. <a data-track="click" data-track-label="link"
                                        data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig6">6</a>.
                                    Whereas Flan-PaLM answers were judged to be helpful in only 60.6% of the cases, this
                                    increased to 80.3% for Med-PaLM answers. However, this remained inferior to the
                                    answers
                                    given by clinicians, which were judged to be helpful 91.1% of the time. Similarly,
                                    Flan-PaLM answers were judged as directly addressing the intent of the user’s
                                    question
                                    in 90.8% of cases. This increased to 94.4% for Med-PaLM, whereas the
                                    clinician-generated
                                    answers were judged as directly addressing intent in 95.9% of cases.</p>
                                <p>The lay user evaluation further demonstrated the benefits of instruction prompt
                                    tuning to
                                    produce answers that are helpful to users and shows that considerable work remains
                                    to be
                                    done to approximate the quality of outputs provided by human clinicians.</p>
                            </div>
                        </div>
                    </section>
                    <section data-title="Discussion">
                        <div class="c-article-section" id="Sec20-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="Sec20">Discussion</h2>
                            <div class="c-article-section__content" id="Sec20-content">
                                <p>Our results suggest that the strong performance in answering medical questions may be
                                    an
                                    emergent ability<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 28"
                                            title="Wei, J. et al. Emergent abilities of large language models. Preprint at 
          https://doi.org/10.48550/arXiv.2206.07682
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR28" id="ref-link-section-d77463470e1396">28</a></sup> of
                                    LLMs combined with effective instruction prompt tuning.</p>
                                <p>We observed strong performance as a result of scaling, with accuracy improving by
                                    approximately 2 times as we scaled the PaLM models from 8B to 540B. The performance
                                    of
                                    PaLM 8B on MedQA was only slightly better than random performance. Accuracy improved
                                    by
                                    more than 30% for PaLM 540B, demonstrating the effectiveness of scaling for
                                    answering
                                    medical questions. We observed similar improvements for the MedMCQA and PubMedQA
                                    datasets. Further, instruction fine-tuning was also effective, with Flan-PaLM models
                                    performing better than the PaLM models across all model size variants on all the
                                    multiple-choice datasets.</p>
                                <p>It is likely that the PaLM pre-training corpus included significant medical-related
                                    content, and one possible explanation for the strong performance of the 540B model
                                    is
                                    that the model has memorized the MultiMedQA evaluation datasets. In Supplementary
                                    Information, section&nbsp;<a data-track="click" data-track-label="link"
                                        data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">1</a>, we analysed the overlap
                                    between
                                    Med-PaLM’s responses to MultiMedQA consumer questions and the PaLM training corpus
                                    and
                                    observed no overlap. We also assessed the overlap between MultiMedQA multiple-choice
                                    questions and the training corpus, observing minimal overlap (Supplementary Table <a
                                        data-track="click" data-track-label="link"
                                        data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">1</a>). Additionally, PaLM<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 1"
                                            title="Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at 
          https://doi.org/10.48550/arXiv.2204.02311
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR1" id="ref-link-section-d77463470e1412">1</a></sup> showed
                                    similar differences in performance of the PaLM 8B and 540B models when evaluating
                                    contaminated and clean test datasets (a contaminated dataset is one in which part of
                                    the
                                    test set is in the model pre-training corpus). These results suggested that
                                    memorization
                                    alone does not explain the strong performance observed by scaling up the models.</p>
                                <p>There have been several efforts to train language models on a biomedical corpus,
                                    especially on PubMed. These include BioGPT<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 21"
                                            title="Luo, R. et al. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Brief. Bioinformatics 23, bbac49 (2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR21"
                                            id="ref-link-section-d77463470e1419">21</a></sup> (355B), PubMedGPT<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 19"
                                            title="Bolton, E. et al. Stanford CRFM introduces PubMedGPT 2.7B. Stanford University 
          https://hai.stanford.edu/news/stanford-crfm-introduces-pubmedgpt-27b
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR19" id="ref-link-section-d77463470e1423">19</a></sup> (2.7B)
                                    and Galactica<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 20"
                                            title="Taylor, R. et al. Galactica: a large language model for science. Preprint at 
          https://doi.org/10.48550/arXiv.2211.09085
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR20" id="ref-link-section-d77463470e1427">20</a></sup>
                                    (120B). Our models were able to outperform these efforts on PubMedQA without any
                                    dataset-specific fine-tuning. Further, the benefits of scale and instruction
                                    fine-tuning
                                    were much more pronounced on the MedQA dataset, which can be considered
                                    out-of-domain
                                    for all these models. Given the results, we can conclude that medical answering
                                    capabilities (recall, reading comprehension and reasoning skills) improved with
                                    scale.
                                </p>
                                <p>However, our human evaluation results on consumer medical question-answering datasets
                                    clearly showed that scale alone was insufficient. Even strong LLMs such as Flan-PaLM
                                    can
                                    generate answers that are inappropriate for use in the safety-critical medical
                                    domain.
                                    However, the Med-PaLM results demonstrated that instruction prompt tuning is a data-
                                    and
                                    parameter-efficient alignment technique that is useful for improving factors related
                                    to
                                    accuracy, factuality, consistency, safety, harm and bias, helping to close the gap
                                    with
                                    clinical experts and bring these models closer to real-world clinical applications.
                                </p>
                            </div>
                        </div>
                    </section>
                    <section data-title="Limitations">
                        <div class="c-article-section" id="Sec21-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="Sec21">Limitations</h2>
                            <div class="c-article-section__content" id="Sec21-content">
                                <p>Our study demonstrates the potential of LLMs for encoding medical knowledge and for
                                    answering medical questions. Below we discuss limitations and outline directions for
                                    future research.</p>
                                <h3 class="c-article__sub-heading" id="Sec22">Expansion of MultiMedQA</h3>
                                <p>Although the MultiMedQA benchmark is diverse and contains questions from a variety of
                                    medical exam, medical research and consumer sources, it is by no means exhaustive.
                                    We
                                    plan to expand the benchmark in the future to include a larger variety of medical
                                    and
                                    scientific domains (such as biology) and formats.</p>
                                <p>A key challenge in clinical environments is eliciting information from patients and
                                    synthesizing findings into an assessment and plan. Multiple-choice
                                    question-answering
                                    tasks are inherently easier than this because they are often grounded in vignettes
                                    compiled by experts and selected to have a generally preferred answer. This is not
                                    true
                                    for all medical decisions. Developing benchmark tasks that reflect real-world
                                    clinical
                                    workflows is an important direction of future research.</p>
                                <p>Furthermore, we only considered English-language datasets in this study, and there is
                                    a
                                    pressing need to expand the scope of the benchmark to support multilingual
                                    evaluations.
                                </p>
                                <h3 class="c-article__sub-heading" id="Sec23">Key LLM capabilities for this setting</h3>
                                <p>Although Flan-PaLM was able to reach state-of-the-art performance on several
                                    multiple-choice medical question-answering benchmarks, our human evaluations clearly
                                    suggested that these models are not at clinician expert level on many clinically
                                    important axes. In order to bridge this gap, several new LLM capabilities need to be
                                    researched and developed including (1) grounding of the responses in authoritative
                                    medical sources and accounting for the time-varying nature of medical consensus; (2)
                                    ability to detect and communicate uncertainty effectively to the user; (3) ability
                                    to
                                    respond to queries in multiple languages; and (4) better alignment to the safety
                                    requirements of the medical domain.</p>
                                <h3 class="c-article__sub-heading" id="Sec24">Improving human evaluation</h3>
                                <p>The rating framework that we proposed for this study represents a promising pilot
                                    approach, but our chosen axes of evaluation were not exhaustive and were subjective
                                    in
                                    nature. For example, the concept of medical or scientific consensus is time-varying
                                    in
                                    nature and is reflective of current understandings of human health and disease and
                                    physiology, which are often coloured by discrimination in race or ethnicity, gender,
                                    age
                                    and ability<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 29"
                                            title="Kington, R. S. et al. Identifying credible sources of health information in social media: principles and attributes. NAM Perspectives 
          https://doi.org/10.31478%2F202107a
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR29" id="ref-link-section-d77463470e1473">29</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 30"
                                            title="Mandavilli, A. Medical journals blind to racism as health crisis, critics say. The New York Times 
          https://www.nytimes.com/2021/06/02/health/jama-racism-bauchner.html
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR30" id="ref-link-section-d77463470e1476">30</a></sup>.
                                    Furthermore, consensus often exists only for topics of relevance to certain groups
                                    (such
                                    as those who are greater in number and/or power) and consensus may be lacking for
                                    certain subpopulations. Additionally, the concept of harm may differ according to
                                    population. Expert assessment of harm may also vary on the basis of location, lived
                                    experience and cultural background. Differences in health literacy may have caused
                                    variability in ratings for both experts and lay users. Further research might test
                                    whether the perceived usefulness and harm of answers varied according to their
                                    understandability and actionability<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 31"
                                            title="Shoemaker, S. J., Wolf, M. S. &amp; Brach, C. Development of the patient education materials assessment tool (pemat): a new measure of understandability and actionability for print and audiovisual patient information. Patient Educ. Couns. 96, 395–403 (2014)."
                                            href="/articles/s41586-023-06291-2#ref-CR31"
                                            id="ref-link-section-d77463470e1480">31</a></sup>.</p>
                                <p>The number of model responses evaluated and the pool of clinicians and laypeople
                                    assessing them were limited, as our results were based on only a single clinician or
                                    layperson evaluating each response. This could be mitigated by inclusion of a
                                    considerably larger and intentionally diverse pool of human raters.</p>
                                <p>We worked with a panel of four qualified clinicians—with expertise in internal
                                    medicine,
                                    paediatrics, surgery and primary care, and based in the USA or the UK—to identify
                                    the
                                    best demonstration examples and craft few-shot prompts. Further research could
                                    expand
                                    the range of clinicians engaged in prompt construction and the selection of exemplar
                                    answers and thereby explore how variation in multiple axes of the types of clinician
                                    participating in this activity might affect LLM behaviour (such as clinician
                                    demographics, geography, specialism, lived experience and others).</p>
                                <p>The pilot framework that we developed could be advanced using best practices for the
                                    design and validation of rating instruments from health, social and behavioural
                                    research<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 32"
                                            title="Boateng, G. O., Neilands, T. B., Frongillo, E. A., Melgar-Quiñonez, H. R. &amp; Young, S. L. Best practices for developing and validating scales for health, social, and behavioral research: a primer. Front. Public Health 6, 149 (2018)."
                                            href="/articles/s41586-023-06291-2#ref-CR32"
                                            id="ref-link-section-d77463470e1493">32</a></sup>. This could entail finding
                                    additional rating items through participatory research and evaluation of rating
                                    items by
                                    domain experts and technology recipients for relevance, representativeness and
                                    technical
                                    quality. The inclusion of a substantially larger pool of human raters would also
                                    enable
                                    testing of instrument generalizability by ratifying the test dimensionality,
                                    test–retest
                                    reliability and validity<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 32"
                                            title="Boateng, G. O., Neilands, T. B., Frongillo, E. A., Melgar-Quiñonez, H. R. &amp; Young, S. L. Best practices for developing and validating scales for health, social, and behavioral research: a primer. Front. Public Health 6, 149 (2018)."
                                            href="/articles/s41586-023-06291-2#ref-CR32"
                                            id="ref-link-section-d77463470e1497">32</a></sup>. Further research could
                                    explore the independent influence of variations in lay raters’ education level,
                                    medical
                                    conditions, caregiver status, experience with healthcare, education level or other
                                    relevant factors on their ratings. The effect of variations in clinician raters’
                                    specialty, demographics, geography or other factors could be similarly explored.</p>
                                <h3 class="c-article__sub-heading" id="Sec25">Fairness and equity considerations</h3>
                                <p>As previously discussed, our approach to evaluating bias is limited as an assessment
                                    of
                                    fairness and equity-related harms. The use of LLMs to answer medical questions can
                                    cause
                                    harms that contribute to health disparities. These harms derive from several
                                    sources,
                                    including the presence of patterns in training data that reflect health inequities
                                    and
                                    algorithmic design choices<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 33"
                                            title="Hooker, S. Moving beyond “algorithmic bias is a data problem”. Patterns 2, 100241 (2021)."
                                            href="/articles/s41586-023-06291-2#ref-CR33"
                                            id="ref-link-section-d77463470e1510">33</a></sup>. This could lead to
                                    systems
                                    that produce differences in behaviour or performance across populations that result
                                    in
                                    downstream harms in medical decision-making<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 34"
                                            title="Chen, I. Y. et al. Ethical machine learning in healthcare. Annu. Rev. Biomed. Data Sci. 4, 123–144 (2021)."
                                            href="/articles/s41586-023-06291-2#ref-CR34"
                                            id="ref-link-section-d77463470e1514">34</a></sup> or reproduce racist
                                    misconceptions regarding the cause of health disparities<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 35"
                                            title="Eneanya, N. D. et al. Health inequities and the inappropriate use of race in nephrology. Nat. Rev. Nephrol. 18, 84–94 (2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR35"
                                            id="ref-link-section-d77463470e1518">35</a>,<a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 36"
                                            title="Vyas, L. G., Eisenstein, L. G. &amp; Jones, D. S. Hidden in plain sight-reconsidering the use of race correction in clinical algorithms. N. Engl. J. Med. 383, 874–882 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR36"
                                            id="ref-link-section-d77463470e1521">36</a></sup>.</p>
                                <p>The development of procedures for the evaluation of bias and fairness-related harms
                                    in
                                    LLMs is ongoing<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 37"
                                            title="Weidinger, L. et al. Ethical and social risks of harm from language models. Preprint at 
          https://doi.org/10.48550/arXiv.2112.04359
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR37" id="ref-link-section-d77463470e1528">37</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 38"
                                            title="Liang, P. et al. Holistic evaluation of language models. Preprint at 
          https://doi.org/10.48550/arXiv.2211.09110
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR38" id="ref-link-section-d77463470e1531">38</a></sup>.
                                    Healthcare is a particularly complex application of LLMs given the safety-critical
                                    nature of the domain and the nuances associated with social and structural bias that
                                    drives health disparities. The intersection of LLMs and healthcare creates unique
                                    opportunities for responsible and ethical innovation of robust assessment and
                                    mitigation
                                    tools for bias, fairness and health equity.</p>
                                <p>We outline opportunities for future research into frameworks for the systematic
                                    identification and mitigation of downstream harms and impacts of LLMs in healthcare
                                    contexts. Key principles include the use of participatory methods to design
                                    contextualized evaluations that reflect the values of patients that may benefit or
                                    be
                                    harmed, grounding the evaluation in one or more specific downstream clinical use
                                    cases<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 39"
                                            title="Liu, X. et al. The medical algorithmic audit. Lancet Digit. Health 4, e384–e397 (2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR39"
                                            id="ref-link-section-d77463470e1538">39</a>,<a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 40"
                                            title="Raji, I. D. et al. Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing. In Proc. 2020 Conference on Fairness, Accountability, and Transparency 33–44 (Association for Computing Machinery, 2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR40"
                                            id="ref-link-section-d77463470e1541">40</a></sup>, and the use of dataset
                                    and
                                    model documentation frameworks for transparent reporting of choices and assumptions
                                    made
                                    during data collection and curation, model development and evaluation<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" title="Rostamzadeh, N. et al. Healthsheet: development of a transparency artifact for health datasets. Preprint at 
          https://doi.org/10.48550/arXiv.2202.13028
          
         (2022)." href="#ref-CR41" id="ref-link-section-d77463470e1545">41</a>,<a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref"
                                            title="Gebru, T. et al. Datasheets for datasets. Commun. ACM 64, 86–92 (2021)."
                                            href="#ref-CR42" id="ref-link-section-d77463470e1545_1">42</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 43"
                                            title="Mitchell, M. et al. Model cards for model reporting. In Proc. conference on Fairness, Accountability, and Transparency 220–229 (Association for Computing Machinery, 2019)."
                                            href="/articles/s41586-023-06291-2#ref-CR43"
                                            id="ref-link-section-d77463470e1548">43</a></sup>. Furthermore, research is
                                    needed into the design of algorithmic procedures and benchmarks that probe for
                                    specific
                                    technical biases that are known to cause harm if not mitigated. For instance,
                                    depending
                                    on the context, it may be relevant to assess the sensitivity of model outputs to
                                    perturbations of demographic identifiers in prompts designed deliberately so that
                                    the
                                    result does not change under the perturbation<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref"
                                            title="Garg, S. et al. Counterfactual fairness in text classification through robustness. In Proc. 2019 AAAI/ACM Conference on AI, Ethics, and Society 219–226 (Association for Computing Machinery, 2019)."
                                            href="#ref-CR44" id="ref-link-section-d77463470e1552">44</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" title="Prabhakaran, V., Hutchinson, B. &amp; Mitchell, M. Perturbation sensitivity analysis to detect unintended model biases. Preprint at 
          https://doi.org/10.48550/arXiv.1910.04210
          
         (2019)." href="#ref-CR45" id="ref-link-section-d77463470e1552_1">45</a>,<a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 46"
                                            title="Zhang, H., Lu, A. X., Abdalla, M., McDermott, M. &amp; Ghassemi, M. Hurtful words: quantifying biases in clinical contextual word embeddings. In Proc. ACM Conference on Health, Inference, and Learning 110–120 (Association for Computing Machinery, 2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR46"
                                            id="ref-link-section-d77463470e1555">46</a></sup>. Additionally, the
                                    aforementioned research activities to build evaluation methods to achieve health
                                    equity
                                    in LLMs require interdisciplinary collaboration to ensure that various scientific
                                    perspectives and methods can be applied to the task of understanding the social and
                                    contextual aspects of health<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref"
                                            title="Matheny, M., Israni, S. T., Ahmed, M. &amp; Whicher, D. eds. Artificial Intelligence in Health Care: The Hope, the Hype, the Promise, the Peril (National Academy of Medicine, 2022)."
                                            href="#ref-CR47" id="ref-link-section-d77463470e1559">47</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" title="The White House Office of Science and Technology Policy. Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People 
          https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf
          
         (The White House, 2022)." href="#ref-CR48" id="ref-link-section-d77463470e1559_1">48</a>,<a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 49"
                                            title="Ethics and Governance of Artificial Intelligence for Health. WHO Guidance (World Health Organization, 2021)."
                                            href="/articles/s41586-023-06291-2#ref-CR49"
                                            id="ref-link-section-d77463470e1562">49</a></sup>.</p>
                                <p>The development of evaluation frameworks for performance, fairness, bias and equity
                                    in
                                    LLMs is a critical research agenda that should be approached with equal rigour and
                                    attention as that given to the work of encoding clinical knowledge in language
                                    models.
                                </p>
                                <h3 class="c-article__sub-heading" id="Sec26">Ethical considerations</h3>
                                <p>This research demonstrates the potential of LLMs for future use in healthcare.
                                    Transitioning from an LLM that is used for answering medical questions to a tool
                                    that
                                    can be used by healthcare providers, administrators and consumers will require
                                    considerable additional research to ensure the safety, reliability, efficacy and
                                    privacy
                                    of the technology. Careful consideration will need to be given to the ethical
                                    deployment
                                    of this technology including rigorous quality assessment when used in different
                                    clinical
                                    settings and guardrails to mitigate against over-reliance on the output of a medical
                                    assistant. For example, the potential harms of using an LLM for diagnosing or
                                    treating
                                    an illness are much greater than those from using an LLM for information about a
                                    disease
                                    or medication. Additional research will be needed to assess LLMs used in healthcare
                                    for
                                    homogenization and amplification of biases and security vulnerabilities inherited
                                    from
                                    base models<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 11"
                                            title="Bommasani, R. et al. On the opportunities and risks of foundation models. Preprint at 
          https://doi.org/10.48550/arXiv.2108.07258
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR11" id="ref-link-section-d77463470e1577">11</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 38"
                                            title="Liang, P. et al. Holistic evaluation of language models. Preprint at 
          https://doi.org/10.48550/arXiv.2211.09110
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR38" id="ref-link-section-d77463470e1580">38</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 50"
                                            title="Bommasani, R., Liang, P. &amp; Lee, T. Language models are changing AI: the need for holistic evaluation. Stanford University 
          https://crfm.stanford.edu/2022/11/17/helm.html
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR50" id="ref-link-section-d77463470e1583">50</a></sup>.</p>
                            </div>
                        </div>
                    </section>
                    <section data-title="Conclusion">
                        <div class="c-article-section" id="Sec27-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="Sec27">Conclusion</h2>
                            <div class="c-article-section__content" id="Sec27-content">
                                <p>The advent of foundation models and LLMs presents a compelling opportunity to rethink
                                    the
                                    development of medical AI and make it easier, safer and more equitable to use. At
                                    the
                                    same time, medicine is an especially complex domain for applications of LLMs.</p>
                                <p>Our research provides a glimpse into the opportunities and the challenges of applying
                                    these technologies to medicine. We anticipate that this study will spark further
                                    conversations and collaborations between patients, consumers, AI researchers,
                                    clinicians, social scientists, ethicists, policymakers and other interested parties
                                    in
                                    order to responsibly translate these early research findings to improve healthcare.
                                </p>
                            </div>
                        </div>
                    </section>
                    <section data-title="Methods">
                        <div class="c-article-section" id="Sec28-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="Sec28">Methods</h2>
                            <div class="c-article-section__content" id="Sec28-content">
                                <h3 class="c-article__sub-heading" id="Sec29">Datasets</h3>
                                <p>To assess the potential of LLMs in medicine, we focused on answering medical
                                    questions.
                                    Answering medical questions requires reading comprehension skills, ability to
                                    accurately
                                    recall medical knowledge and manipulation of expert knowledge. There are several
                                    existing medical question-answering datasets for research. These include datasets
                                    that
                                    assess professional medical knowledge such as medical exam questions<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 3"
                                            title="Jin, D. et al. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. Appl. Sci. 11, 6421 (2021)."
                                            href="/articles/s41586-023-06291-2#ref-CR3"
                                            id="ref-link-section-d77463470e1611">3</a>,<a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 4"
                                            title="Pal, A., Umapathi, L. K. &amp; Sankarasubbu, M. MedMCQA: a large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning 248–260 (Proceedings of Machine Learning Research, 2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR4"
                                            id="ref-link-section-d77463470e1614">4</a></sup>, questions that require
                                    medical
                                    research comprehension skills<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 5" title="Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. &amp; Lu, X. PubMedQA: a dataset for biomedical research question answering. Preprint at 
          https://doi.org/10.48550/arXiv.1909.06146
          
         (2019)." href="/articles/s41586-023-06291-2#ref-CR5" id="ref-link-section-d77463470e1618">5</a></sup>, and
                                    questions that require the ability to assess user intent and provide helpful answers
                                    to
                                    their medical information needs<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 13" title="Ben Abacha, A., Agichtein, E., Pinter, Y. &amp; Demner-Fushman, D. Overview of the medical question answering task at TREC 2017 LiveQA. TREC 
          https://trec.nist.gov/pubs/trec26/papers/Overview-QA.pdf?ref=https://githubhelp.com
          
         (2017)." href="/articles/s41586-023-06291-2#ref-CR13" id="ref-link-section-d77463470e1622">13</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 14"
                                            title="Abacha, A. B. et al. in Studies in Health Technology and Informatics (eds Ohno-Machado, L. &amp; Séroussi, B.) 25–29 (IOS Press, 2019)."
                                            href="/articles/s41586-023-06291-2#ref-CR14"
                                            id="ref-link-section-d77463470e1625">14</a></sup>.</p>
                                <p>We acknowledge that medical knowledge is vast in both quantity and quality. Existing
                                    benchmarks are inherently limited and only provide partial coverage of the space of
                                    medical knowledge. Here we bring together a number of different datasets for
                                    answering
                                    medical questions to enable deeper evaluation of LLM knowledge and move beyond
                                    multiple-choice accuracy or natural language generation metrics such as BLEU. The
                                    datasets we grouped together probe different abilities—some are multiple-choice
                                    questions, whereas others require long-form answers; some are open domain (where
                                    questions are answered without limiting available information to a pre-specified
                                    source), whereas others are closed domain (where questions are answered by
                                    retrieving
                                    content from associated reference text) and come from different sources. There has
                                    been
                                    extensive activity in the field of answering medical questions over recent years and
                                    we
                                    refer to ref. <sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 3"
                                            title="Jin, D. et al. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. Appl. Sci. 11, 6421 (2021)."
                                            href="/articles/s41586-023-06291-2#ref-CR3"
                                            id="ref-link-section-d77463470e1632">3</a></sup> for a comprehensive summary
                                    of
                                    medical question-answering datasets.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec30">MultiMedQA
                                    benchmark</h4>
                                <p>MultiMedQA includes medical exams and research datasets with multiple-choice answers
                                    and
                                    consumer medical question datasets with long-form answers. These include the
                                    MedQA<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 3"
                                            title="Jin, D. et al. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. Appl. Sci. 11, 6421 (2021)."
                                            href="/articles/s41586-023-06291-2#ref-CR3"
                                            id="ref-link-section-d77463470e1643">3</a></sup>, MedMCQA<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 4"
                                            title="Pal, A., Umapathi, L. K. &amp; Sankarasubbu, M. MedMCQA: a large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning 248–260 (Proceedings of Machine Learning Research, 2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR4"
                                            id="ref-link-section-d77463470e1647">4</a></sup>, PubMedQA<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 5"
                                            title="Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. &amp; Lu, X. PubMedQA: a dataset for biomedical research question answering. Preprint at 
          https://doi.org/10.48550/arXiv.1909.06146
          
         (2019)." href="/articles/s41586-023-06291-2#ref-CR5" id="ref-link-section-d77463470e1651">5</a></sup>, MMLU
                                    clinical topics<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 6"
                                            title="Hendrycks, D. et al. Measuring massive multitask language understanding. Preprint at 
          https://doi.org/10.48550/arXiv.2009.03300
          
         (2020)." href="/articles/s41586-023-06291-2#ref-CR6" id="ref-link-section-d77463470e1655">6</a></sup>,
                                    LiveQA<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 13"
                                            title="Ben Abacha, A., Agichtein, E., Pinter, Y. &amp; Demner-Fushman, D. Overview of the medical question answering task at TREC 2017 LiveQA. TREC 
          https://trec.nist.gov/pubs/trec26/papers/Overview-QA.pdf?ref=https://githubhelp.com
          
         (2017)." href="/articles/s41586-023-06291-2#ref-CR13" id="ref-link-section-d77463470e1659">13</a></sup> and
                                    MedicationQA<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 14"
                                            title="Abacha, A. B. et al. in Studies in Health Technology and Informatics (eds Ohno-Machado, L. &amp; Séroussi, B.) 25–29 (IOS Press, 2019)."
                                            href="/articles/s41586-023-06291-2#ref-CR14"
                                            id="ref-link-section-d77463470e1664">14</a></sup> datasets. We further
                                    augmented
                                    MultiMedQA with a new dataset of curated commonly searched health queries:
                                    HealthSearchQA. All the datasets are in the English language and we describe them in
                                    detail below.</p>
                                <p>These datasets vary along the following axes. (1) format: multiple-choice versus
                                    long-form answer questions; (2) capabilities tested: for example, assessing the
                                    recall
                                    of medical facts in isolation versus assessing medical reasoning capabilities in
                                    addition to recall of facts; (3) domain: open domain versus closed domain questions;
                                    (4)
                                    question source: from professional medical exams, medical research or consumers
                                    seeking
                                    medical information; and (5) labels and metadata: presence of labels or explanations
                                    and
                                    their sources. A summary of MultiMedQA is presented in Extended Data Table <a
                                        data-track="click" data-track-label="link" data-track-action="table anchor"
                                        href="/articles/s41586-023-06291-2#Tab1">1</a>.</p>
                                <p>Although MedMCQA, PubMedQA, LiveQA, and MedicationQA provide reference long-form
                                    answers
                                    or explanations, we do not use them in this work. First, the reference answers did
                                    not
                                    come from consistent sources across the different datasets. Answers often came from
                                    automated tools or non-clinicians such as librarians. The construction of the
                                    reference
                                    answers and explanations in these pioneering datasets was not optimized for holistic
                                    or
                                    comprehensive assessments of long-answer quality, which renders them suboptimal for
                                    use
                                    as a ‘ground truth’ against which to assess LLMs using automated natural language
                                    metrics such as BLEU. To alleviate this, as discussed in ‘Human evaluation results’,
                                    we
                                    obtained a standardized set of responses from qualified clinicians to a subset of
                                    the
                                    questions in the benchmark. Second, given the safety-critical requirements of the
                                    medical domain, we believe it is important to move beyond automated measures of
                                    long-form answer generation quality using metrics such as BLEU to those involving
                                    more
                                    nuanced human evaluation frameworks such as the one proposed in this study.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec31">MedQA
                                    (USMLE)
                                </h4>
                                <p>The MedQA dataset<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 3"
                                            title="Jin, D. et al. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. Appl. Sci. 11, 6421 (2021)."
                                            href="/articles/s41586-023-06291-2#ref-CR3"
                                            id="ref-link-section-d77463470e1685">3</a></sup> consists of USMLE-style
                                    questions with four or five possible answers. The development set consists of 11,450
                                    questions and the test set has 1,273 questions.</p>
                                <p><b>Format:</b> question and answer (Q + A), multiple choice, open domain.</p>
                                <p><b>Size (development set/test set):</b> 11,450/1,273.</p>
                                <p><b>Example question:</b> A 65-year-old man with hypertension comes to the physician
                                    for a
                                    routine health maintenance examination. Current medications include atenolol,
                                    lisinopril, and atorvastatin. His pulse is 86 min<sup>−1</sup>, respirations are
                                    18 min<sup>−1</sup>, and blood pressure is 145/95 mmHg. Cardiac examination reveals
                                    end
                                    diastolic murmur. Which of the following is the most likely cause of this physical
                                    examination?</p>
                                <p>
                                    <b>Answers (correct answer in bold):</b>
                                    <b><span class="u-monospace">(A) Decreased compliance of the left
                                            ventricle,</span></b>
                                    <span class="u-monospace">(B) Myxomatous degeneration of the mitral valve (C)
                                        Inflammation of the pericardium (D) Dilation of the aortic root (E) Thickening
                                        of
                                        the mitral valve leaflets.</span>
                                </p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec32">MedMCQA</h4>
                                <p>The MedMCQA dataset<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 4"
                                            title="Pal, A., Umapathi, L. K. &amp; Sankarasubbu, M. MedMCQA: a large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning 248–260 (Proceedings of Machine Learning Research, 2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR4"
                                            id="ref-link-section-d77463470e1730">4</a></sup> consists of more than
                                    194,000
                                    four-option multiple-choice questions from Indian medical entrance examinations
                                    (AIIMS/NEET)<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 4"
                                            title="Pal, A., Umapathi, L. K. &amp; Sankarasubbu, M. MedMCQA: a large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning 248–260 (Proceedings of Machine Learning Research, 2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR4"
                                            id="ref-link-section-d77463470e1734">4</a></sup>. This dataset covers 2,400
                                    healthcare topics and 21 medical subjects. The development set is substantial, with
                                    over
                                    187,000 questions.</p>
                                <p><b>Format:</b> Q + A, multiple choice, open domain.</p>
                                <p><b>Size (dev/test):</b> 187,000/6,100.</p>
                                <p><b>Example question:</b> Which of the following ultrasound findings has the highest
                                    association with aneuploidy?</p>
                                <p><b>Answers (correct answer in bold):</b> <span class="u-monospace">(A) Choroid plexus
                                        cyst (B) Nuchal translucency</span> <b><span class="u-monospace">(C) Cystic
                                            hygroma</span></b> <span class="u-monospace">(D) Single umbilical
                                        artery</span>.
                                </p>
                                <p>
                                    <b>Explanation:</b>
                                    <span class="u-monospace">All the above mentioned are ultrasound findings associated
                                        with increased risk of aneuploidy although the highest association is seen with
                                        cystic hygroma. Nuchal translucency and cystic hygroma are both measured in the
                                        first trimester. Trisomy 21 is the most common aneuploidy associated with
                                        increased
                                        nuchal translucency and cystic hygroma while monosomy X presents as
                                        second-trimester
                                        hygroma.</span>
                                </p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec33">PubMedQA
                                </h4>
                                <p>The PubMedQA dataset<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 5"
                                            title="Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. &amp; Lu, X. PubMedQA: a dataset for biomedical research question answering. Preprint at 
          https://doi.org/10.48550/arXiv.1909.06146
          
         (2019)." href="/articles/s41586-023-06291-2#ref-CR5" id="ref-link-section-d77463470e1785">5</a></sup> consists
                                    of 1,000 expert-labelled question–answer pairs where the task is to produce a
                                    yes/no/maybe multiple-choice answer given a question together with a PubMed abstract
                                    as
                                    context (Q + context + A). Whereas the MedQA and MedMCQA datasets are open domain
                                    question-answering tasks, the PubMedQA task is closed domain, in that it requires
                                    answer
                                    inference from the supporting PubMed abstract context.</p>
                                <p><b>Format:</b> Q + context + A, multiple choice, closed domain.</p>
                                <p><b>Size (development set/test set):</b> 500/500.</p>
                                <p><b>Example question:</b> Double balloon enteroscopy (DBE): is it efficacious and safe
                                    in
                                    a community setting?</p>
                                <p><b>Context:</b> From March 2007 to January 2011, 88 DBE procedures were performed on
                                    66
                                    patients. Indications included evaluation anaemia/gastrointestinal bleed, small
                                    bowel
                                    IBD and dilation of strictures. Video-capsule endoscopy (VCE) was used prior to DBE
                                    in
                                    43 of the 66 patients prior to DBE evaluation. The mean age was 62 years. Thirty-two
                                    patients were female, 15 were African American; 44 antegrade and 44 retrograde DBEs
                                    were
                                    performed. The mean time per antegrade DBE was 107.4 ± 30.0 minutes with a distance
                                    of
                                    318.4 ± 152.9 cm reached past the pylorus. The mean time per lower DBE was
                                    100.7 ± 27.3
                                    minutes with 168.9 ± 109.1 cm meters past the ileocecal valve reached. Endoscopic
                                    therapy in the form of electrocautery to ablate bleeding sources was performed in 20
                                    patients (30.3%), biopsy in 17 patients (25.8%) and dilation of Crohn’s-related
                                    small
                                    bowel strictures in 4 (6.1%). 43 VCEs with pathology noted were performed prior to
                                    DBE,
                                    with findings endoscopically confirmed in 32 cases (74.4%). In 3 cases the DBE
                                    showed
                                    findings not noted on VCE.</p>
                                <p><b>Answer:</b> <span class="u-monospace">Yes</span>.</p>
                                <p><b>Long answer:</b> <span class="u-monospace">DBE appears to be equally safe and
                                        effective when performed in the community setting as compared to a tertiary
                                        referral
                                        centre with a comparable yield, efficacy, and complication rate</span>.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec34">MMLU</h4>
                                <p>MMLU<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 6"
                                            title="Hendrycks, D. et al. Measuring massive multitask language understanding. Preprint at 
          https://doi.org/10.48550/arXiv.2009.03300
          
         (2020)." href="/articles/s41586-023-06291-2#ref-CR6" id="ref-link-section-d77463470e1834">6</a></sup> includes
                                    exam questions from 57 domains. We selected the subtasks most relevant to medical
                                    knowledge: anatomy, clinical knowledge, college medicine, medical genetics,
                                    professional
                                    medicine and college biology. Each MMLU subtask contains multiple-choice questions
                                    with
                                    four options, along with the answers.</p>
                                <p><b>Format:</b> Q + A, multiple choice, open domain.</p>
                                <h3 class="c-article__sub-heading" id="FPar1">Anatomy</h3>
                                <p><b>Size (development set/test set):</b> 14/135.</p>
                                <p><b>Example question:</b> Which of the following controls body temperature, sleep, and
                                    appetite?</p>
                                <p><b>Answer:</b> <span class="u-monospace">(A) Adrenal glands</span> <b><span
                                            class="u-monospace">(B) Hypothalamus</span></b> <span
                                        class="u-monospace">(C)
                                        Pancreas (D) Thalamus</span>.</p>

                                <h3 class="c-article__sub-heading" id="FPar2">Clinical knowledge</h3>
                                <p><b>Size (development set/test set):</b> 29/265.</p>
                                <p><b>Example question:</b> The following are features of Alzheimer’s disease except:
                                </p>
                                <p><b>Answer:</b> <span class="u-monospace">(A) short-term memory loss (B) confusion (C)
                                        poor attention</span> <b><span class="u-monospace">(D) drowsiness</span></b>.
                                </p>

                                <h3 class="c-article__sub-heading" id="FPar3">College medicine</h3>
                                <p><b>Size (development set/test set):</b> 22/173.</p>
                                <p><b>Example question:</b> The main factors determining success in sport are:</p>
                                <p><b>Answer:</b> <span class="u-monospace">(A) a high energy diet and large appetite.
                                        (B)
                                        high intelligence and motivation to succeed. (C) a good coach and the motivation
                                        to
                                        succeed.</span> <b><span class="u-monospace">(D) innate ability and the capacity
                                            to
                                            respond to the training stimulus</span></b>.</p>

                                <h3 class="c-article__sub-heading" id="FPar4">Medical genetics</h3>
                                <p><b>Size (development set/test set):</b> 11/100.</p>
                                <p><b>Example question:</b> The allele associated with sickle cell anemia apparently
                                    reached
                                    a high frequency in some human populations due to:</p>
                                <p><b>Answer:</b> <span class="u-monospace">(A) random mating</span> <b><span
                                            class="u-monospace">(B) superior fitness of heterozygotes in areas where
                                            malaria
                                            was present</span></b> <span class="u-monospace">(C) migration of
                                        individuals
                                        with the allele into other populations (D) a high mutation rate at that specific
                                        gene</span>.</p>

                                <h3 class="c-article__sub-heading" id="FPar5">Professional medicine</h3>
                                <p><b>Size (development set/test set):</b> 31/272.<b>Example question:</b> A 19-year-old
                                    woman noticed a mass in her left breast 2 weeks ago while doing monthly breast
                                    self-examination. Her mother died of metastatic breast cancer at the age of 40
                                    years.
                                    Examination shows large dense breasts; a 2-cm, firm, mobile mass is palpated in the
                                    upper outer quadrant of the left breast. There are no changes in the skin or nipple,
                                    and
                                    there is no palpable axillary adenopathy. Which of the following is the most likely
                                    diagnosis? <b>Answer:</b> <b><span class="u-monospace">(A) Fibroadenoma</span></b>
                                    <span class="u-monospace">(B) Fibrocystic changes of the breast (C) Infiltrating
                                        ductal
                                        carcinoma (D) Intraductal papilloma</span>.</p>

                                <h3 class="c-article__sub-heading" id="FPar6">College biology</h3>
                                <p><b>Size (development set/test set):</b> 16/144.</p>
                                <p><b>Example question:</b> Which of the following is the most direct cause of polyteny
                                    in
                                    somatic cells of certain organisms?</p>
                                <p><b>Answer:</b> <span class="u-monospace">(A) RNA transcription (B) Supercoiling of
                                        chromatin</span> <b><span class="u-monospace">(C) Chromosome replication without
                                            cell division</span></b> <span class="u-monospace">(D) Chromosome
                                        recombination</span>.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec35">LiveQA</h4>
                                <p>The LiveQA dataset<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 13"
                                            title="Ben Abacha, A., Agichtein, E., Pinter, Y. &amp; Demner-Fushman, D. Overview of the medical question answering task at TREC 2017 LiveQA. TREC 
          https://trec.nist.gov/pubs/trec26/papers/Overview-QA.pdf?ref=https://githubhelp.com
          
         (2017)." href="/articles/s41586-023-06291-2#ref-CR13" id="ref-link-section-d77463470e2013">13</a></sup> was
                                    curated as part of the Text Retrieval Challenge (TREC) 2017. The dataset consists of
                                    medical questions submitted by people to the National Library of Medicine (NLM). The
                                    dataset also consists of manually collected reference answers from trusted sources
                                    such
                                    as the National Institute of Health (NIH) website.</p>
                                <p><b>Format:</b> questions and long answers, free text response, open domain.</p>
                                <p><b>Size (development set/test set):</b> 634/104.</p>
                                <p><b>Example question:</b> Could second hand smoke contribute to or cause early AMD?
                                </p>
                                <p><b>Long answer:</b> <span class="u-monospace">Smoking increases a person’s chances of
                                        developing AMD by two to five fold. Because the retina has a high rate of oxygen
                                        consumption, anything that affects oxygen delivery to the retina may affect
                                        vision.
                                        Smoking causes oxidative damage, which may contribute to the development and
                                        progression of this disease. Learn more about why smoking damages the retina,
                                        and
                                        explore a number of steps you can take to protect your vision</span>.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec36">MedicationQA
                                </h4>
                                <p>The MedicationQA dataset<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 14"
                                            title="Abacha, A. B. et al. in Studies in Health Technology and Informatics (eds Ohno-Machado, L. &amp; Séroussi, B.) 25–29 (IOS Press, 2019)."
                                            href="/articles/s41586-023-06291-2#ref-CR14"
                                            id="ref-link-section-d77463470e2049">14</a></sup> consists of commonly asked
                                    consumer questions about medications. In addition to the question, the dataset
                                    contains
                                    annotations corresponding to drug focus and interactions. Similar to LiveQA, we
                                    evaluated the models’ ability to produce long-form answers to the questions in the
                                    test
                                    set.</p>
                                <p><b>Format:</b> Questions, long answers, free text response, open domain.</p>
                                <p><b>Size (development set/test set):</b> NA/674.</p>
                                <p><b>Example question:</b> How does valium affect the brain?</p>
                                <p><b>Focus (drug):</b> Valium.</p>
                                <p><b>Question type:</b> Action.</p>
                                <p>
                                    <b>Long answer:</b>
                                    <span class="u-monospace">Diazepam is a benzodiazepine that exerts anxiolytic,
                                        sedative,
                                        muscle-relaxant, anticonvulsant and amnestic effects. Most of these effects are
                                        thought to result from a facilitation of the action of gamma aminobutyric acid
                                        (GABA), an inhibitory neurotransmitter in the central nervous system.</span>
                                </p>
                                <p><b>Section title:</b> Clinical pharmacology.</p>
                                <p><b>URL:</b> <a
                                        href="https://dailymed.nlm.nih.gov/dailymed/drugInfo.cfm?setid=554baee5-b171-4452-a50a-41a0946f956c"><span
                                            class="u-monospace">https://dailymed.nlm.nih.gov/dailymed/drugInfo.cfm?setid=554baee5-b171-4452-a50a-41a0946f956c</span></a>.
                                </p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec37">
                                    HealthSearchQA
                                </h4>
                                <p>We curated our own additional dataset consisting of 3,173 commonly searched consumer
                                    questions, referred to as HealthSearchQA. The dataset was curated using seed medical
                                    conditions and their associated symptoms. We used the seed data to retrieve
                                    publicly-available commonly searched questions generated by a search engine, which
                                    were
                                    displayed to all users entering the seed terms. We publish the dataset as an open
                                    benchmark for answering medical questions from consumers and hope this will be a
                                    useful
                                    resource for the community, as a dataset reflecting real-world consumer concerns.
                                </p>
                                <p><b>Format:</b> Question only, free text response, open domain.</p>
                                <p><b>Size:</b> 3,173.</p>
                                <p><b>Example question:</b> How serious is atrial fibrillation?</p>
                                <p><b>Example question:</b> What kind of cough comes with Covid?</p>
                                <p><b>Example question:</b> Is blood in phlegm serious?</p>
                                <p>Although MultiMedQA allows us to probe the medical question-answering capabilities of
                                    LLMs along multiple axes, we acknowledge that it is not exhaustive. We plan to
                                    expand
                                    the benchmark to other relevant datasets, such as those probing question-answering
                                    ability from electronic medical records<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 51" title="Pampari, A., Raghavan, P., Liang, J. &amp; Peng, J. emrQA: a large corpus for question answering on electronic medical records. Preprint at 
          https://doi.org/10.48550/arXiv.1809.00732
          
         (2018)." href="/articles/s41586-023-06291-2#ref-CR51" id="ref-link-section-d77463470e2144">51</a></sup> or
                                    those requiring pre-clinical biomedical knowledge<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 52"
                                            title="Tsatsaronis, G. et al. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC Bioinformatics 16, 138 (2015)."
                                            href="/articles/s41586-023-06291-2#ref-CR52"
                                            id="ref-link-section-d77463470e2148">52</a></sup>, in future work.</p>
                                <h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec38">Framework
                                    for
                                    human evaluation</h3>
                                <p>Here we describe our proposed framework for human evaluation of long-form answers to
                                    medical questions.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec39">Clinician
                                    evaluation</h4>
                                <p>Although objective accuracy metrics on multiple-choice questions are a robust measure
                                    of
                                    model performance, they omit several important details. To more deeply assess the
                                    generative outputs of LLMs in open-ended answering of questions on medical topics,
                                    we
                                    developed a pilot framework for human evaluation of long-form model answers to
                                    consumer
                                    medical questions in the LiveQA, MedicationQA, and HealthSearchQA datasets.</p>
                                <p>The pilot framework was inspired by approaches published in a similar domain<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 25"
                                            title="Feng, S. Y., Khetan, V., Sacaleanu, B., Gershman, A. &amp; Hovy, E. CHARD: clinical health-aware reasoning across dimensions for text generation models. Preprint at 
          https://doi.org/10.48550/arXiv.2210.04191
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR25" id="ref-link-section-d77463470e2171">25</a></sup> to
                                    examine the strengths and weaknesses of LLM generations in clinical settings. We
                                    used
                                    focus groups and interviews with clinicians based in the UK, USA and India to
                                    identify
                                    additional axes of evaluation<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 53"
                                            title="Morgado, F. F., Meireles, J. F., Neves, C., Amaral, A. &amp; Ferreira, M. E. Scale development: ten main limitations and recommendations to improve future research practices. Psic. Reflex. Crit. 30, 5 (2017)."
                                            href="/articles/s41586-023-06291-2#ref-CR53"
                                            id="ref-link-section-d77463470e2175">53</a></sup> and expanded the framework
                                    items to address notions of agreement with scientific consensus, possibility and
                                    likelihood of harm, completeness and missingness of answers, and possibility of
                                    bias.
                                    Alignment with scientific consensus was measured by asking raters whether the output
                                    of
                                    the model was aligned with a prevailing scientific consensus (for example, in the
                                    form
                                    of well-accepted clinical practice guidelines), opposed to a scientific consensus;
                                    or
                                    whether no clear scientific consensus exists regarding the question. Harm is a
                                    complex
                                    concept that can be evaluated along several dimensions (for example, physical
                                    health,
                                    mental health, moral, financial and many others). When answering this question,
                                    raters
                                    were asked to focus solely on physical or mental health-related harms, and evaluated
                                    both severity (in a format inspired by the AHRQ common formats for harm<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 26"
                                            title="Williams, T., Szekendi, M., Pavkovic, S., Clevenger, W. &amp; Cerese, J. The reliability of ahrq common format harm scales in rating patient safety events. J. Patient Saf. 11, 52–59 (2015)."
                                            href="/articles/s41586-023-06291-2#ref-CR26"
                                            id="ref-link-section-d77463470e2179">26</a></sup>) and likelihood, under the
                                    assumption that a consumer or physician based on the content of the answer might
                                    take
                                    actions. Bias was assessed broadly by raters considering if the answer contained
                                    information that would be inapplicable or inaccurate to a specific patient
                                    demographic.
                                    The questions asked in the evaluation are summarized in Extended Data Table <a
                                        data-track="click" data-track-label="link" data-track-action="table anchor"
                                        href="/articles/s41586-023-06291-2#Tab3">3</a>.</p>
                                <p>Our framework items’ form, wording and response-scale points were refined by
                                    undertaking
                                    further interviews with triplicate assessments of 25 question-answer tuples per
                                    dataset
                                    by three qualified clinicians. Instructions for the clinicians were written
                                    including
                                    indicative examples of ratings for questions, and iterated until the clinicians’
                                    rating
                                    approaches converged to indicate the instructions were usable. Once the guidelines
                                    had
                                    converged a larger set of question-answer tuples from the consumer medical questions
                                    datasets were evaluated by single-ratings performed by one of nine clinicians based
                                    in
                                    the UK, USA or India and qualified for practice in their respective countries, with
                                    specialist experience including paediatrics, surgery, internal medicine, and primary
                                    care.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec40">Lay user
                                    evaluation</h4>
                                <p>In order to assess the helpfulness and utility of the answers to the consumer medical
                                    questions, we undertook an additional lay user (non-expert) evaluation. This was
                                    performed by five raters without a medical background, all of whom were based in
                                    India.
                                    The goal of this exercise was to assess how well the answer addressed the perceived
                                    intent underlying the question and how helpful and actionable it was. The questions
                                    asked in the evaluation are summarized in Extended Data Table <a data-track="click"
                                        data-track-label="link" data-track-action="table anchor"
                                        href="/articles/s41586-023-06291-2#Tab2">2</a>.</p>
                                <h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec41">Modelling
                                </h3>
                                <p>In this section, we detail LLMs and the techniques used to align them with the
                                    requirements of the medical domain.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec42">Models</h4>
                                <p>We built on the PaLM and Flan-PaLM family of LLMs in this study.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec43">PaLM</h4>
                                <p>PaLM<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 1"
                                            title="Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at 
          https://doi.org/10.48550/arXiv.2204.02311
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR1" id="ref-link-section-d77463470e2224">1</a></sup> is a
                                    densely-activated decoder-only transformer language model trained using
                                    Pathways<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 54"
                                            title="Barham, P. et al. Pathways: asynchronous distributed dataflow for ML. Proc. Mach. Learn. Syst. 4, 430–449 (2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR54"
                                            id="ref-link-section-d77463470e2228">54</a></sup>, a large-scale machine
                                    learning accelerator orchestration system that enables highly efficient training
                                    across
                                    TPU pods. The PaLM training corpus consists of 780 billion tokens representing a
                                    mixture
                                    of webpages, Wikipedia articles, source code, social media conversations, news
                                    articles,
                                    and books. All three PaLM model variants were trained for exactly one epoch of the
                                    training data. We refer to refs. <sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 1" title="Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at 
          https://doi.org/10.48550/arXiv.2204.02311
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR1" id="ref-link-section-d77463470e2232">1</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 55"
                                            title="Thoppilan, R. et al. Lamda: language models for dialog applications. Preprint at 
          https://doi.org/10.48550/arXiv.2201.08239
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR55" id="ref-link-section-d77463470e2235">55</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 56"
                                            title="Du, N. et al. Glam: efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning 5547–5569 (PMLR, 2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR56"
                                            id="ref-link-section-d77463470e2238">56</a></sup> for more details on the
                                    training corpus. At the time of release, PaLM 540B achieved breakthrough
                                    performance,
                                    outperforming finetuned state-of-the-art models on a suite of multi-step reasoning
                                    tasks
                                    and exceeding average human performance on BIG-bench<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 1" title="Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at 
          https://doi.org/10.48550/arXiv.2204.02311
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR1" id="ref-link-section-d77463470e2242">1</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 57"
                                            title="Srivastava, A. et al. Beyond the imitation game: quantifying and extrapolating the capabilities of language models. Preprint at 
          https://doi.org/10.48550/arXiv.2206.04615
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR57" id="ref-link-section-d77463470e2245">57</a></sup>.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec44">Flan-PaLM
                                </h4>
                                <p>In addition to the baseline PaLM models, we also considered the instruction-tuned
                                    counterpart<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 2"
                                            title="Chung, H. W. et al. Scaling instruction-finetuned language models. Preprint at 
          https://doi.org/10.48550/arXiv.2210.11416
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR2" id="ref-link-section-d77463470e2257">2</a></sup>. These
                                    models were trained using instruction tuning—that is, fine-tuning the model on a
                                    collection of datasets in which each example was prefixed with some combination of
                                    instructions and/or few-shot exemplars. In particular, Flan-PaLM<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 2"
                                            title="Chung, H. W. et al. Scaling instruction-finetuned language models. Preprint at 
          https://doi.org/10.48550/arXiv.2210.11416
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR2" id="ref-link-section-d77463470e2261">2</a></sup>
                                    demonstrated the effectiveness of scaling the number of tasks, model size and using
                                    chain-of-thought data<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 16"
                                            title="Wei, J. et al. Chain of thought prompting elicits reasoning in large language models. Preprint at 
          https://doi.org/10.48550/arXiv.2201.11903
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR16" id="ref-link-section-d77463470e2265">16</a></sup> as
                                    instructions. The Flan-PaLM model reached state-of-the-art performance on several
                                    benchmarks such as MMLU, BBH and TyDIQA<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 58"
                                            title="Clark, J. H. et al. Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages. Trans. Assoc. Comput. Linguist. 8, 454–470 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR58"
                                            id="ref-link-section-d77463470e2269">58</a></sup>. Across the suite of
                                    evaluation tasks considered<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 2" title="Chung, H. W. et al. Scaling instruction-finetuned language models. Preprint at 
          https://doi.org/10.48550/arXiv.2210.11416
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR2" id="ref-link-section-d77463470e2273">2</a></sup>,
                                    Flan-PaLM outperformed baseline PaLM by an average of 9.4%, demonstrating the
                                    effectiveness of the instruction tuning approach.</p>
                                <p>In this study, we considered both the PaLM and Flan-PaLM model variants at three
                                    different model sizes: 8B, 62B and 540B, with the largest model using 6,144 TPUv4
                                    chips
                                    for pre-training.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec45">Aligning
                                    LLMs to
                                    the medical domain</h4>
                                <p>General-purpose LLMs like PaLM<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 1" title="Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at 
          https://doi.org/10.48550/arXiv.2204.02311
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR1" id="ref-link-section-d77463470e2289">1</a></sup> and
                                    GPT-3&nbsp;(ref.&nbsp;<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 15"
                                            title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR15"
                                            id="ref-link-section-d77463470e2293">15</a></sup>) have reached
                                    state-of-the-art
                                    performance on a wide variety of tasks on challenging benchmarks such as BIG-bench.
                                    However, given the safety-critical nature of the medical domain, it is necessary to
                                    adapt and align the model with domain-specific data. Typical transfer learning and
                                    domain adaptation methods rely on end-to-end fine-tuning of the model with large
                                    amounts
                                    of in-domain data, an approach that is challenging here given the paucity of medical
                                    data. As such, in this study, we focused on data-efficient alignment strategies
                                    building
                                    on prompting<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 15"
                                            title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR15"
                                            id="ref-link-section-d77463470e2297">15</a></sup> and prompt tuning<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 59"
                                            title="Lester, B., Al-Rfou, R. &amp; Constant, N. The power of scale for parameter-efficient prompt tuning. Preprint at 
          https://doi.org/10.48550/arXiv.2104.08691
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR59" id="ref-link-section-d77463470e2301">59</a></sup>.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec46">Prompting
                                    strategies</h4>
                                <p>GPT-3&nbsp;(ref.&nbsp;<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 15"
                                            title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR15"
                                            id="ref-link-section-d77463470e2313">15</a></sup>) demonstrated that LLMs
                                    are
                                    strong few-shot learners, where fast in-context learning can be achieved through
                                    prompting strategies. Through a handful of demonstration examples encoded as prompt
                                    text
                                    in the input context, these models are able to generalize to new examples and new
                                    tasks
                                    without any gradient updates or fine-tuning. The remarkable success of in-context
                                    few-shot learning has spurred the development of many prompting strategies including
                                    scratchpad<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 60"
                                            title="Nye, M. et al. Show your work: scratchpads for intermediate computation with language models. Preprint at 
          https://doi.org/10.48550/arXiv.2112.00114
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR60" id="ref-link-section-d77463470e2317">60</a></sup>,
                                    chain-of-thought<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 16"
                                            title="Wei, J. et al. Chain of thought prompting elicits reasoning in large language models. Preprint at 
          https://doi.org/10.48550/arXiv.2201.11903
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR16" id="ref-link-section-d77463470e2321">16</a></sup>, and
                                    least-to-most prompting<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 61" title="Zhou, D. et al. Least-to-most prompting enables complex reasoning in large language models. Preprint at 
          https://doi.org/10.48550/arXiv.2205.10625
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR61" id="ref-link-section-d77463470e2325">61</a></sup>,
                                    especially for multi-step computation and reasoning problems such as mathematical
                                    problems<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 62"
                                            title="Cobbe, K. et al. Training verifiers to solve math word problems. Preprint at 
          https://doi.org/10.48550/arXiv.2110.14168
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR62" id="ref-link-section-d77463470e2329">62</a></sup>. In
                                    this study, we focused on standard few-shot, chain-of-thought, and self-consistency
                                    prompting as discussed below.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec47">Few-shot
                                    prompting</h4>
                                <p>The standard few-shot prompting strategy was introduced with GPT-3 (ref. <sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 15"
                                            title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR15"
                                            id="ref-link-section-d77463470e2341">15</a></sup>). Here, the prompt to the
                                    model is designed to include few-shot examples describing the task through
                                    text-based
                                    demonstrations. These demonstrations are typically encoded as input–output pairs.
                                    The
                                    number of examples is typically chosen depending on the number of tokens that can
                                    fit
                                    into the input context window of the model. After the prompt, the model is provided
                                    with
                                    an input and asked to generate a test-time prediction. The zero-shot prompting
                                    counterpart typically only involves an instruction describing the task without
                                    including
                                    any additional examples. Few-shot performance appears to be an emergent
                                    ability<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 28"
                                            title="Wei, J. et al. Emergent abilities of large language models. Preprint at 
          https://doi.org/10.48550/arXiv.2206.07682
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR28" id="ref-link-section-d77463470e2345">28</a></sup> for
                                    many tasks—that is, an ability that is non-existent in small models but rapidly
                                    improves
                                    above random performance beyond a certain model size.</p>
                                <p>In this study, we worked with a panel of qualified clinicians to identify the best
                                    demonstration examples and craft the few-shot prompts. Separate prompts were
                                    designed
                                    for each dataset as detailed in Supplementary Information, section&nbsp;<a
                                        data-track="click" data-track-label="link"
                                        data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">11</a>. The number of few-shot
                                    demonstrations varied depending on the dataset. Typically, we used five input–output
                                    examples for the consumer medical question-answering datasets, but reduced the
                                    number to
                                    three or fewer for PubMedQA given the need to also fit in the abstract context
                                    within
                                    the prompt text.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec48">
                                    Chain-of-thought
                                    prompting</h4>
                                <p>COT<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 16"
                                            title="Wei, J. et al. Chain of thought prompting elicits reasoning in large language models. Preprint at 
          https://doi.org/10.48550/arXiv.2201.11903
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR16" id="ref-link-section-d77463470e2363">16</a></sup>
                                    involves augmenting each few-shot example in the prompt with a step-by-step
                                    breakdown
                                    and a coherent set of intermediate reasoning steps towards the final answer. The
                                    approach is designed to mimic the human thought process when solving problems that
                                    require multi-step computation and reasoning. COT prompting can elicit reasoning
                                    abilities in sufficiently LLMs and dramatically improve performance on tasks such as
                                    mathematical problems<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 16"
                                            title="Wei, J. et al. Chain of thought prompting elicits reasoning in large language models. Preprint at 
          https://doi.org/10.48550/arXiv.2201.11903
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR16" id="ref-link-section-d77463470e2367">16</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 62"
                                            title="Cobbe, K. et al. Training verifiers to solve math word problems. Preprint at 
          https://doi.org/10.48550/arXiv.2110.14168
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR62" id="ref-link-section-d77463470e2370">62</a></sup>.
                                    Further, the appearance of such COT reasoning appears to be an emergent
                                    ability<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 28"
                                            title="Wei, J. et al. Emergent abilities of large language models. Preprint at 
          https://doi.org/10.48550/arXiv.2206.07682
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR28" id="ref-link-section-d77463470e2374">28</a></sup> of
                                    LLMs. COT prompting has been used to achieve breakthrough LLM performance on several
                                    STEM benchmarks<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 63"
                                            title="Lewkowycz, A. et al. Solving quantitative reasoning problems with language models. Preprint at 
          https://doi.org/10.48550/arXiv.2206.14858
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR63" id="ref-link-section-d77463470e2378">63</a></sup>.</p>
                                <p>Many of the medical questions explored in this study involve complex multi-step
                                    reasoning, making them a good fit for COT prompting techniques. Together with
                                    clinicians, we crafted COT prompts to provide clear demonstrations on how to reason
                                    and
                                    answer the given medical questions. Examples of such prompts are detailed in
                                    Supplementary Information, section&nbsp;<a data-track="click"
                                        data-track-label="link" data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">12</a>.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec49">
                                    Self-consistency
                                    prompting</h4>
                                <p>A straightforward strategy to improve the performance on the multiple-choice
                                    benchmarks
                                    is to prompt and sample multiple decoding outputs from the model. The final answer
                                    is
                                    the one received the majority (or plurality) vote. This idea was introduced as
                                    ‘self-consistency’<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 17"
                                            title="Wang, X. et al. Self-consistency improves chain of thought reasoning in language models. Preprint at 
          https://doi.org/10.48550/arXiv.2203.11171
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR17" id="ref-link-section-d77463470e2396">17</a></sup>. The
                                    rationale behind this approach here is that for a domain such as medicine with
                                    complex
                                    reasoning paths, there might be multiple potential routes to the correct answer.
                                    Marginalizing out the reasoning paths can lead to the most consistent answer. The
                                    self-consistency prompting strategy led to particularly strong improvements in
                                    reasoning
                                    tasks<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 63"
                                            title="Lewkowycz, A. et al. Solving quantitative reasoning problems with language models. Preprint at 
          https://doi.org/10.48550/arXiv.2206.14858
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR63" id="ref-link-section-d77463470e2400">63</a></sup>, and
                                    we adopted the same approach for our datasets with multiple-choice questions: MedQA,
                                    MedMCQA, PubMedQA, and MMLU. In this work, all decodes were performed with a
                                    temperature
                                    sampling<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 64"
                                            title="Ackley, D. H., Hinton, G. E. &amp; Sejnowski, T. J. A learning algorithm for boltzmann machines. Cogn. Sci. 9, 147–169 (1985)."
                                            href="/articles/s41586-023-06291-2#ref-CR64"
                                            id="ref-link-section-d77463470e2404">64</a>,<a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 65" title="Ficler, J. &amp; Goldberg, Y. Controlling linguistic style aspects in neural language generation. Preprint at 
          https://doi.org/10.48550/arXiv.1707.02633
          
         (2017)." href="/articles/s41586-023-06291-2#ref-CR65" id="ref-link-section-d77463470e2407">65</a></sup>
                                    constant of 0.7.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec50">Prompt
                                    tuning
                                </h4>
                                <p>Because LLMs have grown to hundreds of billions of parameters<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 1"
                                            title="Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at 
          https://doi.org/10.48550/arXiv.2204.02311
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR1" id="ref-link-section-d77463470e2419">1</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 15"
                                            title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR15"
                                            id="ref-link-section-d77463470e2422">15</a></sup>, fine-tuning them is
                                    extraordinarily computationally expensive. While the success of few-shot prompting
                                    has
                                    alleviated this issue to a large extent, many tasks would benefit further from
                                    gradient-based learning. Prompt tuning<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 59" title="Lester, B., Al-Rfou, R. &amp; Constant, N. The power of scale for parameter-efficient prompt tuning. Preprint at 
          https://doi.org/10.48550/arXiv.2104.08691
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR59" id="ref-link-section-d77463470e2426">59</a></sup> (in
                                    contrast to prompting/priming), is a simple and computationally inexpensive method
                                    to
                                    adapt LLMs to specific downstream tasks, especially with limited data. The approach
                                    involves the learning of soft prompt vectors through backpropagation while keeping
                                    the
                                    rest of the LLM parameters frozen, thus allowing easy reuse of a single model across
                                    tasks.</p>
                                <p>This use of soft prompts can be contrasted with the discrete ‘hard’ text-based
                                    few-shot
                                    prompts popularized by LLMs such as GPT-3&nbsp;(ref.&nbsp;<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 15"
                                            title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR15"
                                            id="ref-link-section-d77463470e2433">15</a></sup>). While prompt tuning can
                                    benefit from any number of labelled examples, typically only a handful of examples
                                    (for
                                    instance, tens) are required to achieve good performance. Further, it was
                                    demonstrated
                                    that prompt-tuned model performance becomes comparable with end-to-end fine-tuning
                                    performance at increased model scale<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 59" title="Lester, B., Al-Rfou, R. &amp; Constant, N. The power of scale for parameter-efficient prompt tuning. Preprint at 
          https://doi.org/10.48550/arXiv.2104.08691
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR59" id="ref-link-section-d77463470e2437">59</a></sup>. Other
                                    related approaches include prefix tuning<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 66" title="Li, X. L. &amp; Liang, P. Prefix-tuning: optimizing continuous prompts for generation. Preprint at 
          https://doi.org/10.48550/arXiv.2101.00190
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR66" id="ref-link-section-d77463470e2441">66</a></sup>, where
                                    prefix activation vectors are prepended to each layer of the LLM encoder and learned
                                    through backpropagation. Prompt tuning can be thought of as a simplification of this
                                    idea, restricting the learnable parameters to only those representing a small number
                                    of
                                    tokens prepended to the input as a soft prompt.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec51">Instruction
                                    prompt tuning</h4>
                                <p>Flan models<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 2"
                                            title="Chung, H. W. et al. Scaling instruction-finetuned language models. Preprint at 
          https://doi.org/10.48550/arXiv.2210.11416
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR2" id="ref-link-section-d77463470e2454">2</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 67"
                                            title="Wei, J. et al. Finetuned language models are zero-shot learners. Preprint at 
          https://doi.org/10.48550/arXiv.2109.01652
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR67" id="ref-link-section-d77463470e2457">67</a></sup>
                                    demonstrated the benefits of multi-task instruction fine-tuning: the Flan-PaLM model
                                    achieved state-of-the-art performance on several benchmarks such as BIG-bench<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 63"
                                            title="Lewkowycz, A. et al. Solving quantitative reasoning problems with language models. Preprint at 
          https://doi.org/10.48550/arXiv.2206.14858
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR63" id="ref-link-section-d77463470e2461">63</a></sup> and
                                    MMLU<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 6"
                                            title="Hendrycks, D. et al. Measuring massive multitask language understanding. Preprint at 
          https://doi.org/10.48550/arXiv.2009.03300
          
         (2020)." href="/articles/s41586-023-06291-2#ref-CR6" id="ref-link-section-d77463470e2465">6</a></sup>. In
                                    particular, Flan-PaLM demonstrated the benefits of using COT data in fine-tuning,
                                    leading to robust improvements in tasks that required reasoning.</p>
                                <p>Given the strong performance of instruction tuning, we built primarily on the
                                    Flan-PALM
                                    model in this work. However, our human evaluation revealed key gaps in Flan-PaLM’s
                                    performance on the consumer medical question-answering datasets, even with few-shot
                                    prompting. To further align the model to the requirements of the safety-critical
                                    medical
                                    domain, we explored additional training specifically on medical data.</p>
                                <p>For this additional training, we used prompt tuning instead of full-model fine-tuning
                                    given compute and clinician data generation costs. Our approach effectively extends
                                    Flan-PaLM’s principle of ‘learning to follow instructions’ to the prompt tuning
                                    stage.
                                    Specifically, rather than using the soft prompt learned by prompt tuning as a
                                    replacement for a task-specific human-engineered prompt, we instead used the soft
                                    prompt
                                    as an initial prefix that is shared across multiple medical datasets, and which is
                                    followed by the relevant task-specific human-engineered prompt (consisting of
                                    instructions and/or few-shot exemplars, which may be chain-of-thought examples)
                                    along
                                    with the actual question and/or context.</p>
                                <p>We refer to this method of prompt tuning as ‘instruction prompt tuning’. Instruction
                                    prompt tuning can thus be seen as a lightweight way (data-efficient,
                                    parameter-efficient, compute-efficient during both training and inference) of
                                    training a
                                    model to follow instructions in one or more domains. In our setting, instruction
                                    prompt
                                    tuning adapted LLMs to better follow the specific type of instructions used in the
                                    family of medical datasets that we targeted.</p>
                                <p>As an aside, instruction prompt tuning is not specific to the medical domain or to
                                    PaLM.
                                    It can be applied in other domains or other LLMs by (1) preparing a training corpus
                                    containing multiple tasks with different instructions, (2) freezing the LLM, (3)
                                    randomly initializing a <i>p</i> × <i>e</i> matrix (where <i>p</i> is the soft
                                    prompt
                                    length and <i>e</i> is the model’s embedding token dimension) representing a
                                    sequence of
                                    soft tokens, (4) prepending the matrix to any embedded inputs to the LLM, and (5)
                                    training the matrix via backpropagation on a negative log-likelihood loss as in
                                    prompt
                                    tuning<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 59"
                                            title="Lester, B., Al-Rfou, R. &amp; Constant, N. The power of scale for parameter-efficient prompt tuning. Preprint at 
          https://doi.org/10.48550/arXiv.2104.08691
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR59" id="ref-link-section-d77463470e2494">59</a></sup>. We
                                    provide additional hyperparameter details for our implementation in Supplementary
                                    Information, section&nbsp;<a data-track="click" data-track-label="link"
                                        data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">2</a>.</p>
                                <p>Given the combination of soft prompt with hard prompt, instruction prompt tuning can
                                    be
                                    considered a type of ‘hard-soft hybrid prompt tuning’<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 68" title="Liu, P. et al. Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing. Preprint at 
          https://doi.org/10.48550/arXiv.2107.13586
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR68" id="ref-link-section-d77463470e2505">68</a></sup>,
                                    alongside existing techniques that insert hard anchor tokens into a soft
                                    prompt<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 69"
                                            title="Liu, X. et al. GPT understands, too. Preprint at 
          https://doi.org/10.48550/arXiv.2103.10385
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR69" id="ref-link-section-d77463470e2509">69</a></sup>,
                                    insert learned soft tokens into a hard prompt<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 70"
                                            title="Han, X., Zhao, W., Ding, N., Liu, Z. &amp; Sun, M. PTR: prompt tuning with rules for text classification. AI Open 3, 182–192 (2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR70"
                                            id="ref-link-section-d77463470e2513">70</a></sup>, or use a learned soft
                                    prompt
                                    as a prefix for a short zero-shot hard prompt<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 71" title="Gu, Y., Han, X., Liu, Z. &amp; Huang, M. PPT: Pre-trained prompt tuning for few-shot learning. Preprint at 
          https://doi.org/10.48550/arXiv.2109.04332
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR71" id="ref-link-section-d77463470e2517">71</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 72"
                                            title="Ye, S., Jang, J., Kim, D., Jo, Y. &amp; Seo, M. Retrieval of soft prompt enhances zero-shot task generalization. Preprint at 
          https://doi.org/10.48550/arXiv.2210.03029
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR72" id="ref-link-section-d77463470e2520">72</a></sup>. To
                                    the best of our knowledge, ours is the first published example of learning a soft
                                    prompt
                                    that is prefixed in front of a full hard prompt containing a mixture of instructions
                                    and
                                    few-shot exemplars.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec52">Putting it
                                    all
                                    together: Med-PaLM</h4>
                                <p>To adapt Flan-PaLM to the medical domain, we applied instruction prompt tuning on a
                                    small
                                    set of exemplars. These examples were effectively used to instruct the model to
                                    produce
                                    text generations more aligned with the requirements of the medical domain, with good
                                    examples of medical comprehension, recall of clinical knowledge, and reasoning on
                                    medical knowledge unlikely to lead to patient harm. Thus, the curation of these
                                    examples
                                    was very important.</p>
                                <p>We randomly sampled examples from MultiMedQA free-response datasets (HealthSearchQA,
                                    MedicationQA, LiveQA) and asked a panel of five clinicians to provide exemplar
                                    answers.
                                    These clinicians were based in the USA and the UK with specialist experience in
                                    primary
                                    care, surgery, internal medicine and paediatrics. Clinicians then filtered out
                                    questions/answer pairs that they decided were not good examples to instruct the
                                    model.
                                    This generally happened when clinicians felt like they could not produce an ‘ideal’
                                    model answer for a given question—for example, if the information required to answer
                                    a
                                    question was not known. We were left with 65 examples across HealthSearchQA,
                                    MedicationQA, and LiveQA used for instruction prompt tuning training.</p>
                                <p>The resulting model, Med-PaLM, was evaluated on the consumer medical
                                    question-answering
                                    datasets of MultiMedQA along with Flan-PaLM. Extended Data Fig. <a
                                        data-track="click" data-track-label="link" data-track-action="figure anchor"
                                        href="/articles/s41586-023-06291-2#Fig7">1</a> gives an overview of our
                                    instruction
                                    prompt tuning approach for Med-PaLM. Further details on the hyperparameter
                                    optimization
                                    and model selection process can be found in Supplementary Information,
                                    section&nbsp;<a data-track="click" data-track-label="link"
                                        data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">2</a>. The model card for Med-PaLM is
                                    provided in Supplementary Information, section&nbsp;<a data-track="click"
                                        data-track-label="link" data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM1">9</a>.</p>
                                <h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec53">Related
                                    work
                                </h3>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec54">Large
                                    language
                                    models</h4>
                                <p>Over the past few years, LLMs have shown impressive performance on natural language
                                    processing tasks<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 1"
                                            title="Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at 
          https://doi.org/10.48550/arXiv.2204.02311
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR1" id="ref-link-section-d77463470e2560">1</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 2"
                                            title="Chung, H. W. et al. Scaling instruction-finetuned language models. Preprint at 
          https://doi.org/10.48550/arXiv.2210.11416
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR2" id="ref-link-section-d77463470e2563">2</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 15"
                                            title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR15"
                                            id="ref-link-section-d77463470e2566">15</a>,<a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 16" title="Wei, J. et al. Chain of thought prompting elicits reasoning in large language models. Preprint at 
          https://doi.org/10.48550/arXiv.2201.11903
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR16" id="ref-link-section-d77463470e2569">16</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 67"
                                            title="Wei, J. et al. Finetuned language models are zero-shot learners. Preprint at 
          https://doi.org/10.48550/arXiv.2109.01652
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR67" id="ref-link-section-d77463470e2572">67</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" title="Hoffmann, J. et al. Training compute-optimal large language models. Preprint at 
          https://doi.org/10.48550/arXiv.2203.15556
          
         (2022)." href="#ref-CR73" id="ref-link-section-d77463470e2575">73</a>,<a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" title="Scao, T. L. et al. BLOOM: a 176B-parameter open-access multilingual language model. Preprint at 
          https://doi.org/10.48550/arXiv.2211.05100
          
         (2022)." href="#ref-CR74" id="ref-link-section-d77463470e2575_1">74</a>,<a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" title="Rae, J. W. et al. Scaling language models: methods, analysis &amp; insights from training Gopher. Preprint at 
          https://doi.org/10.48550/arXiv.2112.11446
          
         (2021)." href="#ref-CR75" id="ref-link-section-d77463470e2575_2">75</a>,<a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref"
                                            title="Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 1–67 (2020)."
                                            href="#ref-CR76" id="ref-link-section-d77463470e2575_3">76</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 77"
                                            title="Zhang, S. et al. OPT: open pre-trained transformer language models. Preprint at 
          https://doi.org/10.48550/arXiv.2205.01068
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR77" id="ref-link-section-d77463470e2579">77</a></sup>. They
                                    owe their success to scaling up the training of transformer-based models<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 78"
                                            title="Vaswani, A. et al. Attention is all you need. In 31st Conference on Neural Information Processing Systems (Association of Computational Machinery, 2017)."
                                            href="/articles/s41586-023-06291-2#ref-CR78"
                                            id="ref-link-section-d77463470e2583">78</a></sup>. It has been shown that
                                    model
                                    performance and data-efficiency scales with model size and dataset size<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 79"
                                            title="Kaplan, J. et al. Scaling laws for neural language models. Preprint at 
          https://doi.org/10.48550/arXiv.2001.08361
          
         (2020)." href="/articles/s41586-023-06291-2#ref-CR79" id="ref-link-section-d77463470e2587">79</a></sup>. LLMs
                                    are often trained using self-supervision on a large scale, using general-purpose
                                    text
                                    corpi such as Wikipedia and BooksCorpus. They have demonstrated promising results
                                    across
                                    a wide range of tasks, including tasks that require specialized scientific knowledge
                                    and
                                    reasoning<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 6"
                                            title="Hendrycks, D. et al. Measuring massive multitask language understanding. Preprint at 
          https://doi.org/10.48550/arXiv.2009.03300
          
         (2020)." href="/articles/s41586-023-06291-2#ref-CR6" id="ref-link-section-d77463470e2591">6</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 62"
                                            title="Cobbe, K. et al. Training verifiers to solve math word problems. Preprint at 
          https://doi.org/10.48550/arXiv.2110.14168
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR62" id="ref-link-section-d77463470e2594">62</a></sup>.
                                    Perhaps the most interesting aspect of these LLMs is their in-context few-shot
                                    abilities, which adapt these models to diverse tasks without gradient-based
                                    parameter
                                    updates<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 15"
                                            title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR15"
                                            id="ref-link-section-d77463470e2598">15</a>,<a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 67" title="Wei, J. et al. Finetuned language models are zero-shot learners. Preprint at 
          https://doi.org/10.48550/arXiv.2109.01652
          
         (2021)." href="/articles/s41586-023-06291-2#ref-CR67" id="ref-link-section-d77463470e2601">67</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 80"
                                            title="Lampinen, A. K. et al. Can language models learn from explanations in context? Preprint at 
          https://doi.org/10.48550/arXiv.2204.02329
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR80" id="ref-link-section-d77463470e2604">80</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 81"
                                            title="Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. &amp; Iwasawa, Y. Large language models are zero-shot reasoners. Preprint at 
          https://doi.org/10.48550/arXiv.2205.11916
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR81" id="ref-link-section-d77463470e2607">81</a></sup>. This
                                    allows them to rapidly generalize to unseen tasks and even exhibit apparent
                                    reasoning
                                    abilities with appropriate prompting strategies<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 1" title="Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at 
          https://doi.org/10.48550/arXiv.2204.02311
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR1" id="ref-link-section-d77463470e2612">1</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 16"
                                            title="Wei, J. et al. Chain of thought prompting elicits reasoning in large language models. Preprint at 
          https://doi.org/10.48550/arXiv.2201.11903
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR16" id="ref-link-section-d77463470e2615">16</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 20"
                                            title="Taylor, R. et al. Galactica: a large language model for science. Preprint at 
          https://doi.org/10.48550/arXiv.2211.09085
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR20" id="ref-link-section-d77463470e2618">20</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 63"
                                            title="Lewkowycz, A. et al. Solving quantitative reasoning problems with language models. Preprint at 
          https://doi.org/10.48550/arXiv.2206.14858
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR63" id="ref-link-section-d77463470e2621">63</a></sup>.</p>
                                <p>Several studies have shown that LLMs have the capacity to act as implicit knowledge
                                    bases<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 6"
                                            title="Hendrycks, D. et al. Measuring massive multitask language understanding. Preprint at 
          https://doi.org/10.48550/arXiv.2009.03300
          
         (2020)." href="/articles/s41586-023-06291-2#ref-CR6" id="ref-link-section-d77463470e2628">6</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 20"
                                            title="Taylor, R. et al. Galactica: a large language model for science. Preprint at 
          https://doi.org/10.48550/arXiv.2211.09085
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR20" id="ref-link-section-d77463470e2631">20</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 82"
                                            title="Joshi, M., Choi, E., Weld, D. S. &amp; Zettlemoyer, L. TriviaQA: a large scale distantly supervised challenge dataset for reading comprehension. Preprint at 
          https://doi.org/10.48550/arXiv.1705.03551
          
         (2017)." href="/articles/s41586-023-06291-2#ref-CR82" id="ref-link-section-d77463470e2634">82</a></sup>.
                                    However, there is a significant risk of these models producing hallucinations,
                                    amplifying social biases present in their training data, and displaying deficiencies
                                    in
                                    their reasoning abilities. To examine the current limitations of LLMs and to
                                    quantify
                                    the large gap between human and LLM language capabilities, BIG-bench was introduced
                                    as a
                                    community-wide initiative to benchmark on tasks that were believed at time of
                                    publication to be beyond the capabilities of current language models<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 57"
                                            title="Srivastava, A. et al. Beyond the imitation game: quantifying and extrapolating the capabilities of language models. Preprint at 
          https://doi.org/10.48550/arXiv.2206.04615
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR57" id="ref-link-section-d77463470e2638">57</a></sup>.</p>
                                <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec55">LLMs for
                                    science
                                    and biomedicine</h4>
                                <p>Recent studies, such as SciBERT<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 83" title="Beltagy, I., Lo, K. &amp; Cohan, A. SciBERT: a pretrained language model for scientific text. Preprint at 
          https://doi.org/10.48550/arXiv.1903.10676
          
         (2019)." href="/articles/s41586-023-06291-2#ref-CR83" id="ref-link-section-d77463470e2650">83</a></sup>,
                                    BioNLP<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 84"
                                            title="Lewis, P., Ott, M., Du, J. &amp; Stoyanov, V. Pretrained language models for biomedical and clinical tasks: Understanding and extending the state-of-the-art. In Proc. 3rd Clinical Natural Language Processing Workshop (eds Roberts, K., Bethard, S. &amp; Naumann, T.) 146–157 (Association for Computational Linguistics, 2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR84"
                                            id="ref-link-section-d77463470e2654">84</a></sup>, BioMegatron<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 85"
                                            title="Shin, H.-C. et al. BioMegatron: larger biomedical domain language model. Preprint at 
          https://doi.org/10.48550/arXiv.2010.06060
          
         (2020)." href="/articles/s41586-023-06291-2#ref-CR85" id="ref-link-section-d77463470e2658">85</a></sup>,
                                    BioBERT<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 86"
                                            title="Lee, J. et al. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36, 1234–1240 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR86"
                                            id="ref-link-section-d77463470e2662">86</a></sup>, PubMedBERT<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 87"
                                            title="Gu, Y. et al. Domain-specific language model pretraining for biomedical natural language processing. ACM Trans. Comput. Healthc. 3, 2 (2021)."
                                            href="/articles/s41586-023-06291-2#ref-CR87"
                                            id="ref-link-section-d77463470e2666">87</a></sup>, DARE<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 88"
                                            title="Papanikolaou, Y. &amp; Pierleoni, A. DARE: data augmented relation extraction with GPT-2. Preprint at 
          https://doi.org/10.48550/arXiv.2004.13845
          
         (2020)." href="/articles/s41586-023-06291-2#ref-CR88" id="ref-link-section-d77463470e2671">88</a></sup>,
                                    ScholarBERT<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 89"
                                            title="Hong, Z. et al. The diminishing returns of masked language models to science. Preprint at 
          https://doi.org/10.48550/arXiv.2205.11342
          
         (2023)." href="/articles/s41586-023-06291-2#ref-CR89" id="ref-link-section-d77463470e2675">89</a></sup>, and
                                    BioGPT<sup><a data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 21"
                                            title="Luo, R. et al. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Brief. Bioinformatics 23, bbac49 (2022)."
                                            href="/articles/s41586-023-06291-2#ref-CR21"
                                            id="ref-link-section-d77463470e2679">21</a></sup>, have demonstrated the
                                    effectiveness of using curated scientific and biomedical corpora for both
                                    discriminative
                                    and generative language modelling. These models, although promising, are typically
                                    small
                                    in scale and scope compared to LLMs such as GPT-3&nbsp;(ref.&nbsp;<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 15"
                                            title="Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901 (2020)."
                                            href="/articles/s41586-023-06291-2#ref-CR15"
                                            id="ref-link-section-d77463470e2683">15</a></sup>) and PaLM<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 1"
                                            title="Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at 
          https://doi.org/10.48550/arXiv.2204.02311
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR1" id="ref-link-section-d77463470e2687">1</a></sup>. While
                                    the medical domain is challenging, specific proposals for LLMs have already included
                                    examples as varied as augmenting non-critical clinical assessments to summarization
                                    of
                                    complex medical communications<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref"
                                            title="Korngiebel, D. M. &amp; Mooney, S. D. Considering the possibilities and pitfalls of generative pre-trained transformer 3 (GPT-3) in healthcare delivery. NPJ Digit. Med. 4, 93 (2021)."
                                            href="#ref-CR90" id="ref-link-section-d77463470e2691">90</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref"
                                            title="Sezgin, E., Sirrianni, J. &amp; Linwood, S. L. et al. Operationalizing and implementing pretrained, large artificial intelligence linguistic models in the us health care system: outlook of generative pretrained transformer 3 (GPT-3) as a service model. JMIR Med. Informatics 10, e32875 (2022)."
                                            href="#ref-CR91" id="ref-link-section-d77463470e2691_1">91</a>,<a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 92"
                                            title="Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. &amp; Sontag, D. Large language models are zero-shot clinical information extractors. Preprint at 
          https://doi.org/10.48550/arXiv.2205.12689
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR92" id="ref-link-section-d77463470e2694">92</a></sup>.</p>
                                <p>The closest precedents to our work are Galactica<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 20" title="Taylor, R. et al. Galactica: a large language model for science. Preprint at 
          https://doi.org/10.48550/arXiv.2211.09085
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR20" id="ref-link-section-d77463470e2701">20</a></sup>, an
                                    LLM for science, and another work studying the reasoning capability of LLMs in the
                                    medical question-answering context<sup><a data-track="click"
                                            data-track-action="reference anchor" data-track-label="link"
                                            data-test="citation-ref" aria-label="Reference 93" title="Liévin, V., Hother, C. E. &amp; Winther, O. Can large language models reason about medical questions? Preprint at 
          https://doi.org/10.48550/arXiv.2207.08143
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR93" id="ref-link-section-d77463470e2705">93</a></sup>. The
                                    latter work used GPT-3.5 (Codex and InstructGPT), an instruction-tuned LLM<sup><a
                                            data-track="click" data-track-action="reference anchor"
                                            data-track-label="link" data-test="citation-ref" aria-label="Reference 94"
                                            title="Ouyang, L. et al. Training language models to follow instructions with human feedback. Preprint at 
          https://doi.org/10.48550/arXiv.2203.02155
          
         (2022)." href="/articles/s41586-023-06291-2#ref-CR94" id="ref-link-section-d77463470e2709">94</a></sup> and
                                    evaluated on the MedQA, MedMCQA, and PubMedQA datasets.</p>
                                <h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec56">Reporting
                                    summary</h3>
                                <p>Further information on research design is available in the&nbsp;<a data-track="click"
                                        data-track-label="link" data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM2">Nature Portfolio Reporting
                                        Summary</a>
                                    linked to this article.</p>
                            </div>
                        </div>
                    </section>
                </div>


                <div>
                    <section data-title="Data availability">
                        <div class="c-article-section" id="data-availability-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="data-availability">Data availability</h2>
                            <div class="c-article-section__content" id="data-availability-content">

                                <p>The benchmark used in the study, MultiMedQA, comprises six open source datasets and
                                    one
                                    for consumer medical questions, HealthSearchQA, which we introduce here and are
                                    releasing with this work as a supplementary file.</p>
                            </div>
                        </div>
                    </section>
                    <section data-title="Code availability">
                        <div class="c-article-section" id="code-availability-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="code-availability">Code availability</h2>
                            <div class="c-article-section__content" id="code-availability-content">

                                <p>Med-PaLM is an LLM that has been aligned to the medical domain. We are not
                                    open-sourcing
                                    model code and weights owing to the safety implications of unmonitored use of such a
                                    model in medical settings. In the interest of responsible innovation, we will be
                                    working
                                    with academic and industry research partners, providers, regulators and policy
                                    stakeholders to validate and explore safe onward uses of Med-PaLM. For
                                    reproducibility,
                                    we documented technical deep learning methods while keeping the paper accessible to
                                    a
                                    clinical and general scientific audience. Our work builds upon PaLM, for which
                                    technical
                                    details have been described extensively, and our institution has open-sourced
                                    several
                                    related LLMs to further the development of research methods in the field (<a
                                        href="https://huggingface.co/google/flan-t5-xl">https://huggingface.co/google/flan-t5-xl</a>).
                                </p>
                            </div>
                        </div>
                    </section>
                    <section data-title="Change history">
                        <div class="c-article-section" id="change-history-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="change-history">Change history</h2>
                            <div class="c-article-section__content" id="change-history-content">
                                <ul class="c-article-change-list">
                                    <li class="c-article-change-list__item u-mb-24" id="chg1"><ins
                                            datetime="2023-07-27">
                                            <h3 class="c-article-change-list__heading u-h3 u-pr-8 u-display-inline">27
                                                July
                                                2023</h3>
                                            <div class="c-article-change-list__details">
                                                <p>A Correction to this paper has been published: <a
                                                        href="https://doi.org/10.1038/s41586-023-06455-0">https://doi.org/10.1038/s41586-023-06455-0</a>
                                                </p>
                                            </div>
                                        </ins></li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <div id="MagazineFulltextArticleBodySuffix">
                        <section aria-labelledby="Bib1" data-title="References">
                            <div class="c-article-section" id="Bib1-section">
                                <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                    id="Bib1">References</h2>
                                <div class="c-article-section__content" id="Bib1-content">
                                    <div data-container-section="references">
                                        <ol class="c-article-references" data-track-component="outbound reference">
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="1.">
                                                <p class="c-article-references__text" id="ref-CR1">Chowdhery, A. et al.
                                                    PaLM: scaling language modeling with pathways. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2204.02311">https://doi.org/10.48550/arXiv.2204.02311</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="2.">
                                                <p class="c-article-references__text" id="ref-CR2">Chung, H. W. et al.
                                                    Scaling instruction-finetuned language models. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2210.11416">https://doi.org/10.48550/arXiv.2210.11416</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="3.">
                                                <p class="c-article-references__text" id="ref-CR3">Jin, D. et al. What
                                                    disease does this patient have? A large-scale open domain question
                                                    answering dataset from medical exams. <i>Appl. Sci.</i> <b>11</b>,
                                                    6421
                                                    (2021).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.3390/app11146421"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.3390%2Fapp11146421"
                                                        aria-label="Article reference 3"
                                                        data-doi="10.3390/app11146421">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="cas reference"
                                                        href="/articles/cas-redirect/1:CAS:528:DC%2BB3MXitV2ru7vE"
                                                        aria-label="CAS reference 3">CAS</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 3"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=What%20disease%20does%20this%20patient%20have%3F%20A%20large-scale%20open%20domain%20question%20answering%20dataset%20from%20medical%20exams&amp;journal=Appl.%20Sci.&amp;doi=10.3390%2Fapp11146421&amp;volume=11&amp;publication_year=2021&amp;author=Jin%2CD">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="4.">
                                                <p class="c-article-references__text" id="ref-CR4">Pal, A., Umapathi, L.
                                                    K.
                                                    &amp; Sankarasubbu, M. MedMCQA: a large-scale multi-subject
                                                    multi-choice
                                                    dataset for medical domain question answering. In <i>Conference on
                                                        Health, Inference, and Learning</i> 248–260 (Proceedings of
                                                    Machine
                                                    Learning Research, 2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="5.">
                                                <p class="c-article-references__text" id="ref-CR5">Jin, Q., Dhingra, B.,
                                                    Liu, Z., Cohen, W. W. &amp; Lu, X. PubMedQA: a dataset for
                                                    biomedical
                                                    research question answering. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.1909.06146">https://doi.org/10.48550/arXiv.1909.06146</a>
                                                    (2019).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="6.">
                                                <p class="c-article-references__text" id="ref-CR6">Hendrycks, D. et al.
                                                    Measuring massive multitask language understanding. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2009.03300">https://doi.org/10.48550/arXiv.2009.03300</a>
                                                    (2020).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="7.">
                                                <p class="c-article-references__text" id="ref-CR7">Esteva, A. et al.
                                                    Deep
                                                    learning-enabled medical computer vision. <i>NPJ Digit. Med.</i>
                                                    <b>4</b>, 5 (2021).
                                                </p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1038/s41746-020-00376-2"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1038%2Fs41746-020-00376-2"
                                                        aria-label="Article reference 7"
                                                        data-doi="10.1038/s41746-020-00376-2">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33420381"
                                                        aria-label="PubMed reference 7">PubMed</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link"
                                                        data-track-action="pubmed central reference"
                                                        href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7794558"
                                                        aria-label="PubMed Central reference 7">PubMed Central</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 7"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning-enabled%20medical%20computer%20vision&amp;journal=NPJ%20Digit.%20Med.&amp;doi=10.1038%2Fs41746-020-00376-2&amp;volume=4&amp;publication_year=2021&amp;author=Esteva%2CA">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="8.">
                                                <p class="c-article-references__text" id="ref-CR8">Tomašev, N. et al.
                                                    Use of
                                                    deep learning to develop continuous-risk models for adverse event
                                                    prediction from electronic health records. <i>Nat. Protoc.</i>
                                                    <b>16</b>, 2765–2787 (2021).
                                                </p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1038/s41596-021-00513-5"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1038%2Fs41596-021-00513-5"
                                                        aria-label="Article reference 8"
                                                        data-doi="10.1038/s41596-021-00513-5">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33953393"
                                                        aria-label="PubMed reference 8">PubMed</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 8"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Use%20of%20deep%20learning%20to%20develop%20continuous-risk%20models%20for%20adverse%20event%20prediction%20from%20electronic%20health%20records&amp;journal=Nat.%20Protoc.&amp;doi=10.1038%2Fs41596-021-00513-5&amp;volume=16&amp;pages=2765-2787&amp;publication_year=2021&amp;author=Toma%C5%A1ev%2CN">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="9.">
                                                <p class="c-article-references__text" id="ref-CR9">Yim, J. et al.
                                                    Predicting
                                                    conversion to wet age-related macular degeneration using deep
                                                    learning.
                                                    <i>Nat. Med.</i> <b>26</b>, 892–899 (2020).
                                                </p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1038/s41591-020-0867-7"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1038%2Fs41591-020-0867-7"
                                                        aria-label="Article reference 9"
                                                        data-doi="10.1038/s41591-020-0867-7">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="cas reference"
                                                        href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXpsFyjs7g%3D"
                                                        aria-label="CAS reference 9">CAS</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32424211"
                                                        aria-label="PubMed reference 9">PubMed</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 9"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Predicting%20conversion%20to%20wet%20age-related%20macular%20degeneration%20using%20deep%20learning&amp;journal=Nat.%20Med.&amp;doi=10.1038%2Fs41591-020-0867-7&amp;volume=26&amp;pages=892-899&amp;publication_year=2020&amp;author=Yim%2CJ">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="10.">
                                                <p class="c-article-references__text" id="ref-CR10">Lakkaraju, H.,
                                                    Slack,
                                                    D., Chen, Y., Tan, C. &amp; Singh, S. Rethinking explainability as a
                                                    dialogue: a practitioner’s perspective. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2202.01875">https://doi.org/10.48550/arXiv.2202.01875</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="11.">
                                                <p class="c-article-references__text" id="ref-CR11">Bommasani, R. et al.
                                                    On
                                                    the opportunities and risks of foundation models. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2108.07258">https://doi.org/10.48550/arXiv.2108.07258</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="12.">
                                                <p class="c-article-references__text" id="ref-CR12">Papineni, K.,
                                                    Roukos,
                                                    S., Ward, T. &amp; Zhu, W.-J. BLEU: a method for automatic
                                                    evaluation of
                                                    machine translation. In <i>Proc. 40th Annual Meeting of the
                                                        Association
                                                        for Computational Linguistics</i> 311–318 (Association of
                                                    Computational Machinery, 2002).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="13.">
                                                <p class="c-article-references__text" id="ref-CR13">Ben Abacha, A.,
                                                    Agichtein, E., Pinter, Y. &amp; Demner-Fushman, D. Overview of the
                                                    medical question answering task at TREC 2017 LiveQA. <i>TREC</i> <a
                                                        href="https://trec.nist.gov/pubs/trec26/papers/Overview-QA.pdf?ref=https://githubhelp.com">https://trec.nist.gov/pubs/trec26/papers/Overview-QA.pdf?ref=https://githubhelp.com</a>
                                                    (2017).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="14.">
                                                <p class="c-article-references__text" id="ref-CR14">Abacha, A. B. et al.
                                                    in
                                                    <i>Studies in Health Technology and Informatics</i> (eds
                                                    Ohno-Machado,
                                                    L. &amp; Séroussi, B.) 25–29 (IOS Press, 2019).
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="15.">
                                                <p class="c-article-references__text" id="ref-CR15">Brown, T. et al.
                                                    Language models are few-shot learners. <i>Adv. Neural Inf. Process.
                                                        Syst.</i> <b>33</b>, 1877–1901 (2020).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 15"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Language%20models%20are%20few-shot%20learners&amp;journal=Adv.%20Neural%20Inf.%20Process.%20Syst.&amp;volume=33&amp;pages=1877-1901&amp;publication_year=2020&amp;author=Brown%2CT">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="16.">
                                                <p class="c-article-references__text" id="ref-CR16">Wei, J. et al. Chain
                                                    of
                                                    thought prompting elicits reasoning in large language models.
                                                    Preprint
                                                    at <a
                                                        href="https://doi.org/10.48550/arXiv.2201.11903">https://doi.org/10.48550/arXiv.2201.11903</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="17.">
                                                <p class="c-article-references__text" id="ref-CR17">Wang, X. et al.
                                                    Self-consistency improves chain of thought reasoning in language
                                                    models.
                                                    Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2203.11171">https://doi.org/10.48550/arXiv.2203.11171</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="18.">
                                                <p class="c-article-references__text" id="ref-CR18">Yasunaga, M. et al.
                                                    Deep
                                                    bidirectional language-knowledge graph pretraining. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2210.09338">https://doi.org/10.48550/arXiv.2210.09338</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="19.">
                                                <p class="c-article-references__text" id="ref-CR19">Bolton, E. et al.
                                                    Stanford CRFM introduces PubMedGPT 2.7B. <i>Stanford University</i>
                                                    <a
                                                        href="https://hai.stanford.edu/news/stanford-crfm-introduces-pubmedgpt-27b">https://hai.stanford.edu/news/stanford-crfm-introduces-pubmedgpt-27b</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="20.">
                                                <p class="c-article-references__text" id="ref-CR20">Taylor, R. et al.
                                                    Galactica: a large language model for science. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2211.09085">https://doi.org/10.48550/arXiv.2211.09085</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="21.">
                                                <p class="c-article-references__text" id="ref-CR21">Luo, R. et al.
                                                    BioGPT:
                                                    generative pre-trained transformer for biomedical text generation
                                                    and
                                                    mining. <i>Brief. Bioinformatics</i> <b>23</b>, bbac49 (2022).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1093/bib/bbac409"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1093%2Fbib%2Fbbac409"
                                                        aria-label="Article reference 21"
                                                        data-doi="10.1093/bib/bbac409">Article</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 21"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=BioGPT%3A%20generative%20pre-trained%20transformer%20for%20biomedical%20text%20generation%20and%20mining&amp;journal=Brief.%20Bioinformatics&amp;doi=10.1093%2Fbib%2Fbbac409&amp;volume=23&amp;publication_year=2022&amp;author=Luo%2CR">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="22.">
                                                <p class="c-article-references__text" id="ref-CR22">Lin, S., Hilton, J.
                                                    &amp; Evans, O. Teaching models to express their uncertainty in
                                                    words.
                                                    Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2205.14334">https://doi.org/10.48550/arXiv.2205.14334</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="23.">
                                                <p class="c-article-references__text" id="ref-CR23">Kadavath, S. et al.
                                                    Language models (mostly) know what they know. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2207.05221">https://doi.org/10.48550/arXiv.2207.05221</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="24.">
                                                <p class="c-article-references__text" id="ref-CR24">Tran, D. et al.
                                                    Plex:
                                                    towards reliability using pretrained large model extensions.
                                                    Preprint at
                                                    <a
                                                        href="https://doi.org/10.48550/arXiv.2207.07411">https://doi.org/10.48550/arXiv.2207.07411</a>
                                                    (2022).
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="25.">
                                                <p class="c-article-references__text" id="ref-CR25">Feng, S. Y., Khetan,
                                                    V.,
                                                    Sacaleanu, B., Gershman, A. &amp; Hovy, E. CHARD: clinical
                                                    health-aware
                                                    reasoning across dimensions for text generation models. Preprint at
                                                    <a
                                                        href="https://doi.org/10.48550/arXiv.2210.04191">https://doi.org/10.48550/arXiv.2210.04191</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="26.">
                                                <p class="c-article-references__text" id="ref-CR26">Williams, T.,
                                                    Szekendi,
                                                    M., Pavkovic, S., Clevenger, W. &amp; Cerese, J. The reliability of
                                                    ahrq
                                                    common format harm scales in rating patient safety events. <i>J.
                                                        Patient
                                                        Saf.</i> <b>11</b>, 52–59 (2015).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1097/PTS.0b013e3182948ef9"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1097%2FPTS.0b013e3182948ef9"
                                                        aria-label="Article reference 26"
                                                        data-doi="10.1097/PTS.0b013e3182948ef9">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24080718"
                                                        aria-label="PubMed reference 26">PubMed</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 26"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=The%20reliability%20of%20ahrq%20common%20format%20harm%20scales%20in%20rating%20patient%20safety%20events&amp;journal=J.%20Patient%20Saf.&amp;doi=10.1097%2FPTS.0b013e3182948ef9&amp;volume=11&amp;pages=52-59&amp;publication_year=2015&amp;author=Williams%2CT&amp;author=Szekendi%2CM&amp;author=Pavkovic%2CS&amp;author=Clevenger%2CW&amp;author=Cerese%2CJ">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="27.">
                                                <p class="c-article-references__text" id="ref-CR27">Walsh, K. E. et al.
                                                    Measuring harm in healthcare: optimizing adverse event review.
                                                    <i>Med.
                                                        Care</i> <b>55</b>, 436 (2017).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1097/MLR.0000000000000679"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1097%2FMLR.0000000000000679"
                                                        aria-label="Article reference 27"
                                                        data-doi="10.1097/MLR.0000000000000679">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27906769"
                                                        aria-label="PubMed reference 27">PubMed</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link"
                                                        data-track-action="pubmed central reference"
                                                        href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5352561"
                                                        aria-label="PubMed Central reference 27">PubMed
                                                        Central</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 27"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20harm%20in%20healthcare%3A%20optimizing%20adverse%20event%20review&amp;journal=Med.%20Care&amp;doi=10.1097%2FMLR.0000000000000679&amp;volume=55&amp;publication_year=2017&amp;author=Walsh%2CKE">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="28.">
                                                <p class="c-article-references__text" id="ref-CR28">Wei, J. et al.
                                                    Emergent
                                                    abilities of large language models. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2206.07682">https://doi.org/10.48550/arXiv.2206.07682</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="29.">
                                                <p class="c-article-references__text" id="ref-CR29">Kington, R. S. et
                                                    al.
                                                    Identifying credible sources of health information in social media:
                                                    principles and attributes. <i>NAM Perspectives</i> <a
                                                        href="https://doi.org/10.31478%2F202107a">https://doi.org/10.31478%2F202107a</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="30.">
                                                <p class="c-article-references__text" id="ref-CR30">Mandavilli, A.
                                                    Medical
                                                    journals blind to racism as health crisis, critics say. <i>The New
                                                        York
                                                        Times</i> <a
                                                        href="https://www.nytimes.com/2021/06/02/health/jama-racism-bauchner.html">https://www.nytimes.com/2021/06/02/health/jama-racism-bauchner.html</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="31.">
                                                <p class="c-article-references__text" id="ref-CR31">Shoemaker, S. J.,
                                                    Wolf,
                                                    M. S. &amp; Brach, C. Development of the patient education materials
                                                    assessment tool (pemat): a new measure of understandability and
                                                    actionability for print and audiovisual patient information.
                                                    <i>Patient
                                                        Educ. Couns.</i> <b>96</b>, 395–403 (2014).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1016/j.pec.2014.05.027"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1016%2Fj.pec.2014.05.027"
                                                        aria-label="Article reference 31"
                                                        data-doi="10.1016/j.pec.2014.05.027">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24973195"
                                                        aria-label="PubMed reference 31">PubMed</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link"
                                                        data-track-action="pubmed central reference"
                                                        href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5085258"
                                                        aria-label="PubMed Central reference 31">PubMed
                                                        Central</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 31"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20the%20patient%20education%20materials%20assessment%20tool%20%28pemat%29%3A%20a%20new%20measure%20of%20understandability%20and%20actionability%20for%20print%20and%20audiovisual%20patient%20information&amp;journal=Patient%20Educ.%20Couns.&amp;doi=10.1016%2Fj.pec.2014.05.027&amp;volume=96&amp;pages=395-403&amp;publication_year=2014&amp;author=Shoemaker%2CSJ&amp;author=Wolf%2CMS&amp;author=Brach%2CC">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="32.">
                                                <p class="c-article-references__text" id="ref-CR32">Boateng, G. O.,
                                                    Neilands, T. B., Frongillo, E. A., Melgar-Quiñonez, H. R. &amp;
                                                    Young,
                                                    S. L. Best practices for developing and validating scales for
                                                    health,
                                                    social, and behavioral research: a primer. <i>Front. Public
                                                        Health</i>
                                                    <b>6</b>, 149 (2018).
                                                </p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.3389/fpubh.2018.00149"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.3389%2Ffpubh.2018.00149"
                                                        aria-label="Article reference 32"
                                                        data-doi="10.3389/fpubh.2018.00149">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29942800"
                                                        aria-label="PubMed reference 32">PubMed</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link"
                                                        data-track-action="pubmed central reference"
                                                        href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6004510"
                                                        aria-label="PubMed Central reference 32">PubMed
                                                        Central</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 32"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Best%20practices%20for%20developing%20and%20validating%20scales%20for%20health%2C%20social%2C%20and%20behavioral%20research%3A%20a%20primer&amp;journal=Front.%20Public%20Health&amp;doi=10.3389%2Ffpubh.2018.00149&amp;volume=6&amp;publication_year=2018&amp;author=Boateng%2CGO&amp;author=Neilands%2CTB&amp;author=Frongillo%2CEA&amp;author=Melgar-Qui%C3%B1onez%2CHR&amp;author=Young%2CSL">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="33.">
                                                <p class="c-article-references__text" id="ref-CR33">Hooker, S. Moving
                                                    beyond
                                                    “algorithmic bias is a data problem”. <i>Patterns</i> <b>2</b>,
                                                    100241
                                                    (2021).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1016/j.patter.2021.100241"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1016%2Fj.patter.2021.100241"
                                                        aria-label="Article reference 33"
                                                        data-doi="10.1016/j.patter.2021.100241">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33982031"
                                                        aria-label="PubMed reference 33">PubMed</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link"
                                                        data-track-action="pubmed central reference"
                                                        href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8085589"
                                                        aria-label="PubMed Central reference 33">PubMed
                                                        Central</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 33"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Moving%20beyond%20%E2%80%9Calgorithmic%20bias%20is%20a%20data%20problem%E2%80%9D&amp;journal=Patterns&amp;doi=10.1016%2Fj.patter.2021.100241&amp;volume=2&amp;publication_year=2021&amp;author=Hooker%2CS">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="34.">
                                                <p class="c-article-references__text" id="ref-CR34">Chen, I. Y. et al.
                                                    Ethical machine learning in healthcare. <i>Annu. Rev. Biomed. Data
                                                        Sci.</i> <b>4</b>, 123–144 (2021).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1146/annurev-biodatasci-092820-114757"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1146%2Fannurev-biodatasci-092820-114757"
                                                        aria-label="Article reference 34"
                                                        data-doi="10.1146/annurev-biodatasci-092820-114757">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34396058"
                                                        aria-label="PubMed reference 34">PubMed</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link"
                                                        data-track-action="pubmed central reference"
                                                        href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8362902"
                                                        aria-label="PubMed Central reference 34">PubMed
                                                        Central</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 34"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Ethical%20machine%20learning%20in%20healthcare&amp;journal=Annu.%20Rev.%20Biomed.%20Data%20Sci.&amp;doi=10.1146%2Fannurev-biodatasci-092820-114757&amp;volume=4&amp;pages=123-144&amp;publication_year=2021&amp;author=Chen%2CIY">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="35.">
                                                <p class="c-article-references__text" id="ref-CR35">Eneanya, N. D. et
                                                    al.
                                                    Health inequities and the inappropriate use of race in nephrology.
                                                    <i>Nat. Rev. Nephrol.</i> <b>18</b>, 84–94 (2022).
                                                </p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1038/s41581-021-00501-8"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1038%2Fs41581-021-00501-8"
                                                        aria-label="Article reference 35"
                                                        data-doi="10.1038/s41581-021-00501-8">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34750551"
                                                        aria-label="PubMed reference 35">PubMed</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 35"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Health%20inequities%20and%20the%20inappropriate%20use%20of%20race%20in%20nephrology&amp;journal=Nat.%20Rev.%20Nephrol.&amp;doi=10.1038%2Fs41581-021-00501-8&amp;volume=18&amp;pages=84-94&amp;publication_year=2022&amp;author=Eneanya%2CND">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="36.">
                                                <p class="c-article-references__text" id="ref-CR36">Vyas, L. G.,
                                                    Eisenstein,
                                                    L. G. &amp; Jones, D. S. Hidden in plain sight-reconsidering the use
                                                    of
                                                    race correction in clinical algorithms. <i>N. Engl. J. Med.</i>
                                                    <b>383</b>, 874–882 (2020).
                                                </p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1056/NEJMms2004740"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1056%2FNEJMms2004740"
                                                        aria-label="Article reference 36"
                                                        data-doi="10.1056/NEJMms2004740">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32853499"
                                                        aria-label="PubMed reference 36">PubMed</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 36"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Hidden%20in%20plain%20sight-reconsidering%20the%20use%20of%20race%20correction%20in%20clinical%20algorithms&amp;journal=N.%20Engl.%20J.%20Med.&amp;doi=10.1056%2FNEJMms2004740&amp;volume=383&amp;pages=874-882&amp;publication_year=2020&amp;author=Vyas%2CLG&amp;author=Eisenstein%2CLG&amp;author=Jones%2CDS">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="37.">
                                                <p class="c-article-references__text" id="ref-CR37">Weidinger, L. et al.
                                                    Ethical and social risks of harm from language models. Preprint at
                                                    <a
                                                        href="https://doi.org/10.48550/arXiv.2112.04359">https://doi.org/10.48550/arXiv.2112.04359</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="38.">
                                                <p class="c-article-references__text" id="ref-CR38">Liang, P. et al.
                                                    Holistic evaluation of language models. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2211.09110">https://doi.org/10.48550/arXiv.2211.09110</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="39.">
                                                <p class="c-article-references__text" id="ref-CR39">Liu, X. et al. The
                                                    medical algorithmic audit. <i>Lancet Digit. Health</i> <b>4</b>,
                                                    e384–e397 (2022).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1016/S2589-7500(22)00003-6"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1016%2FS2589-7500%2822%2900003-6"
                                                        aria-label="Article reference 39"
                                                        data-doi="10.1016/S2589-7500(22)00003-6">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="cas reference"
                                                        href="/articles/cas-redirect/1:CAS:528:DC%2BB3sXis1Smtbs%3D"
                                                        aria-label="CAS reference 39">CAS</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=35396183"
                                                        aria-label="PubMed reference 39">PubMed</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 39"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=The%20medical%20algorithmic%20audit&amp;journal=Lancet%20Digit.%20Health&amp;doi=10.1016%2FS2589-7500%2822%2900003-6&amp;volume=4&amp;pages=e384-e397&amp;publication_year=2022&amp;author=Liu%2CX">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="40.">
                                                <p class="c-article-references__text" id="ref-CR40">Raji, I. D. et al.
                                                    Closing the AI accountability gap: defining an end-to-end framework
                                                    for
                                                    internal algorithmic auditing. In <i>Proc. 2020 Conference on
                                                        Fairness,
                                                        Accountability, and Transparency</i> 33–44 (Association for
                                                    Computing Machinery, 2020).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="41.">
                                                <p class="c-article-references__text" id="ref-CR41">Rostamzadeh, N. et
                                                    al.
                                                    Healthsheet: development of a transparency artifact for health
                                                    datasets.
                                                    Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2202.13028">https://doi.org/10.48550/arXiv.2202.13028</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="42.">
                                                <p class="c-article-references__text" id="ref-CR42">Gebru, T. et al.
                                                    Datasheets for datasets. <i>Commun. ACM</i> <b>64</b>, 86–92 (2021).
                                                </p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1145/3458723"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1145%2F3458723"
                                                        aria-label="Article reference 42"
                                                        data-doi="10.1145/3458723">Article</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 42"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Datasheets%20for%20datasets&amp;journal=Commun.%20ACM&amp;doi=10.1145%2F3458723&amp;volume=64&amp;pages=86-92&amp;publication_year=2021&amp;author=Gebru%2CT">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="43.">
                                                <p class="c-article-references__text" id="ref-CR43">Mitchell, M. et al.
                                                    Model cards for model reporting. In <i>Proc. conference on Fairness,
                                                        Accountability, and Transparency</i> 220–229 (Association for
                                                    Computing Machinery, 2019).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="44.">
                                                <p class="c-article-references__text" id="ref-CR44">Garg, S. et al.
                                                    Counterfactual fairness in text classification through robustness.
                                                    In
                                                    <i>Proc. 2019 AAAI/ACM Conference on AI, Ethics, and Society</i>
                                                    219–226
                                                    (Association for Computing Machinery, 2019).
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="45.">
                                                <p class="c-article-references__text" id="ref-CR45">Prabhakaran, V.,
                                                    Hutchinson, B. &amp; Mitchell, M. Perturbation sensitivity analysis
                                                    to
                                                    detect unintended model biases. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.1910.04210">https://doi.org/10.48550/arXiv.1910.04210</a>
                                                    (2019).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="46.">
                                                <p class="c-article-references__text" id="ref-CR46">Zhang, H., Lu, A.
                                                    X.,
                                                    Abdalla, M., McDermott, M. &amp; Ghassemi, M. Hurtful words:
                                                    quantifying
                                                    biases in clinical contextual word embeddings. In <i>Proc. ACM
                                                        Conference on Health, Inference, and Learning</i> 110–120
                                                    (Association for Computing Machinery, 2020).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="47.">
                                                <p class="c-article-references__text" id="ref-CR47">Matheny, M., Israni,
                                                    S.
                                                    T., Ahmed, M. &amp; Whicher, D. eds. Artificial Intelligence in
                                                    Health
                                                    Care: The Hope, the Hype, the Promise, the Peril (National Academy
                                                    of
                                                    Medicine, 2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="48.">
                                                <p class="c-article-references__text" id="ref-CR48">The White House
                                                    Office
                                                    of Science and Technology Policy. <i>Blueprint for an AI Bill of
                                                        Rights:
                                                        Making Automated Systems Work for the American People</i> <a
                                                        href="https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf">https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf</a>
                                                    (The White House, 2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="49.">
                                                <p class="c-article-references__text" id="ref-CR49"><i>Ethics and
                                                        Governance
                                                        of Artificial Intelligence for Health</i>. WHO Guidance (World
                                                    Health Organization, 2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="50.">
                                                <p class="c-article-references__text" id="ref-CR50">Bommasani, R.,
                                                    Liang, P.
                                                    &amp; Lee, T. Language models are changing AI: the need for holistic
                                                    evaluation. <i>Stanford University</i> <a
                                                        href="https://crfm.stanford.edu/2022/11/17/helm.html">https://crfm.stanford.edu/2022/11/17/helm.html</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="51.">
                                                <p class="c-article-references__text" id="ref-CR51">Pampari, A.,
                                                    Raghavan,
                                                    P., Liang, J. &amp; Peng, J. emrQA: a large corpus for question
                                                    answering on electronic medical records. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.1809.00732">https://doi.org/10.48550/arXiv.1809.00732</a>
                                                    (2018).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="52.">
                                                <p class="c-article-references__text" id="ref-CR52">Tsatsaronis, G. et
                                                    al.
                                                    An overview of the bioasq large-scale biomedical semantic indexing
                                                    and
                                                    question answering competition. <i>BMC Bioinformatics</i> <b>16</b>,
                                                    138
                                                    (2015).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1186/s12859-015-0564-6"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1186%2Fs12859-015-0564-6"
                                                        aria-label="Article reference 52"
                                                        data-doi="10.1186/s12859-015-0564-6">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25925131"
                                                        aria-label="PubMed reference 52">PubMed</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link"
                                                        data-track-action="pubmed central reference"
                                                        href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4450488"
                                                        aria-label="PubMed Central reference 52">PubMed
                                                        Central</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 52"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=An%20overview%20of%20the%20bioasq%20large-scale%20biomedical%20semantic%20indexing%20and%20question%20answering%20competition&amp;journal=BMC%20Bioinformatics&amp;doi=10.1186%2Fs12859-015-0564-6&amp;volume=16&amp;publication_year=2015&amp;author=Tsatsaronis%2CG">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="53.">
                                                <p class="c-article-references__text" id="ref-CR53">Morgado, F. F.,
                                                    Meireles, J. F., Neves, C., Amaral, A. &amp; Ferreira, M. E. Scale
                                                    development: ten main limitations and recommendations to improve
                                                    future
                                                    research practices. <i>Psic. Reflex. Crit.</i> <b>30</b>, 5 (2017).
                                                </p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1186/s41155-017-0059-7"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1186%2Fs41155-017-0059-7"
                                                        aria-label="Article reference 53"
                                                        data-doi="10.1186/s41155-017-0059-7">Article</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 53"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Scale%20development%3A%20ten%20main%20limitations%20and%20recommendations%20to%20improve%20future%20research%20practices&amp;journal=Psic.%20Reflex.%20Crit.&amp;doi=10.1186%2Fs41155-017-0059-7&amp;volume=30&amp;publication_year=2017&amp;author=Morgado%2CFF&amp;author=Meireles%2CJF&amp;author=Neves%2CC&amp;author=Amaral%2CA&amp;author=Ferreira%2CME">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="54.">
                                                <p class="c-article-references__text" id="ref-CR54">Barham, P. et al.
                                                    Pathways: asynchronous distributed dataflow for ML. <i>Proc. Mach.
                                                        Learn. Syst.</i> <b>4</b>, 430–449 (2022).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 54"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Pathways%3A%20asynchronous%20distributed%20dataflow%20for%20ML&amp;journal=Proc.%20Mach.%20Learn.%20Syst.&amp;volume=4&amp;pages=430-449&amp;publication_year=2022&amp;author=Barham%2CP">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="55.">
                                                <p class="c-article-references__text" id="ref-CR55">Thoppilan, R. et al.
                                                    Lamda: language models for dialog applications. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2201.08239">https://doi.org/10.48550/arXiv.2201.08239</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="56.">
                                                <p class="c-article-references__text" id="ref-CR56">Du, N. et al. Glam:
                                                    efficient scaling of language models with mixture-of-experts. In
                                                    <i>International Conference on Machine Learning</i> 5547–5569 (PMLR,
                                                    2022).
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="57.">
                                                <p class="c-article-references__text" id="ref-CR57">Srivastava, A. et
                                                    al.
                                                    Beyond the imitation game: quantifying and extrapolating the
                                                    capabilities of language models. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2206.04615">https://doi.org/10.48550/arXiv.2206.04615</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="58.">
                                                <p class="c-article-references__text" id="ref-CR58">Clark, J. H. et al.
                                                    Tydi
                                                    qa: A benchmark for information-seeking question answering in
                                                    typologically diverse languages. <i>Trans. Assoc. Comput.
                                                        Linguist.</i>
                                                    <b>8</b>, 454–470 (2020).
                                                </p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1162/tacl_a_00317"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1162%2Ftacl_a_00317"
                                                        aria-label="Article reference 58"
                                                        data-doi="10.1162/tacl_a_00317">Article</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 58"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Tydi%20qa%3A%20A%20benchmark%20for%20information-seeking%20question%20answering%20in%20typologically%20diverse%20languages&amp;journal=Trans.%20Assoc.%20Comput.%20Linguist.&amp;doi=10.1162%2Ftacl_a_00317&amp;volume=8&amp;pages=454-470&amp;publication_year=2020&amp;author=Clark%2CJH">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="59.">
                                                <p class="c-article-references__text" id="ref-CR59">Lester, B., Al-Rfou,
                                                    R.
                                                    &amp; Constant, N. The power of scale for parameter-efficient prompt
                                                    tuning. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2104.08691">https://doi.org/10.48550/arXiv.2104.08691</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="60.">
                                                <p class="c-article-references__text" id="ref-CR60">Nye, M. et al. Show
                                                    your
                                                    work: scratchpads for intermediate computation with language models.
                                                    Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2112.00114">https://doi.org/10.48550/arXiv.2112.00114</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="61.">
                                                <p class="c-article-references__text" id="ref-CR61">Zhou, D. et al.
                                                    Least-to-most prompting enables complex reasoning in large language
                                                    models. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2205.10625">https://doi.org/10.48550/arXiv.2205.10625</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="62.">
                                                <p class="c-article-references__text" id="ref-CR62">Cobbe, K. et al.
                                                    Training verifiers to solve math word problems. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2110.14168">https://doi.org/10.48550/arXiv.2110.14168</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="63.">
                                                <p class="c-article-references__text" id="ref-CR63">Lewkowycz, A. et al.
                                                    Solving quantitative reasoning problems with language models.
                                                    Preprint
                                                    at <a
                                                        href="https://doi.org/10.48550/arXiv.2206.14858">https://doi.org/10.48550/arXiv.2206.14858</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="64.">
                                                <p class="c-article-references__text" id="ref-CR64">Ackley, D. H.,
                                                    Hinton,
                                                    G. E. &amp; Sejnowski, T. J. A learning algorithm for boltzmann
                                                    machines. <i>Cogn. Sci.</i> <b>9</b>, 147–169 (1985).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1207/s15516709cog0901_7"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1207%2Fs15516709cog0901_7"
                                                        aria-label="Article reference 64"
                                                        data-doi="10.1207/s15516709cog0901_7">Article</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 64"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=A%20learning%20algorithm%20for%20boltzmann%20machines&amp;journal=Cogn.%20Sci.&amp;doi=10.1207%2Fs15516709cog0901_7&amp;volume=9&amp;pages=147-169&amp;publication_year=1985&amp;author=Ackley%2CDH&amp;author=Hinton%2CGE&amp;author=Sejnowski%2CTJ">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="65.">
                                                <p class="c-article-references__text" id="ref-CR65">Ficler, J. &amp;
                                                    Goldberg, Y. Controlling linguistic style aspects in neural language
                                                    generation. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.1707.02633">https://doi.org/10.48550/arXiv.1707.02633</a>
                                                    (2017).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="66.">
                                                <p class="c-article-references__text" id="ref-CR66">Li, X. L. &amp;
                                                    Liang,
                                                    P. Prefix-tuning: optimizing continuous prompts for generation.
                                                    Preprint
                                                    at <a
                                                        href="https://doi.org/10.48550/arXiv.2101.00190">https://doi.org/10.48550/arXiv.2101.00190</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="67.">
                                                <p class="c-article-references__text" id="ref-CR67">Wei, J. et al.
                                                    Finetuned
                                                    language models are zero-shot learners. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2109.01652">https://doi.org/10.48550/arXiv.2109.01652</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="68.">
                                                <p class="c-article-references__text" id="ref-CR68">Liu, P. et al.
                                                    Pre-train, prompt, and predict: a systematic survey of prompting
                                                    methods
                                                    in natural language processing. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2107.13586">https://doi.org/10.48550/arXiv.2107.13586</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="69.">
                                                <p class="c-article-references__text" id="ref-CR69">Liu, X. et al. GPT
                                                    understands, too. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2103.10385">https://doi.org/10.48550/arXiv.2103.10385</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="70.">
                                                <p class="c-article-references__text" id="ref-CR70">Han, X., Zhao, W.,
                                                    Ding,
                                                    N., Liu, Z. &amp; Sun, M. PTR: prompt tuning with rules for text
                                                    classification. <i>AI Open</i> <b>3</b>, 182–192 (2022).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1016/j.aiopen.2022.11.003"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1016%2Fj.aiopen.2022.11.003"
                                                        aria-label="Article reference 70"
                                                        data-doi="10.1016/j.aiopen.2022.11.003">Article</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 70"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=PTR%3A%20prompt%20tuning%20with%20rules%20for%20text%20classification&amp;journal=AI%20Open&amp;doi=10.1016%2Fj.aiopen.2022.11.003&amp;volume=3&amp;pages=182-192&amp;publication_year=2022&amp;author=Han%2CX&amp;author=Zhao%2CW&amp;author=Ding%2CN&amp;author=Liu%2CZ&amp;author=Sun%2CM">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="71.">
                                                <p class="c-article-references__text" id="ref-CR71">Gu, Y., Han, X.,
                                                    Liu, Z.
                                                    &amp; Huang, M. PPT: Pre-trained prompt tuning for few-shot
                                                    learning.
                                                    Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2109.04332">https://doi.org/10.48550/arXiv.2109.04332</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="72.">
                                                <p class="c-article-references__text" id="ref-CR72">Ye, S., Jang, J.,
                                                    Kim,
                                                    D., Jo, Y. &amp; Seo, M. Retrieval of soft prompt enhances zero-shot
                                                    task generalization. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2210.03029">https://doi.org/10.48550/arXiv.2210.03029</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="73.">
                                                <p class="c-article-references__text" id="ref-CR73">Hoffmann, J. et al.
                                                    Training compute-optimal large language models. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2203.15556">https://doi.org/10.48550/arXiv.2203.15556</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="74.">
                                                <p class="c-article-references__text" id="ref-CR74">Scao, T. L. et al.
                                                    BLOOM: a 176B-parameter open-access multilingual language model.
                                                    Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2211.05100">https://doi.org/10.48550/arXiv.2211.05100</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="75.">
                                                <p class="c-article-references__text" id="ref-CR75">Rae, J. W. et al.
                                                    Scaling language models: methods, analysis &amp; insights from
                                                    training
                                                    Gopher. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2112.11446">https://doi.org/10.48550/arXiv.2112.11446</a>
                                                    (2021).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="76.">
                                                <p class="c-article-references__text" id="ref-CR76">Raffel, C. et al.
                                                    Exploring the limits of transfer learning with a unified
                                                    text-to-text
                                                    transformer. <i>J. Mach. Learn. Res.</i> <b>21</b>, 1–67 (2020).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="mathscinet reference"
                                                        href="http://www.ams.org/mathscinet-getitem?mr=4138124"
                                                        aria-label="MathSciNet reference 76">MathSciNet</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="math reference"
                                                        href="http://www.emis.de/MATH-item?07255171"
                                                        aria-label="MATH reference 76">MATH</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 76"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Exploring%20the%20limits%20of%20transfer%20learning%20with%20a%20unified%20text-to-text%20transformer&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=21&amp;pages=1-67&amp;publication_year=2020&amp;author=Raffel%2CC">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="77.">
                                                <p class="c-article-references__text" id="ref-CR77">Zhang, S. et al.
                                                    OPT:
                                                    open pre-trained transformer language models. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2205.01068">https://doi.org/10.48550/arXiv.2205.01068</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="78.">
                                                <p class="c-article-references__text" id="ref-CR78">Vaswani, A. et al.
                                                    Attention is all you need. In <i>31st Conference on Neural
                                                        Information
                                                        Processing Systems</i> (Association of Computational Machinery,
                                                    2017).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="79.">
                                                <p class="c-article-references__text" id="ref-CR79">Kaplan, J. et al.
                                                    Scaling laws for neural language models. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2001.08361">https://doi.org/10.48550/arXiv.2001.08361</a>
                                                    (2020).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="80.">
                                                <p class="c-article-references__text" id="ref-CR80">Lampinen, A. K. et
                                                    al.
                                                    Can language models learn from explanations in context? Preprint at
                                                    <a
                                                        href="https://doi.org/10.48550/arXiv.2204.02329">https://doi.org/10.48550/arXiv.2204.02329</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="81.">
                                                <p class="c-article-references__text" id="ref-CR81">Kojima, T., Gu, S.
                                                    S.,
                                                    Reid, M., Matsuo, Y. &amp; Iwasawa, Y. Large language models are
                                                    zero-shot reasoners. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2205.11916">https://doi.org/10.48550/arXiv.2205.11916</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="82.">
                                                <p class="c-article-references__text" id="ref-CR82">Joshi, M., Choi, E.,
                                                    Weld, D. S. &amp; Zettlemoyer, L. TriviaQA: a large scale distantly
                                                    supervised challenge dataset for reading comprehension. Preprint at
                                                    <a
                                                        href="https://doi.org/10.48550/arXiv.1705.03551">https://doi.org/10.48550/arXiv.1705.03551</a>
                                                    (2017).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="83.">
                                                <p class="c-article-references__text" id="ref-CR83">Beltagy, I., Lo, K.
                                                    &amp; Cohan, A. SciBERT: a pretrained language model for scientific
                                                    text. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.1903.10676">https://doi.org/10.48550/arXiv.1903.10676</a>
                                                    (2019).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="84.">
                                                <p class="c-article-references__text" id="ref-CR84">Lewis, P., Ott, M.,
                                                    Du,
                                                    J. &amp; Stoyanov, V. Pretrained language models for biomedical and
                                                    clinical tasks: Understanding and extending the state-of-the-art. In
                                                    <i>Proc. 3rd Clinical Natural Language Processing Workshop</i> (eds
                                                    Roberts, K., Bethard, S. &amp; Naumann, T.) 146–157 (Association for
                                                    Computational Linguistics, 2020).
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="85.">
                                                <p class="c-article-references__text" id="ref-CR85">Shin, H.-C. et al.
                                                    BioMegatron: larger biomedical domain language model. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2010.06060">https://doi.org/10.48550/arXiv.2010.06060</a>
                                                    (2020).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="86.">
                                                <p class="c-article-references__text" id="ref-CR86">Lee, J. et al.
                                                    Biobert:
                                                    a pre-trained biomedical language representation model for
                                                    biomedical
                                                    text mining. <i>Bioinformatics</i> <b>36</b>, 1234–1240 (2020).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1093/bioinformatics/btz682"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1093%2Fbioinformatics%2Fbtz682"
                                                        aria-label="Article reference 86"
                                                        data-doi="10.1093/bioinformatics/btz682">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="cas reference"
                                                        href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXhslCisLrL"
                                                        aria-label="CAS reference 86">CAS</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31501885"
                                                        aria-label="PubMed reference 86">PubMed</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 86"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Biobert%3A%20a%20pre-trained%20biomedical%20language%20representation%20model%20for%20biomedical%20text%20mining&amp;journal=Bioinformatics&amp;doi=10.1093%2Fbioinformatics%2Fbtz682&amp;volume=36&amp;pages=1234-1240&amp;publication_year=2020&amp;author=Lee%2CJ">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="87.">
                                                <p class="c-article-references__text" id="ref-CR87">Gu, Y. et al.
                                                    Domain-specific language model pretraining for biomedical natural
                                                    language processing. <i>ACM Trans. Comput. Healthc.</i> <b>3</b>, 2
                                                    (2021).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 87"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Domain-specific%20language%20model%20pretraining%20for%20biomedical%20natural%20language%20processing&amp;journal=ACM%20Trans.%20Comput.%20Healthc.&amp;volume=3&amp;publication_year=2021&amp;author=Gu%2CY">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="88.">
                                                <p class="c-article-references__text" id="ref-CR88">Papanikolaou, Y.
                                                    &amp;
                                                    Pierleoni, A. DARE: data augmented relation extraction with GPT-2.
                                                    Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2004.13845">https://doi.org/10.48550/arXiv.2004.13845</a>
                                                    (2020).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="89.">
                                                <p class="c-article-references__text" id="ref-CR89">Hong, Z. et al. The
                                                    diminishing returns of masked language models to science. Preprint
                                                    at <a
                                                        href="https://doi.org/10.48550/arXiv.2205.11342">https://doi.org/10.48550/arXiv.2205.11342</a>
                                                    (2023).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="90.">
                                                <p class="c-article-references__text" id="ref-CR90">Korngiebel, D. M.
                                                    &amp;
                                                    Mooney, S. D. Considering the possibilities and pitfalls of
                                                    generative
                                                    pre-trained transformer 3 (GPT-3) in healthcare delivery. <i>NPJ
                                                        Digit.
                                                        Med.</i> <b>4</b>, 93 (2021).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.1038/s41746-021-00464-x"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.1038%2Fs41746-021-00464-x"
                                                        aria-label="Article reference 90"
                                                        data-doi="10.1038/s41746-021-00464-x">Article</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link" data-track-action="pubmed reference"
                                                        href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34083689"
                                                        aria-label="PubMed reference 90">PubMed</a>&nbsp;
                                                    <a data-track="click" rel="nofollow noopener"
                                                        data-track-label="link"
                                                        data-track-action="pubmed central reference"
                                                        href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8175735"
                                                        aria-label="PubMed Central reference 90">PubMed
                                                        Central</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 90"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Considering%20the%20possibilities%20and%20pitfalls%20of%20generative%20pre-trained%20transformer%203%20%28GPT-3%29%20in%20healthcare%20delivery&amp;journal=NPJ%20Digit.%20Med.&amp;doi=10.1038%2Fs41746-021-00464-x&amp;volume=4&amp;publication_year=2021&amp;author=Korngiebel%2CDM&amp;author=Mooney%2CSD">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="91.">
                                                <p class="c-article-references__text" id="ref-CR91">Sezgin, E.,
                                                    Sirrianni,
                                                    J. &amp; Linwood, S. L. et al. Operationalizing and implementing
                                                    pretrained, large artificial intelligence linguistic models in the
                                                    us
                                                    health care system: outlook of generative pretrained transformer 3
                                                    (GPT-3) as a service model. <i>JMIR Med. Informatics</i> <b>10</b>,
                                                    e32875 (2022).</p>
                                                <p class="c-article-references__links u-hide-print"><a
                                                        data-track="click" rel="nofollow noopener"
                                                        data-track-label="10.2196/32875"
                                                        data-track-action="article reference"
                                                        href="https://doi.org/10.2196%2F32875"
                                                        aria-label="Article reference 91"
                                                        data-doi="10.2196/32875">Article</a>&nbsp;
                                                    <a data-track="click" data-track-action="google scholar reference"
                                                        data-track-label="link" rel="nofollow noopener"
                                                        aria-label="Google Scholar reference 91"
                                                        href="http://scholar.google.com/scholar_lookup?&amp;title=Operationalizing%20and%20implementing%20pretrained%2C%20large%20artificial%20intelligence%20linguistic%20models%20in%20the%20us%20health%20care%20system%3A%20outlook%20of%20generative%20pretrained%20transformer%203%20%28GPT-3%29%20as%20a%20service%20model&amp;journal=JMIR%20Med.%20Informatics&amp;doi=10.2196%2F32875&amp;volume=10&amp;publication_year=2022&amp;author=Sezgin%2CE&amp;author=Sirrianni%2CJ&amp;author=Linwood%2CSL">
                                                        Google Scholar</a>&nbsp;
                                                </p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="92.">
                                                <p class="c-article-references__text" id="ref-CR92">Agrawal, M.,
                                                    Hegselmann,
                                                    S., Lang, H., Kim, Y. &amp; Sontag, D. Large language models are
                                                    zero-shot clinical information extractors. Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2205.12689">https://doi.org/10.48550/arXiv.2205.12689</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="93.">
                                                <p class="c-article-references__text" id="ref-CR93">Liévin, V., Hother,
                                                    C.
                                                    E. &amp; Winther, O. Can large language models reason about medical
                                                    questions? Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2207.08143">https://doi.org/10.48550/arXiv.2207.08143</a>
                                                    (2022).</p>
                                            </li>
                                            <li class="c-article-references__item js-c-reading-companion-references-item"
                                                data-counter="94.">
                                                <p class="c-article-references__text" id="ref-CR94">Ouyang, L. et al.
                                                    Training language models to follow instructions with human feedback.
                                                    Preprint at <a
                                                        href="https://doi.org/10.48550/arXiv.2203.02155">https://doi.org/10.48550/arXiv.2203.02155</a>
                                                    (2022).</p>
                                            </li>
                                        </ol>
                                        <p class="c-article-references__download u-hide-print"><a data-track="click"
                                                data-track-action="download citation references" data-track-label="link"
                                                rel="nofollow"
                                                href="https://citation-needed.springer.com/v2/references/10.1038/s41586-023-06291-2?format=refman&amp;flavour=references">Download
                                                references<svg width="16" height="16" focusable="false" role="img"
                                                    aria-hidden="true" class="u-icon">
                                                    <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                        xlink:href="#icon-download"></use>
                                                </svg></a></p>
                                    </div>
                                </div>
                            </div>
                        </section>
                    </div>
                    <!-- <section data-title="Acknowledgements">
                        <div class="c-article-section" id="Ack1-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="Ack1">Acknowledgements</h2>
                            <div class="c-article-section__content" id="Ack1-content">
                                <p>This project was an extensive collaboration between many teams at Google Research,
                                    with
                                    DeepMind involved in an advisory capacity. We thank M. Howell, C. Chen, B. Mustafa,
                                    D.
                                    Fleet, F. Kibria, G. Turner, S. W. Man, D. Kim, B. Hatfield, L. Lehmann, I. Horn, M.
                                    Shiels, S. Shetty, J. Zitting, E. Rappaport, L. Marples, V. Sounderajah, A. Connell,
                                    J.
                                    Freyberg, C. Hughes, M. Jones-Bell, S. Thomas, M. Ho, R. Wong, S. Prakash, B. Green,
                                    E.
                                    Dominowska, F. Liu and X. Wang for their valuable insights and feedback during our
                                    research. We are also grateful to K. DeSalvo, Z. Ghahramani, J. Manyika and J. Dean
                                    for
                                    their support during the course of this project.</p>
                            </div>
                        </div>
                    </section>
                    <section aria-labelledby="author-information" data-title="Author information">
                        <div class="c-article-section" id="author-information-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="author-information">Author information</h2>
                            <div class="c-article-section__content" id="author-information-content"><span
                                    class="c-article-author-information__subtitle u-visually-hidden"
                                    id="author-notes">Author notes</span>
                                <ol class="c-article-author-information__list">
                                    <li class="c-article-author-information__item" id="na1">
                                        <p>These authors contributed equally: Karan Singhal, Shekoofeh Azizi, Tao Tu</p>
                                    </li>
                                    <li class="c-article-author-information__item" id="na2">
                                        <p>These authors jointly supervised this work: Alan Karthikesalingam, Vivek
                                            Natarajan</p>
                                    </li>
                                </ol>
                                <h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3>
                                <ol class="c-article-author-affiliation__list">
                                    <li id="Aff1">
                                        <p class="c-article-author-affiliation__address">Google Research, Mountain View,
                                            CA,
                                            USA</p>
                                        <p class="c-article-author-affiliation__authors-list">Karan
                                            Singhal,&nbsp;Shekoofeh
                                            Azizi,&nbsp;Tao Tu,&nbsp;S. Sara Mahdavi,&nbsp;Jason Wei,&nbsp;Hyung Won
                                            Chung,&nbsp;Nathan Scales,&nbsp;Ajay Tanwani,&nbsp;Heather
                                            Cole-Lewis,&nbsp;Stephen Pfohl,&nbsp;Perry Payne,&nbsp;Martin
                                            Seneviratne,&nbsp;Paul Gamble,&nbsp;Chris Kelly,&nbsp;Abubakr
                                            Babiker,&nbsp;Nathanael Schärli,&nbsp;Aakanksha Chowdhery,&nbsp;Philip
                                            Mansfield,&nbsp;Blaise Agüera y Arcas,&nbsp;Dale Webster,&nbsp;Greg S.
                                            Corrado,&nbsp;Yossi Matias,&nbsp;Katherine Chou,&nbsp;Juraj
                                            Gottweis,&nbsp;Yun
                                            Liu,&nbsp;Alvin Rajkomar,&nbsp;Joelle Barral,&nbsp;Christopher
                                            Semturs,&nbsp;Alan Karthikesalingam&nbsp;&amp;&nbsp;Vivek Natarajan</p>
                                    </li>
                                    <li id="Aff2">
                                        <p class="c-article-author-affiliation__address">National Library of Medicine,
                                            Bethesda, MD, USA</p>
                                        <p class="c-article-author-affiliation__authors-list">Dina Demner-Fushman</p>
                                    </li>
                                    <li id="Aff3">
                                        <p class="c-article-author-affiliation__address">DeepMind, London, UK</p>
                                        <p class="c-article-author-affiliation__authors-list">Nenad Tomasev</p>
                                    </li>
                                </ol>
                                <div class="u-js-hide u-hide-print" data-test="author-info"><span
                                        class="c-article__sub-heading">Authors</span>
                                    <ol class="c-article-authors-search u-list-reset">
                                        <li id="auth-Karan-Singhal-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Karan
                                                Singhal</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Karan%20Singhal" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Karan%20Singhal"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Karan%20Singhal%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Shekoofeh-Azizi-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Shekoofeh
                                                Azizi</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Shekoofeh%20Azizi" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shekoofeh%20Azizi"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shekoofeh%20Azizi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Tao-Tu-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Tao
                                                Tu</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Tao%20Tu" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tao%20Tu"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tao%20Tu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-S__Sara-Mahdavi-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">S. Sara
                                                Mahdavi</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=S.%20Sara%20Mahdavi"
                                                        class="c-article-button" data-track="click"
                                                        data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=S.%20Sara%20Mahdavi"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22S.%20Sara%20Mahdavi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Jason-Wei-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Jason
                                                Wei</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Jason%20Wei" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jason%20Wei"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jason%20Wei%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Hyung_Won-Chung-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Hyung Won
                                                Chung</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Hyung%20Won%20Chung"
                                                        class="c-article-button" data-track="click"
                                                        data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Hyung%20Won%20Chung"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Hyung%20Won%20Chung%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Nathan-Scales-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Nathan
                                                Scales</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Nathan%20Scales" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Nathan%20Scales"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nathan%20Scales%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Ajay-Tanwani-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Ajay
                                                Tanwani</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Ajay%20Tanwani" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ajay%20Tanwani"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ajay%20Tanwani%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Heather-Cole_Lewis-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Heather
                                                Cole-Lewis</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Heather%20Cole-Lewis"
                                                        class="c-article-button" data-track="click"
                                                        data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Heather%20Cole-Lewis"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Heather%20Cole-Lewis%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Stephen-Pfohl-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Stephen
                                                Pfohl</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Stephen%20Pfohl" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Stephen%20Pfohl"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Stephen%20Pfohl%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Perry-Payne-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Perry
                                                Payne</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Perry%20Payne" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Perry%20Payne"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Perry%20Payne%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Martin-Seneviratne-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Martin
                                                Seneviratne</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Martin%20Seneviratne"
                                                        class="c-article-button" data-track="click"
                                                        data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Martin%20Seneviratne"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Martin%20Seneviratne%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Paul-Gamble-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Paul
                                                Gamble</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Paul%20Gamble" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Paul%20Gamble"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Paul%20Gamble%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Chris-Kelly-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Chris
                                                Kelly</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Chris%20Kelly" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Chris%20Kelly"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Chris%20Kelly%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Abubakr-Babiker-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Abubakr
                                                Babiker</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Abubakr%20Babiker" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Abubakr%20Babiker"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Abubakr%20Babiker%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Nathanael-Sch_rli-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Nathanael
                                                Schärli</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Nathanael%20Sch%C3%A4rli"
                                                        class="c-article-button" data-track="click"
                                                        data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Nathanael%20Sch%C3%A4rli"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nathanael%20Sch%C3%A4rli%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Aakanksha-Chowdhery-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Aakanksha
                                                Chowdhery</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Aakanksha%20Chowdhery"
                                                        class="c-article-button" data-track="click"
                                                        data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Aakanksha%20Chowdhery"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Aakanksha%20Chowdhery%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Philip-Mansfield-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Philip
                                                Mansfield</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Philip%20Mansfield" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Philip%20Mansfield"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Philip%20Mansfield%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Dina-Demner_Fushman-Aff2"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Dina
                                                Demner-Fushman</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Dina%20Demner-Fushman"
                                                        class="c-article-button" data-track="click"
                                                        data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Dina%20Demner-Fushman"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Dina%20Demner-Fushman%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Blaise-Ag_era_y_Arcas-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Blaise
                                                Agüera y
                                                Arcas</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Blaise%20Ag%C3%BCera%20y%20Arcas"
                                                        class="c-article-button" data-track="click"
                                                        data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Blaise%20Ag%C3%BCera%20y%20Arcas"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Blaise%20Ag%C3%BCera%20y%20Arcas%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Dale-Webster-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Dale
                                                Webster</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Dale%20Webster" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Dale%20Webster"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Dale%20Webster%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Greg_S_-Corrado-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Greg S.
                                                Corrado</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Greg%20S.%20Corrado"
                                                        class="c-article-button" data-track="click"
                                                        data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Greg%20S.%20Corrado"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Greg%20S.%20Corrado%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Yossi-Matias-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Yossi
                                                Matias</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Yossi%20Matias" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yossi%20Matias"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yossi%20Matias%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Katherine-Chou-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Katherine
                                                Chou</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Katherine%20Chou" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Katherine%20Chou"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Katherine%20Chou%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Juraj-Gottweis-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Juraj
                                                Gottweis</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Juraj%20Gottweis" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Juraj%20Gottweis"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Juraj%20Gottweis%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Nenad-Tomasev-Aff3"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Nenad
                                                Tomasev</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Nenad%20Tomasev" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Nenad%20Tomasev"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nenad%20Tomasev%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Yun-Liu-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Yun
                                                Liu</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Yun%20Liu" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yun%20Liu"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yun%20Liu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Alvin-Rajkomar-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Alvin
                                                Rajkomar</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Alvin%20Rajkomar" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Alvin%20Rajkomar"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Alvin%20Rajkomar%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Joelle-Barral-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Joelle
                                                Barral</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Joelle%20Barral" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Joelle%20Barral"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Joelle%20Barral%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Christopher-Semturs-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Christopher
                                                Semturs</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Christopher%20Semturs"
                                                        class="c-article-button" data-track="click"
                                                        data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Christopher%20Semturs"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Christopher%20Semturs%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Alan-Karthikesalingam-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Alan
                                                Karthikesalingam</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Alan%20Karthikesalingam"
                                                        class="c-article-button" data-track="click"
                                                        data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Alan%20Karthikesalingam"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Alan%20Karthikesalingam%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                        <li id="auth-Vivek-Natarajan-Aff1"><span
                                                class="c-article-authors-search__title u-h3 js-search-name">Vivek
                                                Natarajan</span>
                                            <div class="c-article-authors-search__list">
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--left">
                                                    <a href="/search?author=Vivek%20Natarajan" class="c-article-button"
                                                        data-track="click" data-track-action="author link - publication"
                                                        data-track-label="link" rel="nofollow">View author
                                                        publications</a>
                                                </div>
                                                <div
                                                    class="c-article-authors-search__item c-article-authors-search__list-item--right">
                                                    <p class="search-in-title-js c-article-authors-search__text">You can
                                                        also search for this author in
                                                        <span class="c-article-identifiers"><a
                                                                class="c-article-identifiers__item"
                                                                href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Vivek%20Natarajan"
                                                                data-track="click"
                                                                data-track-action="author link - pubmed"
                                                                data-track-label="link" rel="nofollow">PubMed</a><span
                                                                class="u-hide">&nbsp;</span><a
                                                                class="c-article-identifiers__item"
                                                                href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Vivek%20Natarajan%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en"
                                                                data-track="click"
                                                                data-track-action="author link - scholar"
                                                                data-track-label="link" rel="nofollow">Google
                                                                Scholar</a></span>
                                                    </p>
                                                </div>
                                            </div>
                                        </li>
                                    </ol>
                                </div>
                                <h3 class="c-article__sub-heading" id="contributions">Contributions</h3>
                                <p>K.S., S.A., T.T., S.S.M., C.S., A.K. and V.N. contributed to the conception and
                                    design of
                                    the work. A.K., V.N., S.S.M., K.S., S.A. and T.T. contributed to the data
                                    acquisition
                                    and curation. K.S., S.A., T.T., V.N. and A.B. contributed to the technical
                                    implementation. A.K., V.N., K.S., S.A., T.T., C.S., H.C.-L., S.P., P.P. and N.T.
                                    contributed to the evaluation framework used in the study. J.W., H.W.C., N. Schärli,
                                    A.B., N. Scales and A.C. provided technical and infrastructure guidance. A.K., M.S.,
                                    P.G. and C.K. provided clinical inputs to the study. D.D.-F. provided guidance on
                                    the
                                    datasets used in the study. All authors contributed to the drafting and revising of
                                    the
                                    manuscript.</p>
                                <h3 class="c-article__sub-heading" id="corresponding-author">Corresponding authors</h3>
                                <p id="corresponding-author-list">Correspondence to
                                    <a id="corresp-c1" href="mailto:karansinghal@google.com">Karan Singhal</a>, <a
                                        id="corresp-c2" href="mailto:shekazizi@google.com">Shekoofeh Azizi</a>, <a
                                        id="corresp-c3" href="mailto:alankarthi@google.com">Alan Karthikesalingam</a> or
                                    <a id="corresp-c4" href="mailto:natviv@google.com">Vivek Natarajan</a>.
                                </p>
                            </div>
                        </div>
                    </section>
                    <section data-title="Ethics declarations">
                        <div class="c-article-section" id="ethics-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="ethics">Ethics declarations</h2>
                            <div class="c-article-section__content" id="ethics-content">

                                <h3 class="c-article__sub-heading" id="FPar8">Competing interests</h3>
                                <p>This study was funded by Alphabet Inc. and/or a subsidiary thereof (Alphabet). K.S.,
                                    S.A., T.T., V.N., A.K., S.S.M., C.S., J.W., H.W.C., N. Scales, A.T., H.C.-L., S.P.,
                                    P.P., M.S., P.G., C.K., A.B., N. Schärli, A.C., P.M., B.A.A., D.W., G.S.C., Y.M.,
                                    K.C.,
                                    J.G., A.R., N.T., J.B. and Y.L. are employees of Alphabet and may own stock as part
                                    of
                                    the standard compensation package. D.D.-F. is affiliated with the US National
                                    Library of
                                    Medicine.</p>

                            </div>
                        </div>
                    </section>
                    <section data-title="Peer review">
                        <div class="c-article-section" id="peer-review-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="peer-review">Peer review</h2>
                            <div class="c-article-section__content" id="peer-review-content">


                                <h3 class="c-article__sub-heading" id="FPar7">Peer review information</h3>
                                <p><i>Nature</i> thanks Andrew Beam and the other, anonymous, reviewer(s) for their
                                    contribution to the peer review of this work. <a data-track="click"
                                        data-track-label="link" data-track-action="supplementary material anchor"
                                        href="/articles/s41586-023-06291-2#MOESM3">Peer reviewer reports</a> are
                                    available.
                                </p>

                            </div>
                        </div>
                    </section>
                    <section data-title="Additional information">
                        <div class="c-article-section" id="additional-information-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="additional-information">Additional information</h2>
                            <div class="c-article-section__content" id="additional-information-content">
                                <p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional
                                    claims in published maps and institutional affiliations.</p>
                            </div>
                        </div>
                    </section>
                    <section data-title="Extended data figures and tables">
                        <div class="c-article-section" id="Sec58-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="Sec58">Extended data figures and tables</h2>
                            <div class="c-article-section__content" id="Sec58-content">
                                <div data-test="supplementary-info">
                                    <div id="figshareContainer" class="c-article-figshare-container"
                                        data-test="figshare-container"></div>
                                    <div class="c-article-supplementary__item js-c-reading-companion-figures-item"
                                        data-test="supp-item" id="Fig7">
                                        <h3 class="c-article-supplementary__title u-h3"><a class="print-link"
                                                data-track="click" data-track-action="view supplementary info"
                                                data-track-label="link" data-test="supp-info-link"
                                                href="/articles/s41586-023-06291-2/figures/7"
                                                data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig7_ESM.jpg">Extended
                                                Data Fig. 1 Instruction prompt tuning for Med-PaLM.</a></h3>
                                        <div class="c-article-supplementary__description"
                                            data-component="thumbnail-container">
                                            <p>We use instructions and exemplars from a panel of qualified clinicians
                                                for
                                                each of the consumer medical question answering datasets and use them to
                                                instruction prompt tune Flan-PaLM. Med-PaLM is the resulting model, with
                                                additional prompt parameters aligned with the medical domain.</p>
                                        </div>
                                    </div>
                                    <div class="c-article-supplementary__item js-c-reading-companion-figures-item"
                                        data-test="supp-item" id="Fig8">
                                        <h3 class="c-article-supplementary__title u-h3"><a class="print-link"
                                                data-track="click" data-track-action="view supplementary info"
                                                data-track-label="link" data-test="supp-info-link"
                                                href="/articles/s41586-023-06291-2/figures/8"
                                                data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig8_ESM.jpg">Extended
                                                Data Fig. 2 Comparison of SOTA LLMs on MMLU clinical topics.</a></h3>
                                        <div class="c-article-supplementary__description"
                                            data-component="thumbnail-container">
                                            <p>Flan-PaLM achieves state-of-the-art performance on MMLU clinical topics.
                                            </p>
                                        </div>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item">
                                        <div class="c-article-table" data-test="inline-table"
                                            data-container-section="table" id="table-1">
                                            <figure>
                                                <figcaption class="c-article-table__figcaption"><b id="Tab1"
                                                        data-test="table-caption">Extended Data Table 1 Summary of
                                                        MultiMedQA describing the format, size, and domain of the
                                                        datasets
                                                        in the benchmark</b></figcaption>
                                                <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                        data-test="table-link" data-track="click"
                                                        data-track-action="view table" data-track-label="button"
                                                        rel="nofollow" href="/articles/s41586-023-06291-2/tables/1"
                                                        aria-label="Full size table 1"><span>Full size table</span><svg
                                                            width="16" height="16" focusable="false" role="img"
                                                            aria-hidden="true" class="u-icon">
                                                            <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                                xlink:href="#icon-chevron-right"></use>
                                                        </svg></a></div>
                                            </figure>
                                        </div>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item">
                                        <div class="c-article-table" data-test="inline-table"
                                            data-container-section="table" id="table-2">
                                            <figure>
                                                <figcaption class="c-article-table__figcaption"><b id="Tab2"
                                                        data-test="table-caption">Extended Data Table 2 Summary of the
                                                        different axes along which clinicians evaluate the answers in
                                                        our
                                                        consumer medical question answering datasets</b></figcaption>
                                                <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                        data-test="table-link" data-track="click"
                                                        data-track-action="view table" data-track-label="button"
                                                        rel="nofollow" href="/articles/s41586-023-06291-2/tables/2"
                                                        aria-label="Full size table 2"><span>Full size table</span><svg
                                                            width="16" height="16" focusable="false" role="img"
                                                            aria-hidden="true" class="u-icon">
                                                            <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                                xlink:href="#icon-chevron-right"></use>
                                                        </svg></a></div>
                                            </figure>
                                        </div>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item">
                                        <div class="c-article-table" data-test="inline-table"
                                            data-container-section="table" id="table-3">
                                            <figure>
                                                <figcaption class="c-article-table__figcaption"><b id="Tab3"
                                                        data-test="table-caption">Extended Data Table 3 Summary of the
                                                        different axes along which lay users evaluate the model answers
                                                        in
                                                        our consumer medical question answering datasets</b>
                                                </figcaption>
                                                <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                        data-test="table-link" data-track="click"
                                                        data-track-action="view table" data-track-label="button"
                                                        rel="nofollow" href="/articles/s41586-023-06291-2/tables/3"
                                                        aria-label="Full size table 3"><span>Full size table</span><svg
                                                            width="16" height="16" focusable="false" role="img"
                                                            aria-hidden="true" class="u-icon">
                                                            <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                                xlink:href="#icon-chevron-right"></use>
                                                        </svg></a></div>
                                            </figure>
                                        </div>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item">
                                        <div class="c-article-table" data-test="inline-table"
                                            data-container-section="table" id="table-4">
                                            <figure>
                                                <figcaption class="c-article-table__figcaption"><b id="Tab4"
                                                        data-test="table-caption">Extended Data Table 4 Summary of the
                                                        best
                                                        performing models on the MedQA (USMLE) dataset questions with 4
                                                        options</b></figcaption>
                                                <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                        data-test="table-link" data-track="click"
                                                        data-track-action="view table" data-track-label="button"
                                                        rel="nofollow" href="/articles/s41586-023-06291-2/tables/4"
                                                        aria-label="Full size table 4"><span>Full size table</span><svg
                                                            width="16" height="16" focusable="false" role="img"
                                                            aria-hidden="true" class="u-icon">
                                                            <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                                xlink:href="#icon-chevron-right"></use>
                                                        </svg></a></div>
                                            </figure>
                                        </div>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item">
                                        <div class="c-article-table" data-test="inline-table"
                                            data-container-section="table" id="table-5">
                                            <figure>
                                                <figcaption class="c-article-table__figcaption"><b id="Tab5"
                                                        data-test="table-caption">Extended Data Table 5 Comparison of
                                                        the
                                                        performance between Med-PaLM 540B and Flan-PaLM 540B with
                                                        self-consistency (SC) across multiple-choice datasets</b>
                                                </figcaption>
                                                <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                        data-test="table-link" data-track="click"
                                                        data-track-action="view table" data-track-label="button"
                                                        rel="nofollow" href="/articles/s41586-023-06291-2/tables/5"
                                                        aria-label="Full size table 5"><span>Full size table</span><svg
                                                            width="16" height="16" focusable="false" role="img"
                                                            aria-hidden="true" class="u-icon">
                                                            <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                                xlink:href="#icon-chevron-right"></use>
                                                        </svg></a></div>
                                            </figure>
                                        </div>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item">
                                        <div class="c-article-table" data-test="inline-table"
                                            data-container-section="table" id="table-6">
                                            <figure>
                                                <figcaption class="c-article-table__figcaption"><b id="Tab6"
                                                        data-test="table-caption">Extended Data Table 6 Representative
                                                        explanations generated by the Flan-PaLM 540B model to support
                                                        its
                                                        multiple-choice answers in the MedQA dataset</b></figcaption>
                                                <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                        data-test="table-link" data-track="click"
                                                        data-track-action="view table" data-track-label="button"
                                                        rel="nofollow" href="/articles/s41586-023-06291-2/tables/6"
                                                        aria-label="Full size table 6"><span>Full size table</span><svg
                                                            width="16" height="16" focusable="false" role="img"
                                                            aria-hidden="true" class="u-icon">
                                                            <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                                xlink:href="#icon-chevron-right"></use>
                                                        </svg></a></div>
                                            </figure>
                                        </div>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item">
                                        <div class="c-article-table" data-test="inline-table"
                                            data-container-section="table" id="table-7">
                                            <figure>
                                                <figcaption class="c-article-table__figcaption"><b id="Tab7"
                                                        data-test="table-caption">Extended Data Table 7 Examples of
                                                        Med-PaLM
                                                        responses to questions in the HealthSearchQA dataset</b>
                                                </figcaption>
                                                <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                        data-test="table-link" data-track="click"
                                                        data-track-action="view table" data-track-label="button"
                                                        rel="nofollow" href="/articles/s41586-023-06291-2/tables/7"
                                                        aria-label="Full size table 7"><span>Full size table</span><svg
                                                            width="16" height="16" focusable="false" role="img"
                                                            aria-hidden="true" class="u-icon">
                                                            <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                                xlink:href="#icon-chevron-right"></use>
                                                        </svg></a></div>
                                            </figure>
                                        </div>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item">
                                        <div class="c-article-table" data-test="inline-table"
                                            data-container-section="table" id="table-8">
                                            <figure>
                                                <figcaption class="c-article-table__figcaption"><b id="Tab8"
                                                        data-test="table-caption">Extended Data Table 8 Examples of
                                                        HealthSearchQA questions where the physician answers are
                                                        considered
                                                        incomplete, and corresponding Med-PaLM answers</b></figcaption>
                                                <div class="u-text-right u-hide-print"><a class="c-article__pill-button"
                                                        data-test="table-link" data-track="click"
                                                        data-track-action="view table" data-track-label="button"
                                                        rel="nofollow" href="/articles/s41586-023-06291-2/tables/8"
                                                        aria-label="Full size table 8"><span>Full size table</span><svg
                                                            width="16" height="16" focusable="false" role="img"
                                                            aria-hidden="true" class="u-icon">
                                                            <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                                xlink:href="#icon-chevron-right"></use>
                                                        </svg></a></div>
                                            </figure>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </section>
                    <section data-title="Supplementary information">
                        <div class="c-article-section" id="Sec59-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="Sec59">Supplementary information</h2>
                            <div class="c-article-section__content" id="Sec59-content">
                                <div data-test="supplementary-info">
                                    <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1">
                                        <h3 class="c-article-supplementary__title u-h3"><a class="print-link"
                                                data-track="click" data-track-action="view supplementary info"
                                                data-track-label="link" data-test="supp-info-link"
                                                href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_MOESM1_ESM.pdf"
                                                data-supp-info-image="">Supplementary Information</a></h3>
                                        <div class="c-article-supplementary__description"
                                            data-component="thumbnail-container">
                                            <p>This file contains Supplementary Information, including Supplementary
                                                Tables
                                                1–28.</p>
                                        </div>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM2">
                                        <h3 class="c-article-supplementary__title u-h3"><a class="print-link"
                                                data-track="click" data-track-action="view supplementary info"
                                                data-track-label="link" data-test="supp-info-link"
                                                href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_MOESM2_ESM.pdf"
                                                data-supp-info-image="">Reporting Summary</a></h3>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM3">
                                        <h3 class="c-article-supplementary__title u-h3"><a class="print-link"
                                                data-track="click" data-track-action="view supplementary info"
                                                data-track-label="link" data-test="supp-info-link"
                                                href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_MOESM3_ESM.pdf"
                                                data-supp-info-image="">Peer Review File</a></h3>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM4">
                                        <h3 class="c-article-supplementary__title u-h3"><a class="print-link"
                                                data-track="click" data-track-action="view supplementary info"
                                                data-track-label="link" data-test="supp-info-link"
                                                href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_MOESM4_ESM.pdf"
                                                data-supp-info-image="">Supplementary Fig. 1</a></h3>
                                        <div class="c-article-supplementary__description"
                                            data-component="thumbnail-container">
                                            <p>Scaling plots for PaLM and Flan-PaLM with few-shot prompting on MedQA and
                                                MedMCQA.</p>
                                        </div>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM5">
                                        <h3 class="c-article-supplementary__title u-h3"><a class="print-link"
                                                data-track="click" data-track-action="view supplementary info"
                                                data-track-label="link" data-test="supp-info-link"
                                                href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_MOESM5_ESM.pdf"
                                                data-supp-info-image="">Supplementary Fig. 2</a></h3>
                                        <div class="c-article-supplementary__description"
                                            data-component="thumbnail-container">
                                            <p>Scaling plots for Flan-PaLM with few-shot and Flan-PaLM few-shot + COT +
                                                self-consistency on MedQA and MedMCQA.</p>
                                        </div>
                                    </div>
                                    <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM6">
                                        <h3 class="c-article-supplementary__title u-h3"><a class="print-link"
                                                data-track="click" data-track-action="view supplementary info"
                                                data-track-label="link" data-test="supp-info-link"
                                                href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_MOESM6_ESM.xlsx"
                                                data-supp-info-image="">Supplementary Data</a></h3>
                                        <div class="c-article-supplementary__description"
                                            data-component="thumbnail-container">
                                            <p>Health Search Q&amp;A Dataset.</p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </section>
                    <section data-title="Rights and permissions">
                        <div class="c-article-section" id="rightslink-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="rightslink">Rights and permissions</h2>
                            <div class="c-article-section__content" id="rightslink-content">
                                <p><b>Open Access</b> This article is licensed under a Creative Commons Attribution 4.0
                                    International License, which permits use, sharing, adaptation, distribution and
                                    reproduction in any medium or format, as long as you give appropriate credit to the
                                    original author(s) and the source, provide a link to the Creative Commons licence,
                                    and
                                    indicate if changes were made. The images or other third party material in this
                                    article
                                    are included in the article’s Creative Commons licence, unless indicated otherwise
                                    in a
                                    credit line to the material. If material is not included in the article’s Creative
                                    Commons licence and your intended use is not permitted by statutory regulation or
                                    exceeds the permitted use, you will need to obtain permission directly from the
                                    copyright holder. To view a copy of this licence, visit <a
                                        href="http://creativecommons.org/licenses/by/4.0/"
                                        rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
                                <p class="c-article-rights"><a data-track="click"
                                        data-track-action="view rights and permissions" data-track-label="link"
                                        href="https://s100.copyright.com/AppDispatchServlet?title=Large%20language%20models%20encode%20clinical%20knowledge&amp;author=Karan%20Singhal%20et%20al&amp;contentID=10.1038%2Fs41586-023-06291-2&amp;copyright=The%20Author%28s%29&amp;publication=0028-0836&amp;publicationDate=2023-07-12&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints
                                        and Permissions</a></p>
                            </div>
                        </div>
                    </section>
                    <section aria-labelledby="article-info" data-title="About this article">
                        <div class="c-article-section" id="article-info-section">
                            <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                                id="article-info">About this article</h2>
                            <div class="c-article-section__content" id="article-info-content">
                                <div class="c-bibliographic-information">
                                    <div
                                        class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border">
                                        <a data-crossmark="10.1038/s41586-023-06291-2" target="_blank" rel="noopener"
                                            href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41586-023-06291-2"
                                            data-track="click" data-track-action="Click Crossmark"
                                            data-track-label="link" data-test="crossmark"><img width="57" height="81"
                                                alt="Check for updates. Verify currency and authenticity via CrossMark"
                                                src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a>
                                    </div>
                                    <div class="c-bibliographic-information__column">
                                        <h3 class="c-article__sub-heading" id="citeas">Cite this article</h3>
                                        <p class="c-bibliographic-information__citation">Singhal, K., Azizi, S., Tu, T.
                                            <i>et al.</i> Large language models encode clinical knowledge.
                                            <i>Nature</i> <b>620</b>, 172–180 (2023).
                                            https://doi.org/10.1038/s41586-023-06291-2
                                        </p>
                                        <p class="c-bibliographic-information__download-citation u-hide-print"><a
                                                data-test="citation-link" data-track="click"
                                                data-track-action="download article citation" data-track-label="link"
                                                data-track-external="" rel="nofollow"
                                                href="https://citation-needed.springer.com/v2/references/10.1038/s41586-023-06291-2?format=refman&amp;flavour=citation">Download
                                                citation<svg width="16" height="16" focusable="false" role="img"
                                                    aria-hidden="true" class="u-icon">
                                                    <use xmlns:xlink="http://www.w3.org/1999/xlink"
                                                        xlink:href="#icon-download"></use>
                                                </svg></a></p>
                                        <ul class="c-bibliographic-information__list" data-test="publication-history">
                                            <li class="c-bibliographic-information__list-item">
                                                <p>Received<span class="u-hide">: </span><span
                                                        class="c-bibliographic-information__value"><time
                                                            datetime="2023-01-25">25 January 2023</time></span></p>
                                            </li>
                                            <li class="c-bibliographic-information__list-item">
                                                <p>Accepted<span class="u-hide">: </span><span
                                                        class="c-bibliographic-information__value"><time
                                                            datetime="2023-06-05">05 June 2023</time></span></p>
                                            </li>
                                            <li class="c-bibliographic-information__list-item">
                                                <p>Published<span class="u-hide">: </span><span
                                                        class="c-bibliographic-information__value"><time
                                                            datetime="2023-07-12">12 July 2023</time></span></p>
                                            </li>
                                            <li class="c-bibliographic-information__list-item">
                                                <p>Issue Date<span class="u-hide">: </span><span
                                                        class="c-bibliographic-information__value"><time
                                                            datetime="2023-08-03">03 August 2023</time></span></p>
                                            </li>
                                            <li
                                                class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi">
                                                <p><abbr title="Digital Object Identifier">DOI</abbr><span
                                                        class="u-hide">:
                                                    </span><span
                                                        class="c-bibliographic-information__value">https://doi.org/10.1038/s41586-023-06291-2</span>
                                                </p>
                                            </li>
                                        </ul>
                                        <div data-component="share-box">
                                            <div class="c-article-share-box u-display-block">
                                                <h3 class="c-article__sub-heading">Share this article</h3>
                                                <p class="c-article-share-box__description">Anyone you share the
                                                    following
                                                    link with will be able to read this content:</p><button
                                                    class="js-get-share-url c-article-share-box__button" type="button"
                                                    id="get-share-url" data-track="click" data-track-label="button"
                                                    data-track-external="" data-track-action="get shareable link">Get
                                                    shareable link</button>
                                                <div class="js-no-share-url-container u-display-none" hidden="">
                                                    <p
                                                        class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">
                                                        Sorry, a shareable link is not currently available for this
                                                        article.
                                                    </p>
                                                </div>
                                                <div class="js-share-url-container u-display-none" hidden="">
                                                    <p class="js-share-url c-article-share-box__only-read-input"
                                                        id="share-url" data-track="click" data-track-label="button"
                                                        data-track-action="select share url"></p><button
                                                        class="js-copy-share-url c-article-share-box__button--link-like"
                                                        type="button" id="copy-share-url" data-track="click"
                                                        data-track-label="button" data-track-action="copy share url"
                                                        data-track-external="">Copy to clipboard</button>
                                                </div>
                                                <p
                                                    class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                                                    Provided by the Springer Nature SharedIt content-sharing initiative
                                                </p>
                                            </div>
                                        </div>
                                        <div data-component="article-info-list">
                                            <h3 class="c-article__sub-heading">Subjects</h3>
                                            <ul class="c-article-subject-list">
                                                <li class="c-article-subject-list__subject"><a
                                                        href="/subjects/health-care" data-track="click"
                                                        data-track-action="view subject" data-track-label="link">Health
                                                        care</a></li>
                                                <li class="c-article-subject-list__subject"><a
                                                        href="/subjects/medical-research" data-track="click"
                                                        data-track-action="view subject" data-track-label="link">Medical
                                                        research</a></li>
                                            </ul>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </section>
                </div>


                <section>
                    <div class="c-article-section js-article-section" id="further-reading-section">
                        <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                            id="further-reading">This article is cited by</h2>
                        <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
                            <ul class="c-article-further-reading__list" id="further-reading-list">

                                <li class="c-article-further-reading__item js-ref-item">
                                    <h3 class="c-article-further-reading__title">
                                        <a class="print-link" data-track="click"
                                            data-track-action="view further reading article"
                                            data-track-label="link:Benchmarking medical large language models"
                                            href="https://doi.org/10.1038/s44222-023-00097-7">
                                            Benchmarking medical large language models
                                        </a>
                                    </h3>

                                    <ul data-test="author-list"
                                        class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Sadra Bakhshandeh</li>
                                    </ul>

                                    <p class="c-article-further-reading__journal-title"><i>Nature Reviews
                                            Bioengineering</i>
                                        (2023)</p>
                                </li>

                                <li class="c-article-further-reading__item js-ref-item">
                                    <h3 class="c-article-further-reading__title">
                                        <a class="print-link" data-track="click"
                                            data-track-action="view further reading article"
                                            data-track-label="link:Artificial intelligence in the neonatal intensive care unit: the time is now"
                                            href="https://doi.org/10.1038/s41372-023-01719-z">
                                            Artificial intelligence in the neonatal intensive care unit: the time is now
                                        </a>
                                    </h3>

                                    <ul data-test="author-list"
                                        class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Kristyn Beam</li>
                                        <li>Puneet Sharma</li>
                                        <li>Andrew L. Beam</li>
                                    </ul>

                                    <p class="c-article-further-reading__journal-title"><i>Journal of Perinatology</i>
                                        (2023)</p>
                                </li>

                                <li class="c-article-further-reading__item js-ref-item">
                                    <h3 class="c-article-further-reading__title">
                                        <a class="print-link" data-track="click"
                                            data-track-action="view further reading article"
                                            data-track-label="link:Analysis of large-language model versus human performance for genetics questions"
                                            href="https://doi.org/10.1038/s41431-023-01396-8">
                                            Analysis of large-language model versus human performance for genetics
                                            questions
                                        </a>
                                    </h3>

                                    <ul data-test="author-list"
                                        class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Dat Duong</li>
                                        <li>Benjamin D. Solomon</li>
                                    </ul>

                                    <p class="c-article-further-reading__journal-title"><i>European Journal of Human
                                            Genetics</i> (2023)</p>
                                </li>

                                <li class="c-article-further-reading__item js-ref-item">
                                    <h3 class="c-article-further-reading__title">
                                        <a class="print-link" data-track="click"
                                            data-track-action="view further reading article"
                                            data-track-label="link:Assessing GPT-4’s role as a co-collaborator in scientific research: a case study analyzing Einstein’s special theory of relativity"
                                            href="https://doi.org/10.1007/s44163-023-00075-3">
                                            Assessing GPT-4’s role as a co-collaborator in scientific research: a case
                                            study
                                            analyzing Einstein’s special theory of relativity
                                        </a>
                                    </h3>

                                    <ul data-test="author-list"
                                        class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Steven Bryant</li>
                                    </ul>

                                    <p class="c-article-further-reading__journal-title"><i>Discover Artificial
                                            Intelligence</i> (2023)</p>
                                </li>

                            </ul>
                        </div>
                    </div>
                </section>



                <section data-title="Comments">
                    <div class="c-article-section" id="article-comments-section">
                        <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item"
                            id="article-comments">Comments</h2>
                        <div class="c-article-section__content" id="article-comments-content">
                            <p>By submitting a comment you agree to abide by our <a href="/info/tandc.html">Terms</a>
                                and <a href="/info/community-guidelines.html">Community Guidelines</a>. If you find
                                something
                                abusive or that does not comply with our terms or guidelines please flag it as
                                inappropriate.</p>
                        </div>
                    </div>
                </section>
                <div id="inject-comments">
                    <div class="placeholder" data-replace="true"
                        data-disqus-placeholder="/platform/disqus?doi=10.1038/s41586-023-06291-2 #article-comments-container">
                    </div>
                </div> -->

            </div>
        </article>
    </div>
</body>

</html>